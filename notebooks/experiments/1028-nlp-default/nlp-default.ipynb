{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "sns.set()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n",
    "node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fast-layout-7-85\"\n",
    "trans_node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fast-layout-6-79-dataset\"\n",
    "trans_node_config_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout7-81-dataset\"\n",
    "workdir = Path().resolve() / \"out\"\n",
    "workdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "ignores = []\n",
    "for ds in [\"train\", \"valid\", \"test\"]:\n",
    "    records = []\n",
    "    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n",
    "        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n",
    "        for filepath in sorted(datadir.glob(\"*.npz\")):\n",
    "            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "\n",
    "            if (ds != \"test\") and ((\"mlperf\" in filename) or (\"openai\" in filename)):\n",
    "                ignores.append(filepath)\n",
    "                continue\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arch\": arch,\n",
    "                    \"perm\": perm,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": filepath,\n",
    "                    \"node_feat_filepath\": str(\n",
    "                        node_feat_dir / arch / perm / ds / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_feat_filepath\": str(\n",
    "                        trans_node_feat_dir\n",
    "                        / \"layout\"\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_config_filepath\": str(\n",
    "                        trans_node_config_feat_dir\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "    dataset_dict[ds] = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for filepath in tqdm(ignores):\n",
    "#     node_config_feat = np.load(filepath)[\"node_config_feat\"]\n",
    "\n",
    "#     for i in range(1, node_config_feat.shape[0]):\n",
    "#         if not (node_config_feat[0] == node_config_feat[i]).all():\n",
    "#             filepath\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "      <th>cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats  \\\n",
       "0       0         1        19   \n",
       "1       1        54         6   \n",
       "\n",
       "                                                cats  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "1                                 [0, 1, 2, 3, 4, 5]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat = pd.DataFrame(\n",
    "    [\n",
    "        {\"number\": 0, \"num_dims\": 1, \"num_cats\": 19, \"cats\": list(range(19))},\n",
    "        {\"number\": 1, \"num_dims\": 54, \"num_cats\": 6, \"cats\": list(range(6))},\n",
    "    ]\n",
    ")\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats\n",
       "0       0        18         8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat_config = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"number\": 0,\n",
    "            \"num_dims\": 18,\n",
    "            \"num_cats\": 8,\n",
    "        },  # output_layout, input_layout, kernel_layout\n",
    "    ]\n",
    ")\n",
    "dfcat_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset_dict:\n",
    "    for i, row in dataset_dict[ds].iterrows():\n",
    "        np.load(row[\"filepath\"])\n",
    "        np.load(row[\"node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_config_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データクラスを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CatStatus:\n",
    "    dfcat: pd.DataFrame\n",
    "    prefix: str\n",
    "    num_cat_dict: dict[str, int] = field(init=False)\n",
    "    index_dict: dict[str, list[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.num_cat_dict, self.index_dict = {}, {}\n",
    "        dim_start = 0\n",
    "        for i, row in self.dfcat.iterrows():\n",
    "            self.num_cat_dict[f\"{self.prefix}cat_feat{i + 1}\"] = row[\"num_cats\"]\n",
    "            self.index_dict[f\"{self.prefix}cat_feat{i + 1}\"] = list(\n",
    "                range(dim_start, dim_start + row[\"num_dims\"])\n",
    "            )\n",
    "            dim_start += row[\"num_dims\"]\n",
    "\n",
    "\n",
    "cat_status = CatStatus(dfcat=dfcat, prefix=\"\")\n",
    "cat_config_status = CatStatus(dfcat=dfcat_config, prefix=\"config_\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Const:\n",
    "    num_node_flag_feat_dim: int\n",
    "    num_node_cont_feat_dim: int\n",
    "    num_node_cat_feat_dim: int\n",
    "    num_node_config_cont_feat_dim: int\n",
    "\n",
    "    # 演算子の種類\n",
    "    num_operations: int = 120\n",
    "    # 各configの次元数\n",
    "    num_config_dims: int = 6\n",
    "\n",
    "\n",
    "fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"node_feat_filepath\"])\n",
    "trans_fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"trans_node_feat_filepath\"])\n",
    "trans_config_fileobj = np.load(\n",
    "    dataset_dict[\"train\"].iloc[0][\"trans_node_config_filepath\"]\n",
    ")\n",
    "\n",
    "node_flag_feat, node_cont_feat = fileobj[\"node_flag_feat\"], fileobj[\"node_cont_feat\"]\n",
    "node_enum_feat, node_dimension_number_feat = (\n",
    "    fileobj[\"node_enum_feat\"],\n",
    "    fileobj[\"node_dimension_number_feat\"],\n",
    ")\n",
    "trans_node_cont_feat = trans_fileobj[\"node_feat\"]\n",
    "trans_node_config_cont_feat = trans_config_fileobj[\"node_config_cont_feat\"]\n",
    "const = Const(\n",
    "    num_node_flag_feat_dim=node_flag_feat.shape[1] + 1,  # config_idsの分+1\n",
    "    num_node_cont_feat_dim=node_cont_feat.shape[1] + trans_node_cont_feat.shape[1],\n",
    "    num_node_cat_feat_dim=node_enum_feat.shape[1] + node_dimension_number_feat.shape[1],\n",
    "    num_node_config_cont_feat_dim=trans_node_config_cont_feat.shape[2],\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NodeFeatExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope: float = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GNNExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CatEmbedding:\n",
    "    num_cat: int\n",
    "    embedding_dim: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    device: str\n",
    "    cat_embeddings: dict[str, CatEmbedding]\n",
    "    # num_random_samples: int = 30\n",
    "    # num_sampling: int = 33  # 1グラフ毎に1000件（テストと同じ）\n",
    "    # train_batch_size: int = 8\n",
    "    # batch_size: int = 30\n",
    "    num_random_samples: int = 15\n",
    "    num_sampling: int = 10  # 1グラフ毎に1000件（テストと同じ）\n",
    "    train_batch_size: int = 1\n",
    "    batch_size: int = 30\n",
    "    node_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    node_config_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    gnn_extractor: GNNExtractor = field(default_factory=lambda: GNNExtractor())\n",
    "    subgraph_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    epoch: int = 50\n",
    "    T_max: int = 50\n",
    "    eta_min: float = 1e-5\n",
    "    lr: float = 1e-2\n",
    "    weight_decay: float = 0\n",
    "\n",
    "\n",
    "cat_embeddings = {}\n",
    "cat_embeddings.update(\n",
    "    {\"op\": CatEmbedding(num_cat=const.num_operations, embedding_dim=16)}\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_config_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "params = Params(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    cat_embeddings=cat_embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LayoutConfigs:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    node_cont_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 108)\n",
    "\n",
    "    node_cat_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 3)\n",
    "\n",
    "    node_opcode: np.ndarray\n",
    "        ノード演算子、(ノード数,)\n",
    "    edge_index: np.ndarray\n",
    "        エッジ、(エッジ数, 2)\n",
    "\n",
    "    node_config_feat: np.ndarray\n",
    "        設定毎のノード特徴量、(設定数, 設定可能なノード数, 3)\n",
    "\n",
    "    node_config_ids: np.ndarray\n",
    "        設定可能なノードのIndex、(設定可能なノード数,)\n",
    "    config_runtime: np.ndarray\n",
    "        実行時間、(設定数,)\n",
    "    node_splits: np.ndarray\n",
    "        同じパーティションでの計算を意味する。今回は使用しない。(パーティション数, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    node_flag_feat: np.ndarray\n",
    "    node_cont_feat: np.ndarray\n",
    "    node_cat_feat: np.ndarray\n",
    "    node_opcode: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    node_config_feat: np.ndarray\n",
    "    node_config_cont_feat: np.ndarray\n",
    "    node_config_ids: np.ndarray\n",
    "    config_runtime: np.ndarray\n",
    "    node_splits: np.ndarray\n",
    "\n",
    "    cat_status: CatStatus\n",
    "    cat_config_status: CatStatus\n",
    "    target: np.ndarray = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # 設定が存在するノードのフラグ\n",
    "        node_active_feat = np.zeros((self.num_nodes, 1))\n",
    "        node_active_feat[self.node_config_ids, :] = 1\n",
    "        self.node_flag_feat = np.concatenate(\n",
    "            [self.node_flag_feat, node_active_feat], axis=1\n",
    "        )\n",
    "        self.node_cont_feat = self.apply_normalization(x=self.node_cont_feat)\n",
    "        self.node_config_feat = self.node_config_feat + 1  # カテゴリは0~7にする\n",
    "        self.node_splits = np.array(\n",
    "            [\n",
    "                [self.node_splits[0][i], self.node_splits[0][i + 1] - 1]\n",
    "                for i in range(self.node_splits.shape[1] - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.target = self.apply_target_normalization(x=self.config_runtime)\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        \"\"\"ノード数\"\"\"\n",
    "        return self.node_cont_feat.shape[0]\n",
    "\n",
    "    def get_random_config_idx(self, num: int) -> list[int]:\n",
    "        \"\"\"ランダムな設定をサンプルする\"\"\"\n",
    "        num_ = min(self.config_runtime.shape[0], num)\n",
    "        return random.sample(list(range(self.config_runtime.shape[0])), num_)\n",
    "\n",
    "    def get_filled_node_config_feat(\n",
    "        self, index_list: list[int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"指定された設定の設定毎のノード特徴量を取得する。設定がない場合は補完する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray [(len(index_list),ノード数, 18), (len(index_list),ノード数, 連続次元数)]\n",
    "        \"\"\"\n",
    "        # (サンプル数, ノード数) x 3\n",
    "        node_config_feat = np.full(\n",
    "            (len(index_list), self.num_nodes, Const.num_config_dims * 3),\n",
    "            Const.num_config_dims + 1,\n",
    "        )\n",
    "        node_config_feat[:, self.node_config_ids] = self.node_config_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "\n",
    "        node_config_cont_feat = np.zeros(\n",
    "            (len(index_list), self.num_nodes, self.node_config_cont_feat.shape[2])\n",
    "        )\n",
    "        node_config_cont_feat[:, self.node_config_ids] = self.node_config_cont_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "        return node_config_feat, node_config_cont_feat\n",
    "\n",
    "    def get_target(self, index_list: list[int]) -> np.ndarray:\n",
    "        \"\"\"指定された設定の目的変数を取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "        \"\"\"\n",
    "        return self.apply_target_ranking(x=self.config_runtime[index_list])\n",
    "\n",
    "    def apply_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"特徴量の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            2次元行列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            行方向に正規化された行列\n",
    "        \"\"\"\n",
    "        return np.log1p(x / 128)\n",
    "\n",
    "    def apply_target_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"目的変数の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            ベクトル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            正規化されたベクトル\n",
    "        \"\"\"\n",
    "        return np.log(x / x.min())\n",
    "\n",
    "    def apply_target_ranking(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"降順でランキング\"\"\"\n",
    "        return np.argsort(np.argsort(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayoutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    rows: list[dict[str, np.ndarray]]\n",
    "        設定をリストでもつ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: pd.DataFrame,\n",
    "        params: Params,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        self.rows = dataset.to_dict(\"records\")\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "        self.cache_idx = None\n",
    "        self.cache_filepath = None\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return self.params.device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def create_layout_config(self, idx: int) -> LayoutConfigs:\n",
    "        if self.cache_idx != idx:\n",
    "            self.cache_idx = idx\n",
    "            fileobj = np.load(self.rows[self.cache_idx][\"filepath\"])\n",
    "            node_feat_fileobj = np.load(self.rows[self.cache_idx][\"node_feat_filepath\"])\n",
    "            trans_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_feat_filepath\"]\n",
    "            )\n",
    "            trans_config_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_config_filepath\"]\n",
    "            )\n",
    "\n",
    "            node_cont_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_cont_feat\"],\n",
    "                    trans_feat_fileobj[\"node_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            node_cat_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_enum_feat\"],\n",
    "                    node_feat_fileobj[\"node_dimension_number_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            self.cache_layout_config = LayoutConfigs(\n",
    "                node_opcode=fileobj[\"node_opcode\"],\n",
    "                edge_index=fileobj[\"edge_index\"],\n",
    "                node_config_ids=fileobj[\"node_config_ids\"],\n",
    "                config_runtime=fileobj[\"config_runtime\"],\n",
    "                node_splits=fileobj[\"node_splits\"],\n",
    "                node_flag_feat=node_feat_fileobj[\"node_flag_feat\"],\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=fileobj[\"node_config_feat\"],\n",
    "                node_config_cont_feat=trans_config_feat_fileobj[\n",
    "                    \"node_config_cont_feat\"\n",
    "                ],\n",
    "                cat_status=self.cat_status,\n",
    "                cat_config_status=self.cat_config_status,\n",
    "            )\n",
    "        return self.cache_layout_config\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"ランダムな設定を取得する\"\"\"\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "        config_index_list = layout_configs.get_random_config_idx(\n",
    "            num=self.params.num_random_samples\n",
    "        )\n",
    "        return self._get_tensors(\n",
    "            layout_configs=layout_configs, index_list=config_index_list\n",
    "        )\n",
    "\n",
    "    def getitem_as_batch(\n",
    "        self, idx: int\n",
    "    ) -> list[\n",
    "        tuple[\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "        ]\n",
    "    ]:\n",
    "        \"\"\"設定をバッチで取得する\"\"\"\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "\n",
    "        index_list = list(range(layout_configs.config_runtime.shape[0]))\n",
    "        for i_chunk in range(0, len(index_list), self.params.batch_size):\n",
    "            chunk_index_list = index_list[i_chunk : i_chunk + self.params.batch_size]\n",
    "            yield self._get_tensors(\n",
    "                layout_configs=layout_configs, index_list=chunk_index_list\n",
    "            )\n",
    "\n",
    "    def _get_tensors(\n",
    "        self, layout_configs: LayoutConfigs, index_list: list[int]\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"渡された設定のIndexのテンソルを取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layout_configs: LayoutConfigs\n",
    "            Layoutのデータクラス\n",
    "        index_list: list[int]\n",
    "            設定のインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            ノード特徴量(フラグ)\n",
    "        torch.Tensor\n",
    "            ノード特徴量(連続)\n",
    "        dict[str, torch.Tensor]\n",
    "            ノード特徴量(カテゴリ)\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量(連続)\n",
    "        torch.Tensor\n",
    "            ノード演算子\n",
    "        torch.Tensor\n",
    "            エッジ\n",
    "        torch.Tensor\n",
    "            目的変数\n",
    "        \"\"\"\n",
    "        # ノード特徴量(フラグ)\n",
    "        node_flag_feat = torch.tensor(\n",
    "            layout_configs.node_flag_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(連続)\n",
    "        node_cont_feat = torch.tensor(\n",
    "            layout_configs.node_cont_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(カテゴリ)\n",
    "        node_cat_feat = torch.tensor(\n",
    "            layout_configs.node_cat_feat,\n",
    "            dtype=torch.int64,\n",
    "        ).to(self.device)\n",
    "        # 設定毎のノード特徴量(カテゴリ)\n",
    "        (\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "        ) = layout_configs.get_filled_node_config_feat(index_list=index_list)\n",
    "        node_config_feat = torch.tensor(node_config_feat, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        node_config_cont_feat = torch.tensor(\n",
    "            node_config_cont_feat, dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        # ノード演算子\n",
    "        node_opcode = torch.tensor(layout_configs.node_opcode, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # エッジ\n",
    "        edge_index = torch.tensor(\n",
    "            np.swapaxes(layout_configs.edge_index, 0, 1), dtype=torch.int64\n",
    "        ).to(self.device)\n",
    "        # サブグラフ\n",
    "        node_splits = torch.tensor(layout_configs.node_splits, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # ターゲット\n",
    "        target = torch.tensor(\n",
    "            layout_configs.get_target(index_list=index_list),\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        )\n",
    "\n",
    "    def get_ith_file_info(self, i: int) -> dict[str, str]:\n",
    "        row = self.rows[i]\n",
    "        return {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "        }\n",
    "\n",
    "    def get_ith_runtime(self, i: int) -> np.ndarray:\n",
    "        layout_configs = self.create_layout_config(idx=i)\n",
    "        return layout_configs.config_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "\n",
    "class EdgeConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    ノード特徴 + 隣接ノード特徴 + 隣接ノード特徴の一致\n",
    "    参考： https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html#implementing-the-edge-convolution\n",
    "    補足: 集約関数はデフォルトでdim(axis) = -2。つまりノード方向で集約するので気にしなくてOK\n",
    "    https://github.com/pyg-team/pytorch_geometric/blob/1e12d41c28b1fb9793f17646b018071b508864d7/torch_geometric/nn/aggr/basic.py#L38\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_input_dim: int, x_output_dim: int, dropout_p: float):\n",
    "        # \"Add\" aggregation\n",
    "        super().__init__(aggr=\"max\")\n",
    "        self.mlp = nn.Sequential(\n",
    "            # nn.LayerNorm(x_input_dim * 2),\n",
    "            nn.Linear(x_input_dim * 2, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.ReLU(),\n",
    "            # nn.LayerNorm(x_output_dim),\n",
    "            nn.Linear(x_output_dim, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [設定数, N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"propagate()で渡された引数xから自動でx_i, x_jノードを取り出して随時処理を実装する関数\"\"\"\n",
    "        # x_i has shape [設定数, エッジ数, in_channels]\n",
    "        # x_j has shape [設定数, エッジ数, in_channels]\n",
    "        x_cat = torch.cat(\n",
    "            [x_i, x_i - x_j], dim=2\n",
    "        )  # tmp has shape [設定数, エッジ数, 2 * in_channels]\n",
    "        return self.mlp(x_cat)\n",
    "\n",
    "\n",
    "class SimpleLayoutModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params: Params\n",
    "        実験設定のデータクラス\n",
    "    node_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(ノード毎)\n",
    "    node_config_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(設定xノード毎)\n",
    "    node_feat_extractor: torch.nn.Module\n",
    "        ノードの特徴量を抽出するネットワーク\n",
    "    gnn_extractor: torch.nn.Module\n",
    "        グラフの特徴量を抽出するネットワーク\n",
    "    gc: torch.nn.Module\n",
    "        最終層の全結合層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Params,\n",
    "        const: Const,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "\n",
    "        # カテゴリ変数の埋め込み表現\n",
    "        self.embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                k: torch.nn.Embedding(v.num_cat, v.embedding_dim)\n",
    "                for k, v in self.params.cat_embeddings.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # node_featのfeature_extractorを定義\n",
    "        num_node_feat_extractor_input_dim = (\n",
    "            const.num_node_flag_feat_dim\n",
    "            + const.num_node_cont_feat_dim\n",
    "            + self.num_node_feat_embedding_dims\n",
    "        )\n",
    "\n",
    "        node_feat_extractor_layer = []\n",
    "        node_feat_extractor_dims = [\n",
    "            num_node_feat_extractor_input_dim\n",
    "        ] + self.params.node_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_feat_extractor_dims[i],\n",
    "                    out_features=node_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(params.node_feat_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "            self.node_feat_extractor = nn.Sequential(*node_feat_extractor_layer)\n",
    "\n",
    "        # node_config_featのfeature_extractorを定義\n",
    "        num_node_config_feat_extractor_input_dim = (\n",
    "            self.num_node_config_feat_embedding_dims\n",
    "            + const.num_node_config_cont_feat_dim\n",
    "        )\n",
    "\n",
    "        node_config_feat_extractor_layer = []\n",
    "        node_config_feat_extractor_dims = [\n",
    "            num_node_config_feat_extractor_input_dim\n",
    "        ] + self.params.node_config_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_config_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_config_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_config_feat_extractor_dims[i],\n",
    "                    out_features=node_config_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_config_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(\n",
    "                    params.node_config_feat_extractor.leakyrelu_negative_slope\n",
    "                ),\n",
    "            ]\n",
    "        self.node_config_feat_extractor = nn.Sequential(\n",
    "            *node_config_feat_extractor_layer\n",
    "        )\n",
    "\n",
    "        # ノード間のfeature_extractorの定義\n",
    "        num_gnn_extractor_input_dim = (\n",
    "            node_feat_extractor_dims[-1] + node_config_feat_extractor_dims[-1]\n",
    "        )\n",
    "\n",
    "        gnn_extractor_layer = []\n",
    "        gnn_extractor_dims = [\n",
    "            num_gnn_extractor_input_dim\n",
    "        ] + self.params.gnn_extractor.dims\n",
    "        for i in range(len(gnn_extractor_dims) - 1):\n",
    "            gnn_extractor_layer += [\n",
    "                (\n",
    "                    EdgeConv(\n",
    "                        x_input_dim=gnn_extractor_dims[i],\n",
    "                        x_output_dim=gnn_extractor_dims[i + 1],\n",
    "                        dropout_p=params.gnn_extractor.dropout_p,\n",
    "                    ),\n",
    "                    \"x, edge_index -> x\",\n",
    "                ),\n",
    "                nn.LeakyReLU(params.gnn_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "        self.gnn_extractor = Sequential(\"x, edge_index\", gnn_extractor_layer)\n",
    "\n",
    "        # サブグラフのfeature_extractorの定義\n",
    "        # num_subgraph_extractor_input_dim = (\n",
    "        #     self.params.gnn_extractor.dims[-1] + num_gnn_extractor_input_dim\n",
    "        # )\n",
    "\n",
    "        # subgraph_extractor_layer = []\n",
    "        # subgraph_extractor_dims = [\n",
    "        #     num_subgraph_extractor_input_dim\n",
    "        # ] + self.params.node_feat_extractor.dims\n",
    "        # for i in range(len(subgraph_extractor_dims) - 1):\n",
    "        #     subgraph_extractor_layer += [\n",
    "        #         # nn.LayerNorm(subgraph_extractor_dims[i]),\n",
    "        #         nn.Linear(\n",
    "        #             in_features=subgraph_extractor_dims[i],\n",
    "        #             out_features=subgraph_extractor_dims[i + 1],\n",
    "        #         ),\n",
    "        #         # nn.Dropout(params.subgraph_extractor.dropout_p),\n",
    "        #         nn.LeakyReLU(params.subgraph_extractor.leakyrelu_negative_slope),\n",
    "        #     ]\n",
    "        # self.subgraph_extractor = nn.Sequential(*subgraph_extractor_layer)\n",
    "\n",
    "        fc_layer = [\n",
    "            # nn.LayerNorm(subgraph_extractor_dims[-1]),\n",
    "            # nn.Linear(in_features=subgraph_extractor_dims[-1], out_features=1),\n",
    "            nn.Linear(\n",
    "                in_features=self.params.gnn_extractor.dims[-1]\n",
    "                + num_gnn_extractor_input_dim,\n",
    "                out_features=1,\n",
    "            ),\n",
    "        ]\n",
    "        self.fc = nn.Sequential(*fc_layer)\n",
    "        self.to(self.params.device)\n",
    "\n",
    "    @property\n",
    "    def num_node_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        num_embedding_dims += 1 * self.params.cat_embeddings[\"op\"].embedding_dim\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    @property\n",
    "    def num_node_config_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "        node_config_feat: torch.Tensor,\n",
    "        node_config_cont_feat: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        node_splits: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------\n",
    "        node_flag_feat:\n",
    "            ノードの特徴量(node数, フラグ次元数)\n",
    "        node_cont_feat:\n",
    "            ノードの特徴量(node数, 連続次元数)\n",
    "        node_cat_feat:\n",
    "            ノードの特徴量(node数, カテゴリ次元数*埋め込み次元数)\n",
    "        node_config_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 特徴次元数)\n",
    "        node_config_cont_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 連続次元数)\n",
    "        edge_index:\n",
    "            エッジ(2, エッジ数)\n",
    "        node_splits:\n",
    "            サブグラフのインデックス（サブグラフ数, 2)\n",
    "\n",
    "        Returns:\n",
    "        torch.tensor: (設定数)\n",
    "        \"\"\"\n",
    "        # (ノード数,特徴数)のテンソルを作成\n",
    "        node_feat = self._join_node_feature(\n",
    "            node_opcode=node_opcode,\n",
    "            node_flag_feat=node_flag_feat,\n",
    "            node_cont_feat=node_cont_feat,\n",
    "            node_cat_feat=node_cat_feat,\n",
    "        )\n",
    "\n",
    "        # (設定数,ノード数,特徴数)のテンソルを作成\n",
    "        node_config_feat = self._join_node_config_feature(\n",
    "            node_config_feat=node_config_feat,\n",
    "            node_config_cont_feat=node_config_cont_feat,\n",
    "        )\n",
    "\n",
    "        # node_featの抽出器を通す\n",
    "        extracted_node_feat = self.node_feat_extractor(node_feat)\n",
    "\n",
    "        # node_config_featの抽出器を通す\n",
    "        extracted_node_config_feat = self.node_config_feat_extractor(node_config_feat)\n",
    "\n",
    "        # 設定毎のノード特徴に結合する\n",
    "        extracted_feat = self._join_entire_node_config_feat(\n",
    "            node_feat=extracted_node_feat,\n",
    "            node_config_feat=extracted_node_config_feat,\n",
    "        )\n",
    "\n",
    "        # GNN抽出器を通す\n",
    "        conved_extracted_feat = self.gnn_extractor(\n",
    "            x=extracted_feat,\n",
    "            edge_index=edge_index,\n",
    "        )\n",
    "\n",
    "        # 残差を足すイメージ\n",
    "        concat_feat = torch.concat([extracted_feat, conved_extracted_feat], 2)\n",
    "\n",
    "        # subgraph_global_pool_feat_list = []\n",
    "        # for subgraph_start_node_idx, subgraph_end_node_idx in node_splits:\n",
    "        #     subgraph_concat_feat = concat_feat[\n",
    "        #         :, subgraph_start_node_idx : subgraph_end_node_idx + 1, :\n",
    "        #     ]\n",
    "        #     # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        #     subgraph_global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "        #     subgraph_global_pool_feat_list.append(\n",
    "        #         torch.reshape(\n",
    "        #             subgraph_global_pool_feat,\n",
    "        #             (\n",
    "        #                 subgraph_global_pool_feat.shape[0],\n",
    "        #                 1,\n",
    "        #                 subgraph_global_pool_feat.shape[1],\n",
    "        #             ),\n",
    "        #         )\n",
    "        #     )\n",
    "        # # （設定数,サブグラフ数,特徴数)\n",
    "        # subgraph_global_pool_feat = torch.concat(subgraph_global_pool_feat_list, 1)\n",
    "        # subgraph_extracted_feat = self.subgraph_extractor(subgraph_global_pool_feat)\n",
    "\n",
    "        # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        # global_pool_feat = torch.mean(subgraph_extracted_feat, dim=1)\n",
    "        global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "\n",
    "        return torch.squeeze(self.fc(global_pool_feat))\n",
    "\n",
    "    def _join_node_feature(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_featのテンソルを作成\"\"\"\n",
    "        # ノードの埋め込み表現\n",
    "        node_embeddings_list = []\n",
    "        node_embeddings_list.append(self.embeddings[\"op\"](node_opcode))\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](node_cat_feat[:, cat_index])\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (-1, node_embeddings.shape[-2] * node_embeddings.shape[-1]),\n",
    "            )\n",
    "            node_embeddings_list.append(node_embeddings)\n",
    "\n",
    "        # ノード毎で埋め込み、結合(ノード数, 特徴数)\n",
    "        node_embedding_feat = torch.concat(node_embeddings_list, 1)\n",
    "        node_feat = torch.concat(\n",
    "            [node_flag_feat, node_cont_feat, node_embedding_feat], 1\n",
    "        )\n",
    "        return node_feat\n",
    "\n",
    "    def _join_node_config_feature(\n",
    "        self, node_config_feat: torch.Tensor, node_config_cont_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_config_featのテンソルを作成\"\"\"\n",
    "        # 設定xノード毎で埋め込み(設定数, ノード数, 特徴数)\n",
    "        node_config_embeddings_list = []\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](\n",
    "                node_config_feat[:, :, cat_index]\n",
    "            )\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (\n",
    "                    node_embeddings.shape[0],\n",
    "                    -1,\n",
    "                    node_embeddings.shape[-2] * node_embeddings.shape[-1],\n",
    "                ),\n",
    "            )\n",
    "            node_config_embeddings_list.append(node_embeddings)\n",
    "        node_config_feat = torch.concat(\n",
    "            node_config_embeddings_list + [node_config_cont_feat], 2\n",
    "        )\n",
    "        return node_config_feat\n",
    "\n",
    "    def _join_entire_node_config_feat(\n",
    "        self, node_feat: torch.Tensor, node_config_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # ノード毎の特徴量を設定数だけ縦に並べる\n",
    "        node_tiled_feat = torch.tile(\n",
    "            torch.reshape(node_feat, (1, node_feat.shape[0], node_feat.shape[1])),\n",
    "            (node_config_feat.shape[0], 1, 1),\n",
    "        )\n",
    "        return torch.concat([node_tiled_feat, node_config_feat], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ListMLE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.Tensor\n",
    "            予測（要素数, ）\n",
    "        labels: torch.Tensor\n",
    "            目的変数（要素数, ）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        # 正解をソート\n",
    "        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n",
    "        # 予測を正解順でソート\n",
    "        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n",
    "        # 予測値の最大値で予測値を引く（expの爆発予防）\n",
    "        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n",
    "        logits_sorted_by_true = logits_sorted_by_true - logits_max\n",
    "        # ランキングが低いものから累積する(その後正解順に戻す)\n",
    "        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n",
    "            dims=[1]\n",
    "        )\n",
    "        # 誤差\n",
    "        negative_log_likelihood = torch.sum(\n",
    "            torch.log(cumsums) - logits_sorted_by_true, dim=1\n",
    "        )\n",
    "        return torch.mean(negative_log_likelihood)\n",
    "\n",
    "\n",
    "def rankNet(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    # here we generate every pair of indices from the range of document length in the batch\n",
    "    document_pairs_candidates = list(\n",
    "        itertools.product(range(y_true.shape[1]), repeat=2)\n",
    "    )\n",
    "\n",
    "    pairs_true = y_true[:, document_pairs_candidates]\n",
    "    selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "    # here we calculate the relative true relevance of every candidate pair\n",
    "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "    # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
    "    # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
    "    # positive ones for a simpler loss function formulation\n",
    "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "\n",
    "    pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "    weight = None\n",
    "    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
    "    # whether one document is better than the other and not about the actual difference in\n",
    "    # their relevancy levels\n",
    "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "    true_diffs = true_diffs[the_mask]\n",
    "\n",
    "    return nn.BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)\n",
    "\n",
    "\n",
    "def to_cpu_numpy(\n",
    "    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if params.device == \"cuda\":\n",
    "        pred_ = pred.cpu().detach().numpy()\n",
    "        truth_ = truth.cpu().detach().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pred_ = pred.detach().numpy()\n",
    "        truth_ = truth.detach().numpy()\n",
    "    return pred_, truth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "def evaluate_score(dataset: LayoutDataset, model: torch.nn.Module) -> pd.DataFrame:\n",
    "    \"\"\"データセット全件に対してコンペの評価指標を算出する\n",
    "    https://www.kaggle.com/competitions/predict-ai-model-runtime/overview\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    records = []\n",
    "    # 各グラフ毎にスコアを算出\n",
    "    for i in range(len(dataset)):\n",
    "        # グラフ毎に複数サンプル\n",
    "        preds, truths = [], []\n",
    "        current_loss = 0\n",
    "        for _ in range(params.num_sampling):\n",
    "            (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            ) = dataset[i]\n",
    "            pred = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            # loss = criterion(\n",
    "            #     torch.reshape(pred, (1, pred.shape[0])),\n",
    "            #     torch.reshape(target, (1, target.shape[0])),\n",
    "            # )\n",
    "            loss = rankNet(\n",
    "                torch.reshape(pred, (1, pred.shape[0])),\n",
    "                torch.reshape(target, (1, target.shape[0])),\n",
    "            )\n",
    "            current_loss += loss.item()\n",
    "            pred, truth = to_cpu_numpy(params, pred, target)\n",
    "            preds.append(pred)\n",
    "            truths.append(truth)\n",
    "        preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "        current_loss /= params.num_sampling\n",
    "\n",
    "        score = kendalltau(truth, pred).correlation\n",
    "        record = dataset.get_ith_file_info(i)\n",
    "        record.update(\n",
    "            {\n",
    "                \"current_loss\": current_loss,\n",
    "                \"score\": score,\n",
    "            }\n",
    "        )\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_indexes_as_batch(num_data: int, batch_size: int) -> list[list[int]]:\n",
    "    random_indexes = random.sample(list(range(num_data)), num_data)\n",
    "    batches = []\n",
    "    batch_random_indexes = []\n",
    "    for i, index in enumerate(random_indexes):\n",
    "        batch_random_indexes.append(index)\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            batches.append(batch_random_indexes)\n",
    "            batch_random_indexes = []\n",
    "    if len(batch_random_indexes) > 0:\n",
    "        batches.append(\n",
    "            batch_random_indexes\n",
    "            + random_indexes[: (batch_size - len(batch_random_indexes))]\n",
    "        )\n",
    "    return batches\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    dftrain: pd.DataFrame,\n",
    "    dfvalid: pd.DataFrame,\n",
    "    params: Params,\n",
    "    const: Const,\n",
    "    cat_status: CatStatus,\n",
    "    cat_config_status: CatStatus,\n",
    "    savedir: Path,\n",
    "    checkpoint_dir: Path = None,\n",
    ") -> None:\n",
    "    train_layout_dataset = LayoutDataset(\n",
    "        dataset=dftrain,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    valid_layout_dataset = LayoutDataset(\n",
    "        dataset=dfvalid,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "\n",
    "    model = SimpleLayoutModel(\n",
    "        params=params,\n",
    "        const=const,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    if checkpoint_dir is not None:\n",
    "        print(\"学習済みモデルを読み込みます\")\n",
    "        model.load_state_dict(torch.load(checkpoint_dir / f\"final_model.pt\"))\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n",
    "    )\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    best_score = -np.inf\n",
    "    records = []\n",
    "    for epoch in range(params.epoch):\n",
    "        model.train()\n",
    "\n",
    "        num_data = len(train_layout_dataset)\n",
    "        pbar = tqdm(range(num_data))\n",
    "        batch_indexes_list = get_random_indexes_as_batch(\n",
    "            num_data=num_data, batch_size=params.train_batch_size\n",
    "        )\n",
    "        num_batch = len(batch_indexes_list)\n",
    "        num_total_data = sum(\n",
    "            [len(batch_indexes) for batch_indexes in batch_indexes_list]\n",
    "        )\n",
    "        i_total_data = 1\n",
    "\n",
    "        running_losses = []\n",
    "        for i_batch, batch_indexes in enumerate(batch_indexes_list):\n",
    "            # 各グラフのループ\n",
    "            for i_data in batch_indexes:\n",
    "                # グラフの中で複数サンプルする\n",
    "                preds, truths = [], []\n",
    "                current_loss = 0\n",
    "                for _ in range(params.num_sampling):\n",
    "                    (\n",
    "                        node_opcode,\n",
    "                        node_flag_feat,\n",
    "                        node_cont_feat,\n",
    "                        node_cat_feat,\n",
    "                        node_config_feat,\n",
    "                        node_config_cont_feat,\n",
    "                        edge_index,\n",
    "                        node_splits,\n",
    "                        target,\n",
    "                    ) = train_layout_dataset[i_data]\n",
    "                    out = model(\n",
    "                        node_opcode=node_opcode,\n",
    "                        node_flag_feat=node_flag_feat,\n",
    "                        node_cont_feat=node_cont_feat,\n",
    "                        node_cat_feat=node_cat_feat,\n",
    "                        node_config_feat=node_config_feat,\n",
    "                        node_config_cont_feat=node_config_cont_feat,\n",
    "                        edge_index=edge_index,\n",
    "                        node_splits=node_splits,\n",
    "                    )\n",
    "\n",
    "                    # loss = criterion(\n",
    "                    #     torch.reshape(out, (1, out.shape[0])),\n",
    "                    #     torch.reshape(target, (1, target.shape[0])),\n",
    "                    # )\n",
    "                    loss = rankNet(\n",
    "                        torch.reshape(out, (1, out.shape[0])),\n",
    "                        torch.reshape(target, (1, target.shape[0])),\n",
    "                    )\n",
    "                    current_loss += loss.item()\n",
    "                    # loss /= (params.num_sampling * params.train_batch_size)\n",
    "                    loss.backward()\n",
    "\n",
    "                    pred, truth = to_cpu_numpy(params, out, target)\n",
    "                    preds.append(pred)\n",
    "                    truths.append(truth)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step(epoch + i_total_data / num_total_data)\n",
    "                optimizer.zero_grad()\n",
    "                i_total_data += 1\n",
    "\n",
    "                preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "                score = kendalltau(truth, pred).correlation\n",
    "                current_loss /= params.num_sampling\n",
    "                running_losses.append(current_loss)\n",
    "                running_loss = np.mean(running_losses)\n",
    "\n",
    "                record = {\"epoch\": epoch, \"batch\": i_batch}\n",
    "                record.update(train_layout_dataset.get_ith_file_info(i_data))\n",
    "                record.update(\n",
    "                    {\n",
    "                        \"train/running_loss\": running_loss,\n",
    "                        \"train/current_loss\": current_loss,\n",
    "                        \"train/score\": score,\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    }\n",
    "                )\n",
    "                records.append(record)\n",
    "                wandb.log(record)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f\"running loss: {running_loss:.5f}, current loss: {current_loss:.5f} score: {score:.3f}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "            # optimizer.step()\n",
    "            # scheduler.step(epoch + i_batch / num_batch)\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        dfscore = evaluate_score(dataset=valid_layout_dataset, model=model)\n",
    "        avg_loss = dfscore[\"current_loss\"].mean()\n",
    "        avg_score = dfscore[\"score\"].mean()\n",
    "        for _, row_score in dfscore.iterrows():\n",
    "            record = {\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": -1,\n",
    "                \"arch\": row_score[\"arch\"],\n",
    "                \"perm\": row_score[\"perm\"],\n",
    "                \"filename\": row_score[\"filename\"],\n",
    "                \"valid/running_loss\": avg_loss,\n",
    "                \"valid/current_loss\": row_score[\"current_loss\"],\n",
    "                \"valid/score\": row_score[\"score\"],\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            records.append(record)\n",
    "            wandb.log(record)\n",
    "\n",
    "        print(f\"[valid] current loss: {avg_loss:.5f} score: {avg_score:.3f}\")\n",
    "\n",
    "        if best_score < avg_score:\n",
    "            best_score = avg_score\n",
    "            torch.save(model.state_dict(), savedir / \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n",
    "\n",
    "    dflog = pd.DataFrame(records)\n",
    "    dflog.to_csv(savedir / \"log.csv\", index=False)\n",
    "\n",
    "    torch.save(model.state_dict(), savedir / \"final_model.pt\")\n",
    "\n",
    "    del (\n",
    "        train_layout_dataset,\n",
    "        valid_layout_dataset,\n",
    "        model,\n",
    "        optimizer,\n",
    "        dfscore,\n",
    "        dflog,\n",
    "        records,\n",
    "    )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzuuuubo-tetsu\u001b[0m (\u001b[33msun-scan-clan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yamaguchi/kaggle/experiments/1028-nlp-default/wandb/run-20231028_184013-u5sdwd58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/u5sdwd58' target=\"_blank\">1028-nlp-default</a></strong> to <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/u5sdwd58' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/u5sdwd58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/u5sdwd58?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f48619a8a50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch, perm = \"nlp\", \"default\"\n",
    "exptname = str(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"params\": asdict(params),\n",
    "        \"const\": asdict(const),\n",
    "        \"arch\": arch,\n",
    "        \"perm\": perm,\n",
    "        \"validation\": \"hold-out\",\n",
    "    },\n",
    "    name=exptname,\n",
    "    tags=[arch, perm],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e60227cfbc44509dd1e92fed11af73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.80348 score: 0.085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aa3373f02142b189e490310d424f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dftrain = dataset_dict[\"train\"].query(f\"(arch == @arch) & (perm == @perm)\")\n",
    "dfvalid = dataset_dict[\"valid\"].query(f\"(arch == @arch) & (perm == @perm)\")\n",
    "params_opt = deepcopy(params)\n",
    "\n",
    "savedir = workdir / f\"{arch}-{perm}\"\n",
    "savedir.mkdir(exist_ok=True, parents=True)\n",
    "train_model(\n",
    "    dftrain=dftrain,\n",
    "    dfvalid=dfvalid,\n",
    "    params=params_opt,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    "    savedir=savedir,\n",
    "    checkpoint_dir=None,\n",
    ")\n",
    "wandb.alert(title=exptname, text=f\"Train End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_layout_dataset = LayoutDataset(\n",
    "    dataset=dftrain,\n",
    "    params=params,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "\n",
    "model = SimpleLayoutModel(\n",
    "    params=params,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "# model.load_state_dict(torch.load(workdir / \"nlp-default\" / f\"_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    node_opcode,\n",
    "    node_flag_feat,\n",
    "    node_cont_feat,\n",
    "    node_cat_feat,\n",
    "    node_config_feat,\n",
    "    node_config_cont_feat,\n",
    "    edge_index,\n",
    "    node_splits,\n",
    "    target,\n",
    ") = train_layout_dataset[1]\n",
    "out = model(\n",
    "    node_opcode=node_opcode,\n",
    "    node_flag_feat=node_flag_feat,\n",
    "    node_cont_feat=node_cont_feat,\n",
    "    node_cat_feat=node_cat_feat,\n",
    "    node_config_feat=node_config_feat,\n",
    "    node_config_cont_feat=node_config_cont_feat,\n",
    "    edge_index=edge_index,\n",
    "    node_splits=node_splits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 11.,  9., 14.,  4.,  0.,  5., 13.,  3., 10.,  1.,  8.,  2., 12.,\n",
       "         7.], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0224, 0.0327, 0.0379, 0.0143, 0.0296, 0.0378, 0.0366, 0.0237, 0.0260,\n",
       "        0.0363, 0.0382, 0.0366, 0.0368, 0.0314, 0.0242], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yamaguchi/kaggle/experiments/1028-nlp-default/out/nlp-default/log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yamaguchi/kaggle/experiments/1028-nlp-default/nlp-default.ipynb セル 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1028-nlp-default/nlp-default.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m savedir \u001b[39m=\u001b[39m workdir \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00march\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mperm\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1028-nlp-default/nlp-default.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dflog \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(savedir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mlog.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1028-nlp-default/nlp-default.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1028-nlp-default/nlp-default.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, ds \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yamaguchi/kaggle/experiments/1028-nlp-default/out/nlp-default/log.csv'"
     ]
    }
   ],
   "source": [
    "savedir = workdir / f\"{arch}-{perm}\"\n",
    "dflog = pd.read_csv(savedir / \"log.csv\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for i, ds in enumerate([\"train\", \"valid\"]):\n",
    "    dflog_ = dflog.query(\"(phase == @ds)\").groupby(\"epoch\")\n",
    "    axes[i][0].plot(dflog_[\"current_loss\"].mean(), label=\"total\")\n",
    "    axes[i][1].plot(dflog_[\"score\"].mean(), label=\"total\")\n",
    "    if i == 0:\n",
    "        axes[i][0].legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = workdir / f\"{arch}-{perm}\"\n",
    "\n",
    "records = []\n",
    "\n",
    "dftest = dataset_dict[\"test\"].query(f\"(arch == @arch) & (perm == @perm)\")\n",
    "params_opt = deepcopy(params)\n",
    "if arch == \"nlp\":\n",
    "    params_opt.num_random_samples = 30\n",
    "    params_opt.batch_size = 30\n",
    "\n",
    "test_layout_dataset = LayoutDataset(\n",
    "    dataset=dftest,\n",
    "    params=params_opt,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model = SimpleLayoutModel(\n",
    "    params=params_opt,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model.load_state_dict(torch.load(savedir / f\"final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "with tqdm(range(len(test_layout_dataset))) as pbar:\n",
    "    for i in pbar:\n",
    "        file_info = test_layout_dataset.get_ith_file_info(i)\n",
    "\n",
    "        pred_list = []\n",
    "        for (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        ) in test_layout_dataset.getitem_as_batch(i):\n",
    "            pred_batch = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            if params_opt.device == \"cuda\":\n",
    "                pred_batch = pred_batch.cpu().detach().numpy()\n",
    "            else:\n",
    "                pred_batch = pred_batch.detach().numpy()\n",
    "            # pred_batchは高いものほどよい\n",
    "            pred_batch = -pred_batch\n",
    "            pred_list.append(pred_batch)\n",
    "\n",
    "            del (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            )\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pred = np.hstack(pred_list)\n",
    "\n",
    "        ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n",
    "        records.append({\"ID\": ID, \"pred\": \";\".join(list(map(str, pred.argsort())))})\n",
    "\n",
    "del test_layout_dataset, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dfpred = pd.DataFrame(records)\n",
    "dfsub = pd.read_csv(inputdir / \"sample_submission.csv\")\n",
    "dfsub = dfsub.merge(dfpred, on=\"ID\", how=\"left\")\n",
    "dfsub[\"TopConfigs\"] = np.where(\n",
    "    dfsub[\"pred\"].isnull(), dfsub[\"TopConfigs\"], dfsub[\"pred\"]\n",
    ")\n",
    "dfsub[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission_final_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.alert(title=exptname, text=f\"Inference End\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
