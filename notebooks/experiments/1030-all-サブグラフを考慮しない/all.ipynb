{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "sns.set()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n",
    "node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fast-layout-7-85\"\n",
    "trans_node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout6-92-dataset\"\n",
    "trans_node_config_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout7-81-dataset\"\n",
    "workdir = Path().resolve() / \"out\"\n",
    "workdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "ignores = []\n",
    "for ds in [\"train\", \"valid\", \"test\"]:\n",
    "    records = []\n",
    "    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n",
    "        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n",
    "        for filepath in sorted(datadir.glob(\"*.npz\")):\n",
    "            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "\n",
    "            if (ds != \"test\") and ((\"mlperf\" in filename) or (\"openai\" in filename)):\n",
    "                ignores.append(filepath)\n",
    "                continue\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arch\": arch,\n",
    "                    \"perm\": perm,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": filepath,\n",
    "                    \"node_feat_filepath\": str(\n",
    "                        node_feat_dir / arch / perm / ds / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_feat_filepath\": str(\n",
    "                        trans_node_feat_dir\n",
    "                        / \"layout\"\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_config_filepath\": str(\n",
    "                        trans_node_config_feat_dir\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "    dataset_dict[ds] = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for filepath in tqdm(ignores):\n",
    "#     node_config_feat = np.load(filepath)[\"node_config_feat\"]\n",
    "\n",
    "#     for i in range(1, node_config_feat.shape[0]):\n",
    "#         if not (node_config_feat[0] == node_config_feat[i]).all():\n",
    "#             filepath\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "      <th>cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats  \\\n",
       "0       0         1        19   \n",
       "1       1        68         6   \n",
       "\n",
       "                                                cats  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "1                                 [0, 1, 2, 3, 4, 5]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat = pd.DataFrame(\n",
    "    [\n",
    "        {\"number\": 0, \"num_dims\": 1, \"num_cats\": 19, \"cats\": list(range(19))},\n",
    "        {\"number\": 1, \"num_dims\": 54 + 14, \"num_cats\": 6, \"cats\": list(range(6))},\n",
    "    ]\n",
    ")\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats\n",
       "0       0        18         8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat_config = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"number\": 0,\n",
    "            \"num_dims\": 18,\n",
    "            \"num_cats\": 8,\n",
    "        },  # output_layout, input_layout, kernel_layout\n",
    "    ]\n",
    ")\n",
    "dfcat_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset_dict:\n",
    "    for i, row in dataset_dict[ds].iterrows():\n",
    "        np.load(row[\"filepath\"])\n",
    "        np.load(row[\"node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_config_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データクラスを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_configs = 2000\n",
    "NUM_SAMPLES = 1000\n",
    "num_samples = min(NUM_SAMPLES, num_configs)\n",
    "third = NUM_SAMPLES // 3\n",
    "numbers = list(range(num_configs))\n",
    "middle_samples = np.random.choice(\n",
    "    numbers[third:-third], num_samples - 2 * third\n",
    ").tolist()\n",
    "samples = numbers[:third] + numbers[-third:] + middle_samples\n",
    "samples = random.sample(samples, len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CatStatus:\n",
    "    dfcat: pd.DataFrame\n",
    "    prefix: str\n",
    "    num_cat_dict: dict[str, int] = field(init=False)\n",
    "    index_dict: dict[str, list[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.num_cat_dict, self.index_dict = {}, {}\n",
    "        dim_start = 0\n",
    "        for i, row in self.dfcat.iterrows():\n",
    "            self.num_cat_dict[f\"{self.prefix}cat_feat{i + 1}\"] = row[\"num_cats\"]\n",
    "            self.index_dict[f\"{self.prefix}cat_feat{i + 1}\"] = list(\n",
    "                range(dim_start, dim_start + row[\"num_dims\"])\n",
    "            )\n",
    "            dim_start += row[\"num_dims\"]\n",
    "\n",
    "\n",
    "cat_status = CatStatus(dfcat=dfcat, prefix=\"\")\n",
    "cat_config_status = CatStatus(dfcat=dfcat_config, prefix=\"config_\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Const:\n",
    "    num_node_flag_feat_dim: int\n",
    "    num_node_cont_feat_dim: int\n",
    "    num_node_cat_feat_dim: int\n",
    "    num_node_config_cont_feat_dim: int\n",
    "\n",
    "    # 演算子の種類\n",
    "    num_operations: int = 120\n",
    "    # 各configの次元数\n",
    "    num_config_dims: int = 6\n",
    "\n",
    "\n",
    "fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"node_feat_filepath\"])\n",
    "trans_fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"trans_node_feat_filepath\"])\n",
    "trans_config_fileobj = np.load(\n",
    "    dataset_dict[\"train\"].iloc[0][\"trans_node_config_filepath\"]\n",
    ")\n",
    "\n",
    "node_flag_feat, node_cont_feat = fileobj[\"node_flag_feat\"], fileobj[\"node_cont_feat\"]\n",
    "node_enum_feat, node_dimension_number_feat = (\n",
    "    fileobj[\"node_enum_feat\"],\n",
    "    fileobj[\"node_dimension_number_feat\"],\n",
    ")\n",
    "trans_node_cont_feat, trans_node_cat_feat = (\n",
    "    trans_fileobj[\"node_cont_feat\"],\n",
    "    trans_fileobj[\"node_cat_feat\"],\n",
    ")\n",
    "trans_node_config_cont_feat = trans_config_fileobj[\"node_config_cont_feat\"]\n",
    "const = Const(\n",
    "    num_node_flag_feat_dim=node_flag_feat.shape[1] + 1,  # config_idsの分+1\n",
    "    num_node_cont_feat_dim=node_cont_feat.shape[1] + trans_node_cont_feat.shape[1],\n",
    "    num_node_cat_feat_dim=node_enum_feat.shape[1]\n",
    "    + node_dimension_number_feat.shape[1]\n",
    "    + trans_node_cat_feat.shape[1],\n",
    "    num_node_config_cont_feat_dim=trans_node_config_cont_feat.shape[2],\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NodeFeatExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope: float = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GNNExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CatEmbedding:\n",
    "    num_cat: int\n",
    "    embedding_dim: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    device: str\n",
    "    cat_embeddings: dict[str, CatEmbedding]\n",
    "    random_batch_size: int = 30\n",
    "    batch_size: int = 30\n",
    "    node_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    node_config_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    gnn_extractor: GNNExtractor = field(default_factory=lambda: GNNExtractor())\n",
    "    subgraph_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    epoch: int = 50\n",
    "    T_max: int = 50\n",
    "    eta_min: float = 1e-5\n",
    "    lr: float = 1e-2\n",
    "    weight_decay: float = 0\n",
    "\n",
    "\n",
    "cat_embeddings = {}\n",
    "cat_embeddings.update(\n",
    "    {\"op\": CatEmbedding(num_cat=const.num_operations, embedding_dim=16)}\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_config_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "params = Params(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    cat_embeddings=cat_embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LayoutConfigs:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    node_cont_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 108)\n",
    "\n",
    "    node_cat_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 3)\n",
    "\n",
    "    node_opcode: np.ndarray\n",
    "        ノード演算子、(ノード数,)\n",
    "    edge_index: np.ndarray\n",
    "        エッジ、(エッジ数, 2)\n",
    "\n",
    "    node_config_feat: np.ndarray\n",
    "        設定毎のノード特徴量、(設定数, 設定可能なノード数, 3)\n",
    "\n",
    "    node_config_ids: np.ndarray\n",
    "        設定可能なノードのIndex、(設定可能なノード数,)\n",
    "    config_runtime: np.ndarray\n",
    "        実行時間、(設定数,)\n",
    "    node_splits: np.ndarray\n",
    "        同じパーティションでの計算を意味する。今回は使用しない。(パーティション数, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    node_flag_feat: np.ndarray\n",
    "    node_cont_feat: np.ndarray\n",
    "    node_cat_feat: np.ndarray\n",
    "    node_opcode: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    node_config_feat: np.ndarray\n",
    "    node_config_cont_feat: np.ndarray\n",
    "    node_config_ids: np.ndarray\n",
    "    config_runtime: np.ndarray\n",
    "    node_splits: np.ndarray\n",
    "\n",
    "    cat_status: CatStatus\n",
    "    cat_config_status: CatStatus\n",
    "    target: np.ndarray = field(init=False)\n",
    "    argsorted_indexs: list[int] = field(init=False)\n",
    "\n",
    "    NUM_SAMPLES: int = 1000\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # 設定が存在するノードのフラグ\n",
    "        node_active_feat = np.zeros((self.num_nodes, 1))\n",
    "        node_active_feat[self.node_config_ids, :] = 1\n",
    "        self.node_flag_feat = np.concatenate(\n",
    "            [self.node_flag_feat, node_active_feat], axis=1\n",
    "        )\n",
    "        self.node_cont_feat = self.apply_normalization(x=self.node_cont_feat)\n",
    "        self.node_config_feat = self.node_config_feat + 1  # カテゴリは0~7にする\n",
    "        self.node_splits = np.array(\n",
    "            [\n",
    "                [self.node_splits[0][i], self.node_splits[0][i + 1] - 1]\n",
    "                for i in range(self.node_splits.shape[1] - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.target = self.apply_target_normalization(x=self.config_runtime)\n",
    "        self.argsorted_indexs = np.argsort(self.config_runtime).tolist()\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        \"\"\"ノード数\"\"\"\n",
    "        return self.node_cont_feat.shape[0]\n",
    "\n",
    "    def get_random_config_idxs(self) -> list[int]:\n",
    "        \"\"\"tpu_graphのサンプリング方法\n",
    "        https://github.com/google-research-datasets/tpu_graphs/blob/main/tpu_graphs/baselines/layout/data.py#L352\n",
    "        \"\"\"\n",
    "        num_configs = self.config_runtime.shape[0]\n",
    "        num_samples = min(NUM_SAMPLES, num_configs)\n",
    "        third = num_samples // 3\n",
    "\n",
    "        middle_samples = np.random.choice(\n",
    "            self.argsorted_indexs[third:-third], num_samples - 2 * third\n",
    "        ).tolist()\n",
    "        samples = (\n",
    "            self.argsorted_indexs[:third]\n",
    "            + self.argsorted_indexs[-third:]\n",
    "            + middle_samples\n",
    "        )\n",
    "        samples = random.sample(samples, len(samples))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def get_filled_node_config_feat(\n",
    "        self, index_list: list[int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"指定された設定の設定毎のノード特徴量を取得する。設定がない場合は補完する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray [(len(index_list),ノード数, 18), (len(index_list),ノード数, 連続次元数)]\n",
    "        \"\"\"\n",
    "        # (サンプル数, ノード数) x 3\n",
    "        node_config_feat = np.full(\n",
    "            (len(index_list), self.num_nodes, Const.num_config_dims * 3),\n",
    "            Const.num_config_dims + 1,\n",
    "        )\n",
    "        node_config_feat[:, self.node_config_ids] = self.node_config_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "\n",
    "        node_config_cont_feat = np.zeros(\n",
    "            (len(index_list), self.num_nodes, self.node_config_cont_feat.shape[2])\n",
    "        )\n",
    "        node_config_cont_feat[:, self.node_config_ids] = self.node_config_cont_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "        return node_config_feat, node_config_cont_feat\n",
    "\n",
    "    def get_target(self, index_list: list[int]) -> np.ndarray:\n",
    "        \"\"\"指定された設定の目的変数を取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "        \"\"\"\n",
    "        return self.apply_target_ranking(x=self.config_runtime[index_list])\n",
    "\n",
    "    def apply_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"特徴量の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            2次元行列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            行方向に正規化された行列\n",
    "        \"\"\"\n",
    "        x /= 128\n",
    "        x = np.where(x >= 0, np.log1p(x / 128), -np.log1p(-x / 128))\n",
    "        return x\n",
    "\n",
    "    def apply_target_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"目的変数の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            ベクトル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            正規化されたベクトル\n",
    "        \"\"\"\n",
    "        return np.log(x / x.min())\n",
    "\n",
    "    def apply_target_ranking(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"降順でランキング\"\"\"\n",
    "        return np.argsort(np.argsort(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayoutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    rows: list[dict[str, np.ndarray]]\n",
    "        設定をリストでもつ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: pd.DataFrame,\n",
    "        params: Params,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        self.rows = dataset.to_dict(\"records\")\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "        self.cache_idx = None\n",
    "        self.cache_filepath = None\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return self.params.device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def create_layout_config(self, idx: int) -> LayoutConfigs:\n",
    "        if self.cache_idx != idx:\n",
    "            self.cache_idx = idx\n",
    "            fileobj = np.load(self.rows[self.cache_idx][\"filepath\"])\n",
    "            node_feat_fileobj = np.load(self.rows[self.cache_idx][\"node_feat_filepath\"])\n",
    "            trans_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_feat_filepath\"]\n",
    "            )\n",
    "            trans_config_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_config_filepath\"]\n",
    "            )\n",
    "\n",
    "            node_cont_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_cont_feat\"],\n",
    "                    trans_feat_fileobj[\"node_cont_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            node_cat_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_enum_feat\"],\n",
    "                    node_feat_fileobj[\"node_dimension_number_feat\"],\n",
    "                    trans_feat_fileobj[\"node_cat_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            self.cache_layout_config = LayoutConfigs(\n",
    "                node_opcode=fileobj[\"node_opcode\"],\n",
    "                edge_index=fileobj[\"edge_index\"],\n",
    "                node_config_ids=fileobj[\"node_config_ids\"],\n",
    "                config_runtime=fileobj[\"config_runtime\"],\n",
    "                node_splits=fileobj[\"node_splits\"],\n",
    "                node_flag_feat=node_feat_fileobj[\"node_flag_feat\"],\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=fileobj[\"node_config_feat\"],\n",
    "                node_config_cont_feat=trans_config_feat_fileobj[\n",
    "                    \"node_config_cont_feat\"\n",
    "                ],\n",
    "                cat_status=self.cat_status,\n",
    "                cat_config_status=self.cat_config_status,\n",
    "            )\n",
    "        return self.cache_layout_config\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def getitem_as_random_batch(\n",
    "        self, idx: int\n",
    "    ) -> list[\n",
    "        tuple[\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "        ]\n",
    "    ]:\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "\n",
    "        index_list = layout_configs.get_random_config_idxs()\n",
    "        for i_chunk in range(0, len(index_list), self.params.random_batch_size):\n",
    "            chunk_index_list = index_list[\n",
    "                i_chunk : i_chunk + self.params.random_batch_size\n",
    "            ]\n",
    "            yield self._get_tensors(\n",
    "                layout_configs=layout_configs, index_list=chunk_index_list\n",
    "            )\n",
    "\n",
    "    def getitem_as_batch(\n",
    "        self, idx: int\n",
    "    ) -> list[\n",
    "        tuple[\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "        ]\n",
    "    ]:\n",
    "        \"\"\"設定をバッチで取得する\"\"\"\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "\n",
    "        index_list = list(range(layout_configs.config_runtime.shape[0]))\n",
    "        for i_chunk in range(0, len(index_list), self.params.batch_size):\n",
    "            chunk_index_list = index_list[i_chunk : i_chunk + self.params.batch_size]\n",
    "            yield self._get_tensors(\n",
    "                layout_configs=layout_configs, index_list=chunk_index_list\n",
    "            )\n",
    "\n",
    "    def _get_tensors(\n",
    "        self, layout_configs: LayoutConfigs, index_list: list[int]\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"渡された設定のIndexのテンソルを取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layout_configs: LayoutConfigs\n",
    "            Layoutのデータクラス\n",
    "        index_list: list[int]\n",
    "            設定のインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            ノード特徴量(フラグ)\n",
    "        torch.Tensor\n",
    "            ノード特徴量(連続)\n",
    "        dict[str, torch.Tensor]\n",
    "            ノード特徴量(カテゴリ)\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量(連続)\n",
    "        torch.Tensor\n",
    "            ノード演算子\n",
    "        torch.Tensor\n",
    "            エッジ\n",
    "        torch.Tensor\n",
    "            目的変数\n",
    "        \"\"\"\n",
    "        # ノード特徴量(フラグ)\n",
    "        node_flag_feat = torch.tensor(\n",
    "            layout_configs.node_flag_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(連続)\n",
    "        node_cont_feat = torch.tensor(\n",
    "            layout_configs.node_cont_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(カテゴリ)\n",
    "        node_cat_feat = torch.tensor(\n",
    "            layout_configs.node_cat_feat,\n",
    "            dtype=torch.int64,\n",
    "        ).to(self.device)\n",
    "        # 設定毎のノード特徴量(カテゴリ)\n",
    "        (\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "        ) = layout_configs.get_filled_node_config_feat(index_list=index_list)\n",
    "        node_config_feat = torch.tensor(node_config_feat, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        node_config_cont_feat = torch.tensor(\n",
    "            node_config_cont_feat, dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        # ノード演算子\n",
    "        node_opcode = torch.tensor(layout_configs.node_opcode, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # エッジ\n",
    "        edge_index = torch.tensor(\n",
    "            np.swapaxes(layout_configs.edge_index, 0, 1), dtype=torch.int64\n",
    "        ).to(self.device)\n",
    "        # サブグラフ\n",
    "        node_splits = torch.tensor(layout_configs.node_splits, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # ターゲット\n",
    "        target = torch.tensor(\n",
    "            layout_configs.get_target(index_list=index_list),\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        )\n",
    "\n",
    "    def get_ith_file_info(self, i: int) -> dict[str, str]:\n",
    "        row = self.rows[i]\n",
    "        return {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "        }\n",
    "\n",
    "    def get_ith_runtime(self, i: int) -> np.ndarray:\n",
    "        layout_configs = self.create_layout_config(idx=i)\n",
    "        return layout_configs.config_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "\n",
    "class EdgeConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    ノード特徴 + 隣接ノード特徴 + 隣接ノード特徴の一致\n",
    "    参考： https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html#implementing-the-edge-convolution\n",
    "    補足: 集約関数はデフォルトでdim(axis) = -2。つまりノード方向で集約するので気にしなくてOK\n",
    "    https://github.com/pyg-team/pytorch_geometric/blob/1e12d41c28b1fb9793f17646b018071b508864d7/torch_geometric/nn/aggr/basic.py#L38\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_input_dim: int, x_output_dim: int, dropout_p: float):\n",
    "        # \"Add\" aggregation\n",
    "        super().__init__(aggr=\"max\")\n",
    "        self.mlp = nn.Sequential(\n",
    "            # nn.LayerNorm(x_input_dim * 2),\n",
    "            nn.Linear(x_input_dim * 2, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.ReLU(),\n",
    "            # nn.LayerNorm(x_output_dim),\n",
    "            nn.Linear(x_output_dim, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [設定数, N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"propagate()で渡された引数xから自動でx_i, x_jノードを取り出して随時処理を実装する関数\"\"\"\n",
    "        # x_i has shape [設定数, エッジ数, in_channels]\n",
    "        # x_j has shape [設定数, エッジ数, in_channels]\n",
    "        x_cat = torch.cat(\n",
    "            [x_i, x_i - x_j], dim=2\n",
    "        )  # tmp has shape [設定数, エッジ数, 2 * in_channels]\n",
    "        return self.mlp(x_cat)\n",
    "\n",
    "\n",
    "class SimpleLayoutModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params: Params\n",
    "        実験設定のデータクラス\n",
    "    node_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(ノード毎)\n",
    "    node_config_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(設定xノード毎)\n",
    "    node_feat_extractor: torch.nn.Module\n",
    "        ノードの特徴量を抽出するネットワーク\n",
    "    gnn_extractor: torch.nn.Module\n",
    "        グラフの特徴量を抽出するネットワーク\n",
    "    gc: torch.nn.Module\n",
    "        最終層の全結合層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Params,\n",
    "        const: Const,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "\n",
    "        # カテゴリ変数の埋め込み表現\n",
    "        self.embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                k: torch.nn.Embedding(v.num_cat, v.embedding_dim)\n",
    "                for k, v in self.params.cat_embeddings.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # node_featのfeature_extractorを定義\n",
    "        num_node_feat_extractor_input_dim = (\n",
    "            const.num_node_flag_feat_dim\n",
    "            + const.num_node_cont_feat_dim\n",
    "            + self.num_node_feat_embedding_dims\n",
    "        )\n",
    "\n",
    "        node_feat_extractor_layer = []\n",
    "        node_feat_extractor_dims = [\n",
    "            num_node_feat_extractor_input_dim\n",
    "        ] + self.params.node_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_feat_extractor_dims[i],\n",
    "                    out_features=node_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(params.node_feat_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "            self.node_feat_extractor = nn.Sequential(*node_feat_extractor_layer)\n",
    "\n",
    "        # node_config_featのfeature_extractorを定義\n",
    "        num_node_config_feat_extractor_input_dim = (\n",
    "            self.num_node_config_feat_embedding_dims\n",
    "            + const.num_node_config_cont_feat_dim\n",
    "        )\n",
    "\n",
    "        node_config_feat_extractor_layer = []\n",
    "        node_config_feat_extractor_dims = [\n",
    "            num_node_config_feat_extractor_input_dim\n",
    "        ] + self.params.node_config_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_config_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_config_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_config_feat_extractor_dims[i],\n",
    "                    out_features=node_config_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_config_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(\n",
    "                    params.node_config_feat_extractor.leakyrelu_negative_slope\n",
    "                ),\n",
    "            ]\n",
    "        self.node_config_feat_extractor = nn.Sequential(\n",
    "            *node_config_feat_extractor_layer\n",
    "        )\n",
    "\n",
    "        # ノード間のfeature_extractorの定義\n",
    "        num_gnn_extractor_input_dim = (\n",
    "            node_feat_extractor_dims[-1] + node_config_feat_extractor_dims[-1]\n",
    "        )\n",
    "\n",
    "        gnn_extractor_layer = []\n",
    "        gnn_extractor_dims = [\n",
    "            num_gnn_extractor_input_dim\n",
    "        ] + self.params.gnn_extractor.dims\n",
    "        for i in range(len(gnn_extractor_dims) - 1):\n",
    "            gnn_extractor_layer += [\n",
    "                (\n",
    "                    EdgeConv(\n",
    "                        x_input_dim=gnn_extractor_dims[i],\n",
    "                        x_output_dim=gnn_extractor_dims[i + 1],\n",
    "                        dropout_p=params.gnn_extractor.dropout_p,\n",
    "                    ),\n",
    "                    \"x, edge_index -> x\",\n",
    "                ),\n",
    "                nn.LeakyReLU(params.gnn_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "        self.gnn_extractor = Sequential(\"x, edge_index\", gnn_extractor_layer)\n",
    "\n",
    "        # # サブグラフのfeature_extractorの定義\n",
    "        # num_subgraph_extractor_input_dim = (\n",
    "        #     self.params.gnn_extractor.dims[-1] + num_gnn_extractor_input_dim\n",
    "        # )\n",
    "\n",
    "        # subgraph_extractor_layer = []\n",
    "        # subgraph_extractor_dims = [\n",
    "        #     num_subgraph_extractor_input_dim\n",
    "        # ] + self.params.node_feat_extractor.dims\n",
    "        # for i in range(len(subgraph_extractor_dims) - 1):\n",
    "        #     subgraph_extractor_layer += [\n",
    "        #         # nn.LayerNorm(subgraph_extractor_dims[i]),\n",
    "        #         nn.Linear(\n",
    "        #             in_features=subgraph_extractor_dims[i],\n",
    "        #             out_features=subgraph_extractor_dims[i + 1],\n",
    "        #         ),\n",
    "        #         # nn.Dropout(params.subgraph_extractor.dropout_p),\n",
    "        #         nn.LeakyReLU(params.subgraph_extractor.leakyrelu_negative_slope),\n",
    "        #     ]\n",
    "        # self.subgraph_extractor = nn.Sequential(*subgraph_extractor_layer)\n",
    "\n",
    "        fc_layer = [\n",
    "            # nn.LayerNorm(subgraph_extractor_dims[-1]),\n",
    "            # nn.Linear(in_features=subgraph_extractor_dims[-1], out_features=1),\n",
    "            nn.Linear(\n",
    "                in_features=self.params.gnn_extractor.dims[-1]\n",
    "                + num_gnn_extractor_input_dim,\n",
    "                out_features=1,\n",
    "            ),\n",
    "        ]\n",
    "        self.fc = nn.Sequential(*fc_layer)\n",
    "        self.to(self.params.device)\n",
    "\n",
    "    @property\n",
    "    def num_node_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        num_embedding_dims += 1 * self.params.cat_embeddings[\"op\"].embedding_dim\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    @property\n",
    "    def num_node_config_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "        node_config_feat: torch.Tensor,\n",
    "        node_config_cont_feat: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        node_splits: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------\n",
    "        node_flag_feat:\n",
    "            ノードの特徴量(node数, フラグ次元数)\n",
    "        node_cont_feat:\n",
    "            ノードの特徴量(node数, 連続次元数)\n",
    "        node_cat_feat:\n",
    "            ノードの特徴量(node数, カテゴリ次元数*埋め込み次元数)\n",
    "        node_config_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 特徴次元数)\n",
    "        node_config_cont_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 連続次元数)\n",
    "        edge_index:\n",
    "            エッジ(2, エッジ数)\n",
    "        node_splits:\n",
    "            サブグラフのインデックス（サブグラフ数, 2)\n",
    "\n",
    "        Returns:\n",
    "        torch.tensor: (設定数)\n",
    "        \"\"\"\n",
    "        # (ノード数,特徴数)のテンソルを作成\n",
    "        node_feat = self._join_node_feature(\n",
    "            node_opcode=node_opcode,\n",
    "            node_flag_feat=node_flag_feat,\n",
    "            node_cont_feat=node_cont_feat,\n",
    "            node_cat_feat=node_cat_feat,\n",
    "        )\n",
    "\n",
    "        # (設定数,ノード数,特徴数)のテンソルを作成\n",
    "        node_config_feat = self._join_node_config_feature(\n",
    "            node_config_feat=node_config_feat,\n",
    "            node_config_cont_feat=node_config_cont_feat,\n",
    "        )\n",
    "\n",
    "        # node_featの抽出器を通す\n",
    "        extracted_node_feat = self.node_feat_extractor(node_feat)\n",
    "\n",
    "        # node_config_featの抽出器を通す\n",
    "        extracted_node_config_feat = self.node_config_feat_extractor(node_config_feat)\n",
    "\n",
    "        # 設定毎のノード特徴に結合する\n",
    "        extracted_feat = self._join_entire_node_config_feat(\n",
    "            node_feat=extracted_node_feat,\n",
    "            node_config_feat=extracted_node_config_feat,\n",
    "        )\n",
    "\n",
    "        # GNN抽出器を通す\n",
    "        conved_extracted_feat = self.gnn_extractor(\n",
    "            x=extracted_feat,\n",
    "            edge_index=edge_index,\n",
    "        )\n",
    "\n",
    "        # 残差を足すイメージ\n",
    "        concat_feat = torch.concat([extracted_feat, conved_extracted_feat], 2)\n",
    "\n",
    "        # subgraph_global_pool_feat_list = []\n",
    "        # for subgraph_start_node_idx, subgraph_end_node_idx in node_splits:\n",
    "        #     subgraph_concat_feat = concat_feat[\n",
    "        #         :, subgraph_start_node_idx : subgraph_end_node_idx + 1, :\n",
    "        #     ]\n",
    "        #     # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        #     subgraph_global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "        #     subgraph_global_pool_feat_list.append(\n",
    "        #         torch.reshape(\n",
    "        #             subgraph_global_pool_feat,\n",
    "        #             (\n",
    "        #                 subgraph_global_pool_feat.shape[0],\n",
    "        #                 1,\n",
    "        #                 subgraph_global_pool_feat.shape[1],\n",
    "        #             ),\n",
    "        #         )\n",
    "        #     )\n",
    "        # # （設定数,サブグラフ数,特徴数)\n",
    "        # subgraph_global_pool_feat = torch.concat(subgraph_global_pool_feat_list, 1)\n",
    "        # subgraph_extracted_feat = self.subgraph_extractor(subgraph_global_pool_feat)\n",
    "\n",
    "        # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        # global_pool_feat = torch.mean(subgraph_extracted_feat, dim=1)\n",
    "        global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "\n",
    "        return torch.squeeze(self.fc(global_pool_feat))\n",
    "\n",
    "    def _join_node_feature(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_featのテンソルを作成\"\"\"\n",
    "        # ノードの埋め込み表現\n",
    "        node_embeddings_list = []\n",
    "        node_embeddings_list.append(self.embeddings[\"op\"](node_opcode))\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](node_cat_feat[:, cat_index])\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (-1, node_embeddings.shape[-2] * node_embeddings.shape[-1]),\n",
    "            )\n",
    "            node_embeddings_list.append(node_embeddings)\n",
    "\n",
    "        # ノード毎で埋め込み、結合(ノード数, 特徴数)\n",
    "        node_embedding_feat = torch.concat(node_embeddings_list, 1)\n",
    "        node_feat = torch.concat(\n",
    "            [node_flag_feat, node_cont_feat, node_embedding_feat], 1\n",
    "        )\n",
    "        return node_feat\n",
    "\n",
    "    def _join_node_config_feature(\n",
    "        self, node_config_feat: torch.Tensor, node_config_cont_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_config_featのテンソルを作成\"\"\"\n",
    "        # 設定xノード毎で埋め込み(設定数, ノード数, 特徴数)\n",
    "        node_config_embeddings_list = []\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](\n",
    "                node_config_feat[:, :, cat_index]\n",
    "            )\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (\n",
    "                    node_embeddings.shape[0],\n",
    "                    -1,\n",
    "                    node_embeddings.shape[-2] * node_embeddings.shape[-1],\n",
    "                ),\n",
    "            )\n",
    "            node_config_embeddings_list.append(node_embeddings)\n",
    "        node_config_feat = torch.concat(\n",
    "            node_config_embeddings_list + [node_config_cont_feat], 2\n",
    "        )\n",
    "        return node_config_feat\n",
    "\n",
    "    def _join_entire_node_config_feat(\n",
    "        self, node_feat: torch.Tensor, node_config_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # ノード毎の特徴量を設定数だけ縦に並べる\n",
    "        node_tiled_feat = torch.tile(\n",
    "            torch.reshape(node_feat, (1, node_feat.shape[0], node_feat.shape[1])),\n",
    "            (node_config_feat.shape[0], 1, 1),\n",
    "        )\n",
    "        return torch.concat([node_tiled_feat, node_config_feat], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ListMLE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.Tensor\n",
    "            予測（要素数, ）\n",
    "        labels: torch.Tensor\n",
    "            目的変数（要素数, ）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        # 正解をソート\n",
    "        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n",
    "        # 予測を正解順でソート\n",
    "        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n",
    "        # 予測値の最大値で予測値を引く（expの爆発予防）\n",
    "        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n",
    "        logits_sorted_by_true = logits_sorted_by_true - logits_max\n",
    "        # ランキングが低いものから累積する(その後正解順に戻す)\n",
    "        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n",
    "            dims=[1]\n",
    "        )\n",
    "        # 誤差\n",
    "        negative_log_likelihood = torch.sum(\n",
    "            torch.log(cumsums) - logits_sorted_by_true, dim=1\n",
    "        )\n",
    "        return torch.mean(negative_log_likelihood)\n",
    "\n",
    "\n",
    "def rankNet(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    # here we generate every pair of indices from the range of document length in the batch\n",
    "    document_pairs_candidates = list(\n",
    "        itertools.product(range(y_true.shape[1]), repeat=2)\n",
    "    )\n",
    "\n",
    "    pairs_true = y_true[:, document_pairs_candidates]\n",
    "    selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "    # here we calculate the relative true relevance of every candidate pair\n",
    "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "    # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
    "    # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
    "    # positive ones for a simpler loss function formulation\n",
    "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "\n",
    "    pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "    weight = None\n",
    "    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
    "    # whether one document is better than the other and not about the actual difference in\n",
    "    # their relevancy levels\n",
    "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "    true_diffs = true_diffs[the_mask]\n",
    "\n",
    "    return nn.BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)\n",
    "\n",
    "\n",
    "def to_cpu_numpy(\n",
    "    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if params.device == \"cuda\":\n",
    "        pred_ = pred.cpu().detach().numpy()\n",
    "        truth_ = truth.cpu().detach().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pred_ = pred.detach().numpy()\n",
    "        truth_ = truth.detach().numpy()\n",
    "    return pred_, truth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "def evaluate_score(dataset: LayoutDataset, model: torch.nn.Module) -> pd.DataFrame:\n",
    "    \"\"\"データセット全件に対してコンペの評価指標を算出する\n",
    "    https://www.kaggle.com/competitions/predict-ai-model-runtime/overview\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    records = []\n",
    "    # 各グラフ毎にスコアを算出\n",
    "    for graph_index in range(len(dataset)):\n",
    "        # グラフ毎に1000件をバッチに分けて取得\n",
    "        preds, truths = [], []\n",
    "        for (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        ) in dataset.getitem_as_random_batch(graph_index):\n",
    "            pred = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            pred, truth = to_cpu_numpy(params, pred, target)\n",
    "            preds.append(pred)\n",
    "            truths.append(truth)\n",
    "\n",
    "        preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "\n",
    "        loss = rankNet(\n",
    "            torch.tensor(preds.reshape(1, -1)),\n",
    "            torch.tensor(truths.reshape(1, -1)),\n",
    "        )\n",
    "        graph_loss = loss.item()\n",
    "        score = kendalltau(truth, pred).correlation\n",
    "\n",
    "        record = dataset.get_ith_file_info(graph_index)\n",
    "        record.update(\n",
    "            {\n",
    "                \"graph_loss\": graph_loss,\n",
    "                \"score\": score,\n",
    "            }\n",
    "        )\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    dftrain: pd.DataFrame,\n",
    "    dfvalid: pd.DataFrame,\n",
    "    params: Params,\n",
    "    const: Const,\n",
    "    cat_status: CatStatus,\n",
    "    cat_config_status: CatStatus,\n",
    "    savedir: Path,\n",
    "    checkpoint_dir: Path = None,\n",
    ") -> None:\n",
    "    train_layout_dataset = LayoutDataset(\n",
    "        dataset=dftrain,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    valid_layout_dataset = LayoutDataset(\n",
    "        dataset=dfvalid,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "\n",
    "    model = SimpleLayoutModel(\n",
    "        params=params,\n",
    "        const=const,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    if checkpoint_dir is not None:\n",
    "        print(\"学習済みモデルを読み込みます\")\n",
    "        model.load_state_dict(torch.load(checkpoint_dir / f\"final_model.pt\"))\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n",
    "    )\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    best_score = -np.inf\n",
    "    records = []\n",
    "    for epoch in range(params.epoch):\n",
    "        model.train()\n",
    "\n",
    "        num_graph = len(train_layout_dataset)\n",
    "        pbar = tqdm(range(num_graph))\n",
    "        graph_indexes = random.sample(list(range(num_graph)), num_graph)\n",
    "\n",
    "        epoch_losses = []\n",
    "        epoch_loss = 0  # 各グラフの誤差を総和（エポックの誤差）\n",
    "\n",
    "        # グラフをシャッフルして取得\n",
    "        for i_graph, graph_index in enumerate(graph_indexes):\n",
    "            graph_info = train_layout_dataset.get_ith_file_info(graph_index)\n",
    "            graph_arch, graph_perm = graph_info[\"arch\"], graph_info[\"perm\"]\n",
    "            # 各グラフで1000件をバッチに分けて取得\n",
    "            preds, truths = [], []\n",
    "            graph_loss = 0  # バッチの誤差を総和（グラフの誤差）\n",
    "            num_batch_count = 0\n",
    "            for (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            ) in train_layout_dataset.getitem_as_random_batch(graph_index):\n",
    "                out = model(\n",
    "                    node_opcode=node_opcode,\n",
    "                    node_flag_feat=node_flag_feat,\n",
    "                    node_cont_feat=node_cont_feat,\n",
    "                    node_cat_feat=node_cat_feat,\n",
    "                    node_config_feat=node_config_feat,\n",
    "                    node_config_cont_feat=node_config_cont_feat,\n",
    "                    edge_index=edge_index,\n",
    "                    node_splits=node_splits,\n",
    "                )\n",
    "                # loss = criterion(\n",
    "                #     torch.reshape(out, (1, out.shape[0])),\n",
    "                #     torch.reshape(target, (1, target.shape[0])),\n",
    "                # )\n",
    "                loss = rankNet(\n",
    "                    torch.reshape(out, (1, out.shape[0])),\n",
    "                    torch.reshape(target, (1, target.shape[0])),\n",
    "                )\n",
    "                loss.backward()\n",
    "                graph_loss += loss.item()\n",
    "\n",
    "                pred, truth = to_cpu_numpy(params, out, target)\n",
    "                preds.append(pred)\n",
    "                truths.append(truth)\n",
    "                num_batch_count += 1\n",
    "\n",
    "            # 各グラフ毎に勾配降下\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + i_graph / num_graph)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "            score = kendalltau(truth, pred).correlation\n",
    "            graph_loss /= num_batch_count  # 各バッチの平均をグラフの誤差とする\n",
    "            epoch_loss += graph_loss\n",
    "\n",
    "            record = {\n",
    "                \"epoch\": epoch,\n",
    "                \"i_graph\": i_graph,\n",
    "                f\"train-{graph_arch}-{graph_perm}/epoch_loss\": epoch_loss\n",
    "                / (i_graph + 1),\n",
    "                f\"train-{graph_arch}-{graph_perm}/graph_loss\": graph_loss,\n",
    "                f\"train-{graph_arch}-{graph_perm}/score\": score,\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            record.update(graph_info)\n",
    "            records.append(record)\n",
    "\n",
    "            wandb.log(record)\n",
    "            pbar.set_description(\n",
    "                f\"running loss: {epoch_loss / (i_graph + 1):.5f}, graph loss: {graph_loss:.5f} score: {score:.3f}\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        dfscore = evaluate_score(dataset=valid_layout_dataset, model=model)\n",
    "        avg_loss = dfscore[\"graph_loss\"].mean()\n",
    "        avg_score = dfscore[\"score\"].mean()\n",
    "        for _, row_score in dfscore.iterrows():\n",
    "            graph_arch, graph_perm = row_score[\"arch\"], row_score[\"perm\"]\n",
    "            record = {\n",
    "                \"epoch\": epoch,\n",
    "                \"i_graph\": -1,\n",
    "                \"arch\": graph_arch,\n",
    "                \"perm\": graph_perm,\n",
    "                \"filename\": row_score[\"filename\"],\n",
    "                f\"valid-{graph_arch}-{graph_perm}/epoch_loss\": avg_loss,\n",
    "                f\"valid-{graph_arch}-{graph_perm}/graph_loss\": row_score[\"graph_loss\"],\n",
    "                f\"valid-{graph_arch}-{graph_perm}/score\": row_score[\"score\"],\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            records.append(record)\n",
    "            wandb.log(record)\n",
    "\n",
    "        print(f\"[valid] current loss: {avg_loss:.5f} score: {avg_score:.3f}\")\n",
    "\n",
    "        if best_score < avg_score:\n",
    "            best_score = avg_score\n",
    "            torch.save(model.state_dict(), savedir / \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n",
    "\n",
    "    dflog = pd.DataFrame(records)\n",
    "    dflog.to_csv(savedir / \"log.csv\", index=False)\n",
    "\n",
    "    torch.save(model.state_dict(), savedir / \"final_model.pt\")\n",
    "\n",
    "    del (\n",
    "        train_layout_dataset,\n",
    "        valid_layout_dataset,\n",
    "        model,\n",
    "        optimizer,\n",
    "        dfscore,\n",
    "        dflog,\n",
    "        records,\n",
    "    )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzuuuubo-tetsu\u001b[0m (\u001b[33msun-scan-clan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yamaguchi/kaggle/experiments/1030-all-サブグラフを考慮しない/wandb/run-20231030_090232-8srx6ld9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/8srx6ld9' target=\"_blank\">1030-all-サブグラフを考慮しない</a></strong> to <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/8srx6ld9' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/8srx6ld9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/8srx6ld9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8bf7f0c150>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exptname = str(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"params\": asdict(params),\n",
    "        \"const\": asdict(const),\n",
    "        \"validation\": \"hold-out\",\n",
    "    },\n",
    "    name=exptname,\n",
    "    tags=[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acbb58edb134f6fb1ed8313e7846011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.99740 score: 0.348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5e819af4524c468bc1b79bcfd7c8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.92324 score: 0.412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d8df0338c9454398562962514a364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 44.99101 score: 0.352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34c6b48e3cc4e0ba4c4c62fc829825e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 27.17856 score: 0.263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2b4c0337794dc399c4b89ad43f580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 86.65612 score: 0.040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ca0e6c4d904e3ba2d9b74b886e66f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 22.45465 score: 0.125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e5d7ee8a9a4d79b8bf5309cebaaab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 13.60389 score: 0.364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2882eed94cb48e9a51f3314273c5f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 80.31060 score: 0.386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70de71bededc44a297207216dbfcaff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 28.85929 score: 0.117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b7d0eb376e4a2d89940329213a83a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 15.22840 score: 0.265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28824674c2224f16b3a489a7e687c962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 9.68697 score: 0.270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58597367983247a2bcce5be6124dd927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 8.23266 score: 0.261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f82110d30d46c2aa1a750ce22fda23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 6.91352 score: 0.241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29159de5aada40fc8daa1c329f6f1861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 5.36580 score: 0.245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334277091bdb41599e4d7bfe49579855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 4.45518 score: 0.301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfddd3424d8499aa716a6378e913118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 3.69559 score: 0.193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb097f2197244eaab9a07c3b02ae7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 7.12581 score: 0.225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fceabb5b6f45e6ab8abb4fa26323db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 6.90438 score: 0.310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90c96ed28034f29876c0871378ce4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yamaguchi/kaggle/experiments/1030-all-サブグラフを考慮しない/all.ipynb セル 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dftrain \u001b[39m=\u001b[39m dataset_dict[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dfvalid \u001b[39m=\u001b[39m dataset_dict[\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m train_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     dftrain\u001b[39m=\u001b[39;49mdftrain,\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     dfvalid\u001b[39m=\u001b[39;49mdfvalid,\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     const\u001b[39m=\u001b[39;49mconst,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     cat_status\u001b[39m=\u001b[39;49mcat_status,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     cat_config_status\u001b[39m=\u001b[39;49mcat_config_status,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     savedir\u001b[39m=\u001b[39;49mworkdir,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     checkpoint_dir\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m wandb\u001b[39m.\u001b[39malert(title\u001b[39m=\u001b[39mexptname, text\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain End\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/yamaguchi/kaggle/experiments/1030-all-サブグラフを考慮しない/all.ipynb セル 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m loss \u001b[39m=\u001b[39m rankNet(\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     torch\u001b[39m.\u001b[39mreshape(out, (\u001b[39m1\u001b[39m, out\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])),\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     torch\u001b[39m.\u001b[39mreshape(target, (\u001b[39m1\u001b[39m, target\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])),\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m graph_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m pred, truth \u001b[39m=\u001b[39m to_cpu_numpy(params, out, target)\n\u001b[1;32m    <a href='vscode-notebook-cell://tunnel%2Bdrumehiron-calc/home/yamaguchi/kaggle/experiments/1030-all-%E3%82%B5%E3%83%96%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E8%80%83%E6%85%AE%E3%81%97%E3%81%AA%E3%81%84/all.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m preds\u001b[39m.\u001b[39mappend(pred)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_everything(43)\n",
    "dftrain = dataset_dict[\"train\"]\n",
    "dfvalid = dataset_dict[\"valid\"]\n",
    "\n",
    "train_model(\n",
    "    dftrain=dftrain,\n",
    "    dfvalid=dfvalid,\n",
    "    params=params,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    "    savedir=workdir,\n",
    "    checkpoint_dir=None,\n",
    ")\n",
    "wandb.alert(title=exptname, text=f\"Train End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedir = workdir / f\"{arch}-{perm}\"\n",
    "# dflog = pd.read_csv(savedir / \"log.csv\")\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "# for i, ds in enumerate([\"train\", \"valid\"]):\n",
    "#     dflog_ = dflog.query(\"(phase == @ds)\").groupby(\"epoch\")\n",
    "#     axes[i][0].plot(dflog_[\"current_loss\"].mean(), label=\"total\")\n",
    "#     axes[i][1].plot(dflog_[\"score\"].mean(), label=\"taotal\")\n",
    "#     if i == 0:\n",
    "#         axes[i][0].legend()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a4ca9e0bac4c669dd27091f29f656a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savedir = workdir\n",
    "\n",
    "records = []\n",
    "\n",
    "dftest = dataset_dict[\"test\"]\n",
    "\n",
    "test_layout_dataset = LayoutDataset(\n",
    "    dataset=dftest,\n",
    "    params=params,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model = SimpleLayoutModel(\n",
    "    params=params,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model.load_state_dict(torch.load(workdir / \"final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "with tqdm(range(len(test_layout_dataset))) as pbar:\n",
    "    for i in pbar:\n",
    "        file_info = test_layout_dataset.get_ith_file_info(i)\n",
    "\n",
    "        pred_list = []\n",
    "        for (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        ) in test_layout_dataset.getitem_as_batch(i):\n",
    "            pred_batch = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            if params.device == \"cuda\":\n",
    "                pred_batch = pred_batch.cpu().detach().numpy()\n",
    "            else:\n",
    "                pred_batch = pred_batch.detach().numpy()\n",
    "            # pred_batchは高いものほどよい\n",
    "            pred_batch = -pred_batch\n",
    "            pred_list.append(pred_batch)\n",
    "\n",
    "            del (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            )\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pred = np.hstack(pred_list)\n",
    "\n",
    "        ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n",
    "        records.append({\"ID\": ID, \"pred\": \";\".join(list(map(str, pred.argsort())))})\n",
    "\n",
    "del test_layout_dataset, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dfpred = pd.DataFrame(records)\n",
    "dfsub = pd.read_csv(inputdir / \"sample_submission.csv\")\n",
    "dfsub = dfsub.merge(dfpred, on=\"ID\", how=\"left\")\n",
    "dfsub[\"TopConfigs\"] = np.where(\n",
    "    dfsub[\"pred\"].isnull(), dfsub[\"TopConfigs\"], dfsub[\"pred\"]\n",
    ")\n",
    "dfsub[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission_final_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.alert(title=exptname, text=f\"Inference End\")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
