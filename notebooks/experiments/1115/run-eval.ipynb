{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"tags":[]},"outputs":[],"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import itertools\n","import random\n","import gc\n","import torch\n","import os\n","from copy import deepcopy\n","from torch import nn\n","from torch.utils.data import Dataset\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torch_geometric.nn import GCNConv, Sequential\n","from tqdm.notebook import tqdm\n","from pathlib import Path\n","from dataclasses import dataclass, field\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import wandb\n","from dataclasses import asdict\n","import json\n","\n","sns.set()\n","\n","warnings.simplefilter(\"ignore\")\n","\n","GPU = \"cuda:0\""]},{"cell_type":"markdown","metadata":{},"source":["## データセットを準備\n"]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[]},"outputs":[],"source":["rootdir = Path().resolve().parent.parent\n","inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n","node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout7-85-dataset\"\n","tile_node_feat_dir = rootdir / \"data\" / \"extra-feat-1114\"\n","trans_node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout6-92-dataset\"\n","trans_node_config_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout7-81-dataset\"\n","workdir = Path().resolve() / \"out\"\n","workdir.mkdir(exist_ok=True, parents=True)"]},{"cell_type":"code","execution_count":20,"metadata":{"tags":[]},"outputs":[],"source":["dataset_dict = {}\n","ignores = []\n","for ds in [\"train\", \"valid\", \"test\"]:\n","    records = []\n","    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n","        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n","        for filepath in sorted(datadir.glob(\"*.npz\")):\n","            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n","\n","            # if (ds != \"test\") and ((\"mlperf\" in filename) or (\"openai\" in filename)):\n","            #     ignores.append(filepath)\n","            #     continue\n","            records.append(\n","                {\n","                    \"arch\": arch,\n","                    \"perm\": perm,\n","                    \"filename\": filename,\n","                    \"filepath\": filepath,\n","                    \"node_feat_filepath\": str(\n","                        node_feat_dir / arch / perm / ds / f\"{filename}.npz\"\n","                    ),\n","                    \"tile_node_feat_filepath\": str(\n","                        tile_node_feat_dir / arch / perm / ds / f\"{filename}.npz\"\n","                    ),\n","                    \"trans_node_feat_filepath\": str(\n","                        trans_node_feat_dir\n","                        / \"layout\"\n","                        / arch\n","                        / perm\n","                        / ds\n","                        / f\"{filename}.npz\"\n","                    ),\n","                    \"trans_node_config_filepath\": str(\n","                        trans_node_config_feat_dir\n","                        / arch\n","                        / perm\n","                        / ds\n","                        / f\"{filename}.npz\"\n","                    ),\n","                }\n","            )\n","    dataset_dict[ds] = pd.DataFrame(records)"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[]},"outputs":[],"source":["# for filepath in tqdm(ignores):\n","#     node_config_feat = np.load(filepath)[\"node_config_feat\"]\n","\n","#     for i in range(1, node_config_feat.shape[0]):\n","#         if not (node_config_feat[0] == node_config_feat[i]).all():\n","#             filepath\n","#             break"]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>num_dims</th>\n","      <th>num_cats</th>\n","      <th>cats</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>68</td>\n","      <td>6</td>\n","      <td>[0, 1, 2, 3, 4, 5]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   number  num_dims  num_cats  \\\n","0       0         1        19   \n","1       1        68         6   \n","\n","                                                cats  \n","0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n","1                                 [0, 1, 2, 3, 4, 5]  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dfcat = pd.DataFrame(\n","    [\n","        {\"number\": 0, \"num_dims\": 1, \"num_cats\": 19, \"cats\": list(range(19))},\n","        {\"number\": 1, \"num_dims\": 54 + 14, \"num_cats\": 6, \"cats\": list(range(6))},\n","    ]\n",")\n","dfcat.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>number</th>\n","      <th>num_dims</th>\n","      <th>num_cats</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>18</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   number  num_dims  num_cats\n","0       0        18         8"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dfcat_config = pd.DataFrame(\n","    [\n","        {\n","            \"number\": 0,\n","            \"num_dims\": 18,\n","            \"num_cats\": 8,\n","        },  # output_layout, input_layout, kernel_layout\n","    ]\n",")\n","dfcat_config"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["for ds in dataset_dict:\n","    for i, row in dataset_dict[ds].iterrows():\n","        np.load(row[\"filepath\"])\n","        np.load(row[\"node_feat_filepath\"])\n","        np.load(row[\"tile_node_feat_filepath\"])\n","        np.load(row[\"trans_node_feat_filepath\"])\n","        np.load(row[\"trans_node_config_filepath\"])"]},{"cell_type":"markdown","metadata":{},"source":["# データクラスを定義\n"]},{"cell_type":"code","execution_count":8,"metadata":{"tags":[]},"outputs":[],"source":["@dataclass\n","class CatStatus:\n","    dfcat: pd.DataFrame\n","    prefix: str\n","    num_cat_dict: dict[str, int] = field(init=False)\n","    index_dict: dict[str, list[int]] = field(init=False)\n","\n","    def __post_init__(self) -> None:\n","        self.num_cat_dict, self.index_dict = {}, {}\n","        dim_start = 0\n","        for i, row in self.dfcat.iterrows():\n","            self.num_cat_dict[f\"{self.prefix}cat_feat{i + 1}\"] = row[\"num_cats\"]\n","            self.index_dict[f\"{self.prefix}cat_feat{i + 1}\"] = list(\n","                range(dim_start, dim_start + row[\"num_dims\"])\n","            )\n","            dim_start += row[\"num_dims\"]\n","\n","\n","cat_status = CatStatus(dfcat=dfcat, prefix=\"\")\n","cat_config_status = CatStatus(dfcat=dfcat_config, prefix=\"config_\")\n","\n","\n","@dataclass\n","class Const:\n","    num_node_flag_feat_dim: int\n","    num_node_cont_feat_dim: int\n","    num_node_cat_feat_dim: int\n","    num_node_config_cont_feat_dim: int\n","\n","    # 演算子の種類\n","    num_operations: int = 120\n","    # 各configの次元数\n","    num_config_dims: int = 6\n","\n","\n","fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"node_feat_filepath\"])\n","tile_fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"tile_node_feat_filepath\"])\n","trans_fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"trans_node_feat_filepath\"])\n","trans_config_fileobj = np.load(\n","    dataset_dict[\"train\"].iloc[0][\"trans_node_config_filepath\"]\n",")\n","\n","node_flag_feat, node_cont_feat = fileobj[\"node_flag_feat\"], fileobj[\"node_cont_feat\"]\n","node_enum_feat, node_dimension_number_feat = (\n","    fileobj[\"node_enum_feat\"],\n","    fileobj[\"node_dimension_number_feat\"],\n",")\n","node_tile_cont_feat = tile_fileobj[\"node_feat\"]\n","trans_node_cont_feat, trans_node_cat_feat = (\n","    trans_fileobj[\"node_cont_feat\"],\n","    trans_fileobj[\"node_cat_feat\"],\n",")\n","trans_node_config_cont_feat = trans_config_fileobj[\"node_config_cont_feat\"]\n","const = Const(\n","    num_node_flag_feat_dim=node_flag_feat.shape[1] + 1,  # config_idsの分+1\n","    num_node_cont_feat_dim=node_cont_feat.shape[1]\n","    + trans_node_cont_feat.shape[1]\n","    + node_tile_cont_feat.shape[1],\n","    num_node_cat_feat_dim=node_enum_feat.shape[1]\n","    + node_dimension_number_feat.shape[1]\n","    + trans_node_cat_feat.shape[1],\n","    num_node_config_cont_feat_dim=trans_node_config_cont_feat.shape[2],\n",")\n","\n","\n","@dataclass\n","class NodeFeatExtractor:\n","    dims: list[int] = field(default_factory=lambda: [64, 64])\n","    leakyrelu_negative_slope: float = 0.1\n","    dropout_p: float = 0.1\n","\n","\n","@dataclass\n","class GNNExtractor:\n","    dims: list[int] = field(default_factory=lambda: [64, 64])\n","    leakyrelu_negative_slope = 0.1\n","    dropout_p: float = 0.1\n","\n","\n","@dataclass\n","class CatEmbedding:\n","    num_cat: int\n","    embedding_dim: int\n","\n","\n","@dataclass\n","class Params:\n","    device: str\n","    cat_embeddings: dict[str, CatEmbedding]\n","    random_batch_size: int = 25\n","    batch_size: int = 25\n","    node_feat_extractor: NodeFeatExtractor = field(\n","        default_factory=lambda: NodeFeatExtractor(dropout_p=0)\n","    )\n","    node_config_feat_extractor: NodeFeatExtractor = field(\n","        default_factory=lambda: NodeFeatExtractor(dropout_p=0)\n","    )\n","    gnn_extractor: GNNExtractor = field(\n","        default_factory=lambda: GNNExtractor(dropout_p=0)\n","    )\n","    subgraph_extractor: NodeFeatExtractor = field(\n","        default_factory=lambda: NodeFeatExtractor(dropout_p=0)\n","    )\n","    epoch: int = 20\n","    T_max: int = 20\n","    eta_min: float = 1e-5\n","    lr: float = 1e-3\n","    weight_decay: float = 0\n","    grad_clip_max_norm: float = 1.0\n","    grad_clip_norm_type: float = 2.0\n","\n","    num_max_nodes: int = 45000\n","\n","\n","cat_embeddings = {}\n","cat_embeddings.update(\n","    {\"op\": CatEmbedding(num_cat=const.num_operations, embedding_dim=16)}\n",")\n","cat_embeddings.update(\n","    {\n","        k: CatEmbedding(num_cat=v, embedding_dim=16)\n","        for k, v in cat_status.num_cat_dict.items()\n","    }\n",")\n","cat_embeddings.update(\n","    {\n","        k: CatEmbedding(num_cat=v, embedding_dim=8)\n","        for k, v in cat_config_status.num_cat_dict.items()\n","    }\n",")\n","params = Params(\n","    device=GPU if torch.cuda.is_available() else \"cpu\",\n","    cat_embeddings=cat_embeddings,\n",")\n","\n","\n","@dataclass\n","class LayoutConfigs:\n","    \"\"\"\n","    Attributes\n","    ----------\n","    node_cont_feat: np.ndarray\n","        ノード特徴量、(ノード数, 108)\n","\n","    node_cat_feat: np.ndarray\n","        ノード特徴量、(ノード数, 3)\n","\n","    node_opcode: np.ndarray\n","        ノード演算子、(ノード数,)\n","    edge_index: np.ndarray\n","        エッジ、(エッジ数, 2)\n","\n","    node_config_feat: np.ndarray\n","        設定毎のノード特徴量、(設定数, 設定可能なノード数, 3)\n","\n","    node_config_ids: np.ndarray\n","        設定可能なノードのIndex、(設定可能なノード数,)\n","    config_runtime: np.ndarray\n","        実行時間、(設定数,)\n","    node_splits: np.ndarray\n","        同じパーティションでの計算を意味する。今回は使用しない。(パーティション数, 2)\n","    \"\"\"\n","\n","    node_flag_feat: np.ndarray\n","    node_cont_feat: np.ndarray\n","    node_cat_feat: np.ndarray\n","    node_opcode: np.ndarray\n","    edge_index: np.ndarray\n","    node_config_feat: np.ndarray\n","    node_config_cont_feat: np.ndarray\n","    node_config_ids: np.ndarray\n","    config_runtime: np.ndarray\n","    node_splits: np.ndarray\n","\n","    cat_status: CatStatus\n","    cat_config_status: CatStatus\n","    target: np.ndarray = field(init=False)\n","    argsorted_indexs: list[int] = field(init=False)\n","\n","    NUM_SAMPLES: int = 1000\n","\n","    def __post_init__(self) -> None:\n","        # 設定が存在するノードのフラグ\n","        node_active_feat = np.zeros((self.num_nodes, 1))\n","        node_active_feat[self.node_config_ids, :] = 1\n","        self.node_flag_feat = np.concatenate(\n","            [self.node_flag_feat, node_active_feat], axis=1\n","        )\n","        self.node_cont_feat = self.apply_normalization(x=self.node_cont_feat)\n","        self.node_config_feat = self.node_config_feat + 1  # カテゴリは0~7にする\n","        self.node_splits = np.array(\n","            [\n","                [self.node_splits[0][i], self.node_splits[0][i + 1] - 1]\n","                for i in range(self.node_splits.shape[1] - 1)\n","            ]\n","        )\n","        self.target = self.apply_target_ranking(x=self.config_runtime)\n","        self.argsorted_indexs = np.argsort(self.config_runtime).tolist()\n","\n","    @property\n","    def num_nodes(self) -> int:\n","        \"\"\"ノード数\"\"\"\n","        return self.node_cont_feat.shape[0]\n","\n","    def get_random_config_idxs(self) -> list[int]:\n","        \"\"\"tpu_graphのサンプリング方法\n","        https://github.com/google-research-datasets/tpu_graphs/blob/main/tpu_graphs/baselines/layout/data.py#L352\n","        \"\"\"\n","        num_configs = self.config_runtime.shape[0]\n","        num_samples = min(self.NUM_SAMPLES, num_configs)\n","\n","        samples = random.sample(list(range(num_configs)), num_samples)\n","        return samples\n","\n","        # third = num_samples // 3\n","\n","        # middle_samples = np.random.choice(\n","        #     self.argsorted_indexs[third:-third], num_samples - 2 * third\n","        # ).tolist()\n","        # samples = (\n","        #     self.argsorted_indexs[:third]\n","        #     + self.argsorted_indexs[-third:]\n","        #     + middle_samples\n","        # )\n","        # samples = random.sample(samples, len(samples))\n","\n","        # return samples\n","\n","    def get_filled_node_config_feat(\n","        self, index_list: list[int]\n","    ) -> tuple[np.ndarray, np.ndarray]:\n","        \"\"\"指定された設定の設定毎のノード特徴量を取得する。設定がない場合は補完する。\n","        Parameters\n","        ----------\n","        index_list: list[int]\n","            設定のIndex\n","\n","        Returns\n","        -------\n","        np.ndarray [(len(index_list),ノード数, 18), (len(index_list),ノード数, 連続次元数)]\n","        \"\"\"\n","        # (サンプル数, ノード数) x 3\n","        node_config_feat = np.full(\n","            (len(index_list), self.num_nodes, Const.num_config_dims * 3),\n","            Const.num_config_dims + 1,\n","        )\n","        node_config_feat[:, self.node_config_ids] = self.node_config_feat[\n","            index_list, :, :\n","        ]\n","\n","        node_config_cont_feat = np.zeros(\n","            (len(index_list), self.num_nodes, self.node_config_cont_feat.shape[2])\n","        )\n","        node_config_cont_feat[:, self.node_config_ids] = self.node_config_cont_feat[\n","            index_list, :, :\n","        ]\n","        return node_config_feat, node_config_cont_feat\n","\n","    def get_target(self, index_list: list[int]) -> np.ndarray:\n","        \"\"\"指定された設定の目的変数を取得する\n","\n","        Parameters\n","        ----------\n","        index_list: list[int]\n","            設定のIndex\n","\n","        Returns\n","        -------\n","        np.ndarray\n","        \"\"\"\n","        return self.target[index_list]\n","\n","    def apply_normalization(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"特徴量の正規化\n","\n","        Parameters\n","        ----------\n","        x: np.ndarray\n","            2次元行列\n","\n","        Returns\n","        -------\n","        x: np.ndarray\n","            行方向に正規化された行列\n","        \"\"\"\n","        x /= 128\n","        x = np.where(x >= 0, np.log1p(x / 128), -np.log1p(-x / 128))\n","        return x\n","\n","    def apply_target_normalization(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"目的変数の正規化\n","\n","        Parameters\n","        ----------\n","        x: np.ndarray\n","            ベクトル\n","\n","        Returns\n","        -------\n","        x: np.ndarray\n","            正規化されたベクトル\n","        \"\"\"\n","        return np.log(x / x.min())\n","\n","    def apply_target_ranking(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"降順でランキング\"\"\"\n","        return np.argsort(np.argsort(-x))"]},{"cell_type":"markdown","metadata":{},"source":["## データセットを定義\n"]},{"cell_type":"code","execution_count":9,"metadata":{"tags":[]},"outputs":[],"source":["class LayoutDataset(Dataset):\n","    \"\"\"\n","    Attributes\n","    ----------\n","    rows: list[dict[str, np.ndarray]]\n","        設定をリストでもつ\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dataset: pd.DataFrame,\n","        params: Params,\n","        cat_status: CatStatus,\n","        cat_config_status: CatStatus,\n","    ) -> None:\n","        self.rows = dataset.to_dict(\"records\")\n","        self.params = params\n","        self.cat_status = cat_status\n","        self.cat_config_status = cat_config_status\n","        self.cache_idx = None\n","        self.cache_filepath = None\n","\n","    @property\n","    def device(self) -> str:\n","        return self.params.device\n","\n","    def __len__(self) -> int:\n","        return len(self.rows)\n","\n","    def create_layout_config(self, idx: int) -> LayoutConfigs:\n","        if self.cache_idx != idx:\n","            self.cache_idx = idx\n","            fileobj = np.load(self.rows[self.cache_idx][\"filepath\"])\n","            node_feat_fileobj = np.load(self.rows[self.cache_idx][\"node_feat_filepath\"])\n","            tile_node_feat_fileobj = np.load(\n","                self.rows[self.cache_idx][\"tile_node_feat_filepath\"]\n","            )\n","            trans_feat_fileobj = np.load(\n","                self.rows[self.cache_idx][\"trans_node_feat_filepath\"]\n","            )\n","            trans_config_feat_fileobj = np.load(\n","                self.rows[self.cache_idx][\"trans_node_config_filepath\"]\n","            )\n","\n","            node_cont_feat = np.concatenate(\n","                [\n","                    node_feat_fileobj[\"node_cont_feat\"],\n","                    trans_feat_fileobj[\"node_cont_feat\"],\n","                    tile_node_feat_fileobj[\"node_feat\"],\n","                ],\n","                axis=1,\n","            )\n","\n","            node_cat_feat = np.concatenate(\n","                [\n","                    node_feat_fileobj[\"node_enum_feat\"],\n","                    node_feat_fileobj[\"node_dimension_number_feat\"],\n","                    trans_feat_fileobj[\"node_cat_feat\"],\n","                ],\n","                axis=1,\n","            )\n","\n","            self.cache_layout_config = LayoutConfigs(\n","                node_opcode=fileobj[\"node_opcode\"],\n","                edge_index=fileobj[\"edge_index\"],\n","                node_config_ids=fileobj[\"node_config_ids\"],\n","                config_runtime=fileobj[\"config_runtime\"],\n","                node_splits=fileobj[\"node_splits\"],\n","                node_flag_feat=node_feat_fileobj[\"node_flag_feat\"],\n","                node_cont_feat=node_cont_feat,\n","                node_cat_feat=node_cat_feat,\n","                node_config_feat=fileobj[\"node_config_feat\"],\n","                node_config_cont_feat=trans_config_feat_fileobj[\n","                    \"node_config_cont_feat\"\n","                ],\n","                cat_status=self.cat_status,\n","                cat_config_status=self.cat_config_status,\n","            )\n","        return self.cache_layout_config\n","\n","    def __getitem__(\n","        self, idx: int\n","    ) -> tuple[\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","    ]:\n","        raise NotImplementedError()\n","\n","    def getitem_as_random_batch(\n","        self, idx: int\n","    ) -> list[\n","        tuple[\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","        ]\n","    ]:\n","        layout_configs = self.create_layout_config(idx=idx)\n","\n","        index_list = layout_configs.get_random_config_idxs()\n","        for i_chunk in range(0, len(index_list), self.params.random_batch_size):\n","            chunk_index_list = index_list[\n","                i_chunk : i_chunk + self.params.random_batch_size\n","            ]\n","            yield self._get_tensors(\n","                layout_configs=layout_configs, index_list=chunk_index_list\n","            )\n","\n","    def getitem_as_batch(\n","        self, idx: int\n","    ) -> list[\n","        tuple[\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","            torch.Tensor,\n","        ]\n","    ]:\n","        \"\"\"設定をバッチで取得する\"\"\"\n","        layout_configs = self.create_layout_config(idx=idx)\n","        batch_size = int(\n","            self.params.batch_size\n","            * (self.params.num_max_nodes / layout_configs.num_nodes)\n","        )\n","\n","        index_list = list(range(layout_configs.config_runtime.shape[0]))\n","        for i_chunk in range(0, len(index_list), batch_size):\n","            chunk_index_list = index_list[i_chunk : i_chunk + batch_size]\n","            yield self._get_tensors(\n","                layout_configs=layout_configs, index_list=chunk_index_list\n","            )\n","\n","    def _get_tensors(\n","        self, layout_configs: LayoutConfigs, index_list: list[int]\n","    ) -> tuple[\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","        torch.Tensor,\n","    ]:\n","        \"\"\"渡された設定のIndexのテンソルを取得する\n","\n","        Parameters\n","        ----------\n","        layout_configs: LayoutConfigs\n","            Layoutのデータクラス\n","        index_list: list[int]\n","            設定のインデックス\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","            ノード特徴量(フラグ)\n","        torch.Tensor\n","            ノード特徴量(連続)\n","        dict[str, torch.Tensor]\n","            ノード特徴量(カテゴリ)\n","        torch.Tensor\n","            設定毎のノード特徴量\n","        torch.Tensor\n","            設定毎のノード特徴量(連続)\n","        torch.Tensor\n","            ノード演算子\n","        torch.Tensor\n","            エッジ\n","        torch.Tensor\n","            目的変数\n","        \"\"\"\n","        # ノード特徴量(フラグ)\n","        node_flag_feat = torch.tensor(\n","            layout_configs.node_flag_feat,\n","            dtype=torch.float32,\n","        ).to(self.device)\n","        # ノード特徴量(連続)\n","        node_cont_feat = torch.tensor(\n","            layout_configs.node_cont_feat,\n","            dtype=torch.float32,\n","        ).to(self.device)\n","        # ノード特徴量(カテゴリ)\n","        node_cat_feat = torch.tensor(\n","            layout_configs.node_cat_feat,\n","            dtype=torch.int64,\n","        ).to(self.device)\n","        # 設定毎のノード特徴量(カテゴリ)\n","        (\n","            node_config_feat,\n","            node_config_cont_feat,\n","        ) = layout_configs.get_filled_node_config_feat(index_list=index_list)\n","        node_config_feat = torch.tensor(node_config_feat, dtype=torch.int64).to(\n","            self.device\n","        )\n","        node_config_cont_feat = torch.tensor(\n","            node_config_cont_feat, dtype=torch.float32\n","        ).to(self.device)\n","        # ノード演算子\n","        node_opcode = torch.tensor(layout_configs.node_opcode, dtype=torch.int64).to(\n","            self.device\n","        )\n","        # エッジ\n","        edge_index = torch.tensor(\n","            np.swapaxes(layout_configs.edge_index, 0, 1), dtype=torch.int64\n","        ).to(self.device)\n","        # サブグラフ\n","        node_splits = torch.tensor(layout_configs.node_splits, dtype=torch.int64).to(\n","            self.device\n","        )\n","        # 設定のids\n","        node_config_ids = torch.tensor(\n","            layout_configs.node_config_ids, dtype=torch.int64\n","        ).to(self.device)\n","        # ターゲット\n","        target = torch.tensor(\n","            layout_configs.get_target(index_list=index_list),\n","            dtype=torch.float32,\n","        ).to(self.device)\n","\n","        return (\n","            node_opcode,\n","            node_flag_feat,\n","            node_cont_feat,\n","            node_cat_feat,\n","            node_config_feat,\n","            node_config_cont_feat,\n","            edge_index,\n","            node_splits,\n","            node_config_ids,\n","            target,\n","        )\n","\n","    def get_ith_file_info(self, i: int) -> dict[str, str]:\n","        row = self.rows[i]\n","        return {\n","            \"arch\": row[\"arch\"],\n","            \"perm\": row[\"perm\"],\n","            \"filename\": row[\"filename\"],\n","        }\n","\n","    def get_ith_runtime(self, i: int) -> np.ndarray:\n","        layout_configs = self.create_layout_config(idx=i)\n","        return layout_configs.config_runtime"]},{"cell_type":"markdown","metadata":{},"source":["## モデルを定義\n"]},{"cell_type":"code","execution_count":10,"metadata":{"tags":[]},"outputs":[],"source":["from torch_geometric.nn import MessagePassing\n","\n","\n","class EdgeConv(MessagePassing):\n","    \"\"\"\n","    ノード特徴 + 隣接ノード特徴 + 隣接ノード特徴の一致\n","    参考： https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html#implementing-the-edge-convolution\n","    補足: 集約関数はデフォルトでdim(axis) = -2。つまりノード方向で集約するので気にしなくてOK\n","    https://github.com/pyg-team/pytorch_geometric/blob/1e12d41c28b1fb9793f17646b018071b508864d7/torch_geometric/nn/aggr/basic.py#L38\n","    \"\"\"\n","\n","    def __init__(self, x_input_dim: int, x_output_dim: int, dropout_p: float):\n","        # \"Add\" aggregation\n","        super().__init__(aggr=\"max\")\n","        self.mlp = nn.Sequential(\n","            nn.Dropout(dropout_p),\n","            # nn.LayerNorm(x_input_dim * 2),\n","            nn.Linear(x_input_dim * 2, x_output_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_p),\n","            # nn.LayerNorm(x_output_dim),\n","            nn.Linear(x_output_dim, x_output_dim),\n","        )\n","\n","    def forward(self, x, edge_index):\n","        # x has shape [設定数, N, in_channels]\n","        # edge_index has shape [2, E]\n","        return self.propagate(edge_index, x=x)\n","\n","    def message(self, x_i, x_j):\n","        \"\"\"propagate()で渡された引数xから自動でx_i, x_jノードを取り出して随時処理を実装する関数\"\"\"\n","        # x_i has shape [設定数, エッジ数, in_channels]\n","        # x_j has shape [設定数, エッジ数, in_channels]\n","        x_cat = torch.cat(\n","            [x_i, x_i - x_j], dim=2\n","        )  # tmp has shape [設定数, エッジ数, 2 * in_channels]\n","        return self.mlp(x_cat)\n","\n","\n","class SimpleLayoutModel(torch.nn.Module):\n","    \"\"\"\n","\n","    Attributes\n","    ----------\n","    params: Params\n","        実験設定のデータクラス\n","    node_embeddings: torch.Tensor\n","        カテゴリ変数の埋め込み表現(ノード毎)\n","    node_config_embeddings: torch.Tensor\n","        カテゴリ変数の埋め込み表現(設定xノード毎)\n","    node_feat_extractor: torch.nn.Module\n","        ノードの特徴量を抽出するネットワーク\n","    gnn_extractor: torch.nn.Module\n","        グラフの特徴量を抽出するネットワーク\n","    gc: torch.nn.Module\n","        最終層の全結合層\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        params: Params,\n","        const: Const,\n","        cat_status: CatStatus,\n","        cat_config_status: CatStatus,\n","    ) -> None:\n","        super().__init__()\n","        self.params = params\n","        self.cat_status = cat_status\n","        self.cat_config_status = cat_config_status\n","\n","        # カテゴリ変数の埋め込み表現\n","        self.embeddings = nn.ModuleDict(\n","            {\n","                k: torch.nn.Embedding(v.num_cat, v.embedding_dim)\n","                for k, v in self.params.cat_embeddings.items()\n","            }\n","        )\n","\n","        # node_featのfeature_extractorを定義\n","        num_node_feat_extractor_input_dim = (\n","            const.num_node_flag_feat_dim\n","            + const.num_node_cont_feat_dim\n","            + self.num_node_feat_embedding_dims\n","        )\n","\n","        node_feat_extractor_layer = []\n","        node_feat_extractor_dims = [\n","            num_node_feat_extractor_input_dim\n","        ] + self.params.node_feat_extractor.dims\n","        for i in range(len(node_feat_extractor_dims) - 1):\n","            node_feat_extractor_layer += [\n","                # nn.LayerNorm(node_feat_extractor_dims[i]),\n","                nn.Dropout(params.node_feat_extractor.dropout_p),\n","                nn.Linear(\n","                    in_features=node_feat_extractor_dims[i],\n","                    out_features=node_feat_extractor_dims[i + 1],\n","                ),\n","                nn.LeakyReLU(params.node_feat_extractor.leakyrelu_negative_slope),\n","            ]\n","            self.node_feat_extractor = nn.Sequential(*node_feat_extractor_layer)\n","\n","        # node_config_featのfeature_extractorを定義\n","        num_node_config_feat_extractor_input_dim = (\n","            self.num_node_config_feat_embedding_dims\n","            + const.num_node_config_cont_feat_dim\n","        )\n","\n","        node_config_feat_extractor_layer = []\n","        node_config_feat_extractor_dims = [\n","            num_node_config_feat_extractor_input_dim\n","        ] + self.params.node_config_feat_extractor.dims\n","        for i in range(len(node_feat_extractor_dims) - 1):\n","            node_config_feat_extractor_layer += [\n","                # nn.LayerNorm(node_config_feat_extractor_dims[i]),\n","                nn.Dropout(params.node_config_feat_extractor.dropout_p),\n","                nn.Linear(\n","                    in_features=node_config_feat_extractor_dims[i],\n","                    out_features=node_config_feat_extractor_dims[i + 1],\n","                ),\n","                nn.LeakyReLU(\n","                    params.node_config_feat_extractor.leakyrelu_negative_slope\n","                ),\n","            ]\n","        self.node_config_feat_extractor = nn.Sequential(\n","            *node_config_feat_extractor_layer\n","        )\n","\n","        # ノード間のfeature_extractorの定義\n","        num_gnn_extractor_input_dim = (\n","            node_feat_extractor_dims[-1] + node_config_feat_extractor_dims[-1]\n","        )\n","\n","        gnn_extractor_layer = []\n","        gnn_extractor_dims = [\n","            num_gnn_extractor_input_dim\n","        ] + self.params.gnn_extractor.dims\n","        for i in range(len(gnn_extractor_dims) - 1):\n","            gnn_extractor_layer += [\n","                (\n","                    EdgeConv(\n","                        x_input_dim=gnn_extractor_dims[i],\n","                        x_output_dim=gnn_extractor_dims[i + 1],\n","                        dropout_p=params.gnn_extractor.dropout_p,\n","                    ),\n","                    \"x, edge_index -> x\",\n","                ),\n","                nn.LeakyReLU(params.gnn_extractor.leakyrelu_negative_slope),\n","            ]\n","        self.gnn_extractor = Sequential(\"x, edge_index\", gnn_extractor_layer)\n","\n","        fc_layer = [\n","            # nn.LayerNorm(subgraph_extractor_dims[-1]),\n","            # nn.Linear(in_features=subgraph_extractor_dims[-1], out_features=1),\n","            nn.Linear(\n","                in_features=self.params.gnn_extractor.dims[-1]\n","                + num_gnn_extractor_input_dim,\n","                out_features=1,\n","            ),\n","        ]\n","        self.fc = nn.Sequential(*fc_layer)\n","        self.to(self.params.device)\n","\n","    @property\n","    def num_node_feat_embedding_dims(self) -> int:\n","        num_embedding_dims = 0\n","        num_embedding_dims += 1 * self.params.cat_embeddings[\"op\"].embedding_dim\n","        for cat_name, cat_index in self.cat_status.index_dict.items():\n","            num_embedding_dims += (\n","                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n","            )\n","        return num_embedding_dims\n","\n","    @property\n","    def num_node_config_feat_embedding_dims(self) -> int:\n","        num_embedding_dims = 0\n","        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n","            num_embedding_dims += (\n","                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n","            )\n","        return num_embedding_dims\n","\n","    def forward(\n","        self,\n","        node_opcode: torch.Tensor,\n","        node_flag_feat: torch.Tensor,\n","        node_cont_feat: torch.Tensor,\n","        node_cat_feat: torch.Tensor,\n","        node_config_feat: torch.Tensor,\n","        node_config_cont_feat: torch.Tensor,\n","        edge_index: torch.Tensor,\n","        node_splits: torch.Tensor,\n","        node_config_ids: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Parameters\n","        ------\n","        node_flag_feat:\n","            ノードの特徴量(node数, フラグ次元数)\n","        node_cont_feat:\n","            ノードの特徴量(node数, 連続次元数)\n","        node_cat_feat:\n","            ノードの特徴量(node数, カテゴリ次元数*埋め込み次元数)\n","        node_config_feat:\n","            設定毎のノードの特徴量(設定数, node数, 特徴次元数)\n","        node_config_cont_feat:\n","            設定毎のノードの特徴量(設定数, node数, 連続次元数)\n","        edge_index:\n","            エッジ(2, エッジ数)\n","        node_splits:\n","            サブグラフのインデックス（サブグラフ数, 2)\n","\n","        Returns:\n","        torch.tensor: (設定数)\n","        \"\"\"\n","        # (ノード数,特徴数)のテンソルを作成\n","        node_feat = self._join_node_feature(\n","            node_opcode=node_opcode,\n","            node_flag_feat=node_flag_feat,\n","            node_cont_feat=node_cont_feat,\n","            node_cat_feat=node_cat_feat,\n","        )\n","\n","        # (設定数,ノード数,特徴数)のテンソルを作成\n","        node_config_feat = self._join_node_config_feature(\n","            node_config_feat=node_config_feat,\n","            node_config_cont_feat=node_config_cont_feat,\n","        )\n","\n","        # node_featの抽出器を通す\n","        extracted_node_feat = self.node_feat_extractor(node_feat)\n","\n","        # node_config_featの抽出器を通す\n","        extracted_node_config_feat = self.node_config_feat_extractor(node_config_feat)\n","\n","        # 設定毎のノード特徴に結合する\n","        extracted_feat = self._join_entire_node_config_feat(\n","            node_feat=extracted_node_feat,\n","            node_config_feat=extracted_node_config_feat,\n","        )\n","\n","        # GNN抽出器を通す\n","        conved_extracted_feat = self.gnn_extractor(\n","            x=extracted_feat,\n","            edge_index=edge_index,\n","        )\n","\n","        # 残差を足すイメージ\n","        concat_feat = torch.concat([extracted_feat, conved_extracted_feat], 2)\n","        concat_feat = concat_feat[:, node_config_ids, :]\n","\n","        # ノードの特徴量を足し合わせる(Global mean Pooling)\n","        # global_pool_feat = torch.mean(subgraph_extracted_feat, dim=1)\n","        global_pool_feat = torch.mean(concat_feat, dim=1)\n","\n","        return torch.squeeze(self.fc(global_pool_feat))\n","\n","    def _join_node_feature(\n","        self,\n","        node_opcode: torch.Tensor,\n","        node_flag_feat: torch.Tensor,\n","        node_cont_feat: torch.Tensor,\n","        node_cat_feat: torch.Tensor,\n","    ) -> torch.Tensor:\n","        \"\"\"node_featのテンソルを作成\"\"\"\n","        # ノードの埋め込み表現\n","        node_embeddings_list = []\n","        node_embeddings_list.append(self.embeddings[\"op\"](node_opcode))\n","        for cat_name, cat_index in self.cat_status.index_dict.items():\n","            node_embeddings = self.embeddings[cat_name](node_cat_feat[:, cat_index])\n","            node_embeddings = torch.reshape(\n","                node_embeddings,\n","                (-1, node_embeddings.shape[-2] * node_embeddings.shape[-1]),\n","            )\n","            node_embeddings_list.append(node_embeddings)\n","\n","        # ノード毎で埋め込み、結合(ノード数, 特徴数)\n","        node_embedding_feat = torch.concat(node_embeddings_list, 1)\n","        node_feat = torch.concat(\n","            [node_flag_feat, node_cont_feat, node_embedding_feat], 1\n","        )\n","        return node_feat\n","\n","    def _join_node_config_feature(\n","        self, node_config_feat: torch.Tensor, node_config_cont_feat: torch.Tensor\n","    ) -> torch.Tensor:\n","        \"\"\"node_config_featのテンソルを作成\"\"\"\n","        # 設定xノード毎で埋め込み(設定数, ノード数, 特徴数)\n","        node_config_embeddings_list = []\n","        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n","            node_embeddings = self.embeddings[cat_name](\n","                node_config_feat[:, :, cat_index]\n","            )\n","            node_embeddings = torch.reshape(\n","                node_embeddings,\n","                (\n","                    node_embeddings.shape[0],\n","                    -1,\n","                    node_embeddings.shape[-2] * node_embeddings.shape[-1],\n","                ),\n","            )\n","            node_config_embeddings_list.append(node_embeddings)\n","        node_config_feat = torch.concat(\n","            node_config_embeddings_list + [node_config_cont_feat], 2\n","        )\n","        return node_config_feat\n","\n","    def _join_entire_node_config_feat(\n","        self, node_feat: torch.Tensor, node_config_feat: torch.Tensor\n","    ) -> torch.Tensor:\n","        # ノード毎の特徴量を設定数だけ縦に並べる\n","        node_tiled_feat = torch.tile(\n","            torch.reshape(node_feat, (1, node_feat.shape[0], node_feat.shape[1])),\n","            (node_config_feat.shape[0], 1, 1),\n","        )\n","        return torch.concat([node_tiled_feat, node_config_feat], 2)"]},{"cell_type":"markdown","metadata":{},"source":["## 学習\n"]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[]},"outputs":[],"source":["class ListMLE(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","\n","    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","\n","        Parameters\n","        ----------\n","        logits: torch.Tensor\n","            予測（要素数, ）\n","        labels: torch.Tensor\n","            目的変数（要素数, ）\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","        \"\"\"\n","        # 正解をソート\n","        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n","        # 予測を正解順でソート\n","        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n","        # 予測値の最大値で予測値を引く（expの爆発予防）\n","        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n","        logits_sorted_by_true = logits_sorted_by_true - logits_max\n","        # ランキングが低いものから累積する(その後正解順に戻す)\n","        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n","            dims=[1]\n","        )\n","        # 誤差\n","        negative_log_likelihood = torch.sum(\n","            torch.log(cumsums) - logits_sorted_by_true, dim=1\n","        )\n","        return torch.mean(negative_log_likelihood)\n","\n","\n","def rankNet(y_pred, y_true):\n","    \"\"\"\n","    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n","    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n","    :param y_true: ground truth labels, shape [batch_size, slate_length]\n","    :return: loss value, a torch.Tensor\n","    \"\"\"\n","    y_pred = y_pred.clone()\n","    y_true = y_true.clone()\n","\n","    # here we generate every pair of indices from the range of document length in the batch\n","    document_pairs_candidates = list(\n","        itertools.product(range(y_true.shape[1]), repeat=2)\n","    )\n","\n","    pairs_true = y_true[:, document_pairs_candidates]\n","    selected_pred = y_pred[:, document_pairs_candidates]\n","\n","    # here we calculate the relative true relevance of every candidate pair\n","    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n","    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n","\n","    # here we filter just the pairs that are 'positive' and did not involve a padded instance\n","    # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n","    # positive ones for a simpler loss function formulation\n","    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n","\n","    pred_diffs = pred_diffs[the_mask]\n","\n","    weight = None\n","    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n","    # whether one document is better than the other and not about the actual difference in\n","    # their relevancy levels\n","    true_diffs = (true_diffs > 0).type(torch.float32)\n","    true_diffs = true_diffs[the_mask]\n","\n","    return nn.BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)\n","\n","\n","def to_cpu_numpy(\n","    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",") -> tuple[np.ndarray, np.ndarray]:\n","    if params.device == GPU:\n","        pred_ = pred.cpu().detach().numpy()\n","        truth_ = truth.cpu().detach().numpy()\n","        torch.cuda.empty_cache()\n","    else:\n","        pred_ = pred.detach().numpy()\n","        truth_ = truth.detach().numpy()\n","    return pred_, truth_"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from scipy.stats import kendalltau\n","\n","\n","def evaluate_score(dataset: LayoutDataset, model: torch.nn.Module) -> pd.DataFrame:\n","    \"\"\"データセット全件に対してコンペの評価指標を算出する\n","    https://www.kaggle.com/competitions/predict-ai-model-runtime/overview\n","    \"\"\"\n","    model.eval()\n","    # criterion = ListMLE()\n","\n","    records = []\n","    eval_preds = []\n","    # 各グラフ毎にスコアを算出\n","    for graph_index in range(len(dataset)):\n","        # グラフ毎に1000件をバッチに分けて取得\n","        preds, truths = [], []\n","        for (\n","            node_opcode,\n","            node_flag_feat,\n","            node_cont_feat,\n","            node_cat_feat,\n","            node_config_feat,\n","            node_config_cont_feat,\n","            edge_index,\n","            node_splits,\n","            node_config_ids,\n","            target,\n","        ) in dataset.getitem_as_random_batch(graph_index):\n","            pred = model(\n","                node_opcode=node_opcode,\n","                node_flag_feat=node_flag_feat,\n","                node_cont_feat=node_cont_feat,\n","                node_cat_feat=node_cat_feat,\n","                node_config_feat=node_config_feat,\n","                node_config_cont_feat=node_config_cont_feat,\n","                edge_index=edge_index,\n","                node_splits=node_splits,\n","                node_config_ids=node_config_ids,\n","            )\n","            pred, truth = to_cpu_numpy(params, pred, target)\n","            preds.append(pred)\n","            truths.append(truth)\n","\n","        preds, truths = np.hstack(preds), np.hstack(truths)\n","\n","        loss = rankNet(\n","            torch.tensor(preds.reshape(1, -1)),\n","            torch.tensor(truths.reshape(1, -1)),\n","        )\n","        graph_loss = loss.item()\n","        score = kendalltau(truths, preds).correlation\n","\n","        record = dataset.get_ith_file_info(graph_index)\n","        record.update(\n","            {\n","                \"graph_loss\": graph_loss,\n","                \"score\": score,\n","            }\n","        )\n","        records.append(record)\n","        eval_preds.append(preds)\n","    return pd.DataFrame(records), eval_preds"]},{"cell_type":"markdown","metadata":{},"source":["### 学習\n"]},{"cell_type":"code","execution_count":13,"metadata":{"tags":[]},"outputs":[],"source":["def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","def train_model(\n","    dftrain: pd.DataFrame,\n","    dfvalid: pd.DataFrame,\n","    params: Params,\n","    const: Const,\n","    cat_status: CatStatus,\n","    cat_config_status: CatStatus,\n","    savedir: Path,\n","    checkpointdir: Path = None,\n",") -> None:\n","    train_layout_dataset = LayoutDataset(\n","        dataset=dftrain,\n","        params=params,\n","        cat_status=cat_status,\n","        cat_config_status=cat_config_status,\n","    )\n","    valid_layout_dataset = LayoutDataset(\n","        dataset=dfvalid,\n","        params=params,\n","        cat_status=cat_status,\n","        cat_config_status=cat_config_status,\n","    )\n","\n","    model = SimpleLayoutModel(\n","        params=params,\n","        const=const,\n","        cat_status=cat_status,\n","        cat_config_status=cat_config_status,\n","    )\n","    if checkpointdir is not None:\n","        print(\"学習済みモデルを読み込みます\")\n","        model.load_state_dict(torch.load(checkpointdir / f\"final_model.pt\"))\n","\n","    optimizer = torch.optim.Adam(\n","        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n","    )\n","    scheduler = CosineAnnealingLR(\n","        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n","    )\n","    # criterion = ListMLE()\n","\n","    best_score = -np.inf\n","    records = []\n","    num_train_log, num_valid_log = 0, 0\n","    for epoch in range(params.epoch):\n","        model.train()\n","\n","        num_graph = len(train_layout_dataset)\n","        pbar = tqdm(range(num_graph))\n","        graph_indexes = random.sample(list(range(num_graph)), num_graph)\n","\n","        epoch_losses = []\n","        epoch_loss = 0  # 各グラフの誤差を総和（エポックの誤差）\n","\n","        # グラフをシャッフルして取得\n","        for i_graph, graph_index in enumerate(graph_indexes):\n","            graph_info = train_layout_dataset.get_ith_file_info(graph_index)\n","            graph_arch, graph_perm = graph_info[\"arch\"], graph_info[\"perm\"]\n","            # 各グラフで1000件をバッチに分けて取得\n","            preds, truths = [], []\n","            graph_loss = 0  # バッチの誤差を総和（グラフの誤差）\n","            num_batch_count = 0\n","            for (\n","                node_opcode,\n","                node_flag_feat,\n","                node_cont_feat,\n","                node_cat_feat,\n","                node_config_feat,\n","                node_config_cont_feat,\n","                edge_index,\n","                node_splits,\n","                node_config_ids,\n","                target,\n","            ) in train_layout_dataset.getitem_as_random_batch(graph_index):\n","                out = model(\n","                    node_opcode=node_opcode,\n","                    node_flag_feat=node_flag_feat,\n","                    node_cont_feat=node_cont_feat,\n","                    node_cat_feat=node_cat_feat,\n","                    node_config_feat=node_config_feat,\n","                    node_config_cont_feat=node_config_cont_feat,\n","                    edge_index=edge_index,\n","                    node_splits=node_splits,\n","                    node_config_ids=node_config_ids,\n","                )\n","                # loss = criterion(\n","                #     torch.reshape(out, (1, out.shape[0])),\n","                #     torch.reshape(target, (1, target.shape[0])),\n","                # )\n","                loss = rankNet(\n","                    torch.reshape(out, (1, out.shape[0])),\n","                    torch.reshape(target, (1, target.shape[0])),\n","                )\n","                loss.backward()\n","                graph_loss += loss.item()\n","\n","                pred, truth = to_cpu_numpy(params, out, target)\n","                preds.append(pred)\n","                truths.append(truth)\n","                num_batch_count += 1\n","\n","            # 各グラフ毎に勾配降下\n","            nn.utils.clip_grad_norm_(\n","                model.parameters(),\n","                max_norm=params.grad_clip_max_norm,\n","                norm_type=params.grad_clip_norm_type,\n","            )\n","            optimizer.step()\n","            scheduler.step(epoch + i_graph / num_graph)\n","            optimizer.zero_grad()\n","\n","            preds, truths = np.hstack(preds), np.hstack(truths)\n","            score = kendalltau(truths, preds).correlation\n","            graph_loss /= num_batch_count  # 各バッチの平均をグラフの誤差とする\n","            epoch_loss += graph_loss\n","\n","            record = {\n","                \"epoch\": epoch,\n","                \"i_graph\": i_graph,\n","                \"num_train_log\": num_train_log,\n","                f\"train-{graph_arch}-{graph_perm}/epoch_loss\": epoch_loss\n","                / (i_graph + 1),\n","                f\"train/epoch_loss\": epoch_loss / (i_graph + 1),\n","                f\"train-{graph_arch}-{graph_perm}/graph_loss\": graph_loss,\n","                f\"train/graph_loss\": graph_loss,\n","                f\"train-{graph_arch}-{graph_perm}/score\": score,\n","                f\"train/score\": score,\n","                \"lr\": scheduler.get_last_lr()[0],\n","                f\"train-{graph_arch}-{graph_perm}/pred\": preds,\n","                f\"train/pred\": preds,\n","            }\n","            record.update(graph_info)\n","            records.append(record)\n","\n","            wandb.log(record)\n","            num_train_log += 1\n","            pbar.set_description(\n","                f\"running loss: {epoch_loss / (i_graph + 1):.5f}, graph loss: {graph_loss:.5f} score: {score:.3f}\"\n","            )\n","            pbar.update(1)\n","\n","        model.eval()\n","        torch.cuda.empty_cache()\n","\n","        dfscore, eval_preds = evaluate_score(dataset=valid_layout_dataset, model=model)\n","        avg_loss = dfscore[\"graph_loss\"].mean()\n","        avg_score = dfscore[\"score\"].mean()\n","        for i_eval, row_score in dfscore.iterrows():\n","            graph_arch, graph_perm = row_score[\"arch\"], row_score[\"perm\"]\n","            record = {\n","                \"epoch\": epoch,\n","                \"i_graph\": -1,\n","                \"num_valid_log\": num_valid_log,\n","                \"arch\": graph_arch,\n","                \"perm\": graph_perm,\n","                \"filename\": row_score[\"filename\"],\n","                f\"valid-{graph_arch}-{graph_perm}/epoch_loss\": avg_loss,\n","                f\"valid/epoch_loss\": avg_loss,\n","                f\"valid-{graph_arch}-{graph_perm}/graph_loss\": row_score[\"graph_loss\"],\n","                f\"valid/graph_loss\": row_score[\"graph_loss\"],\n","                f\"valid-{graph_arch}-{graph_perm}/score\": row_score[\"score\"],\n","                f\"valid/score\": row_score[\"score\"],\n","                \"lr\": scheduler.get_last_lr()[0],\n","                f\"valid-{graph_arch}-{graph_perm}/pred\": eval_preds[i_eval],\n","                f\"valid/pred\": eval_preds[i_eval],\n","            }\n","            records.append(record)\n","            wandb.log(record)\n","            num_valid_log += 1\n","\n","        print(f\"[valid] current loss: {avg_loss:.5f} score: {avg_score:.3f}\")\n","\n","        if best_score < avg_score:\n","            best_score = avg_score\n","            torch.save(model.state_dict(), savedir / \"best_model.pt\")\n","        torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n","\n","    dflog = pd.DataFrame(records)\n","    dflog.to_csv(savedir / \"log.csv\", index=False)\n","\n","    torch.save(model.state_dict(), savedir / \"final_model.pt\")\n","\n","    del (\n","        train_layout_dataset,\n","        valid_layout_dataset,\n","        model,\n","        optimizer,\n","        dfscore,\n","        dflog,\n","        records,\n","    )\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["### ranknet random\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4d4c14e0b444c09b5ec074384cbbce1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/54 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arch</th>\n","      <th>perm</th>\n","      <th>filename</th>\n","      <th>score</th>\n","      <th>num_config</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>albert_en_xlarge_batch_size_16_test</td>\n","      <td>0.445358</td>\n","      <td>54584</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>bert_en_cased_L-12_H-768_A-12_batch_size_16_test</td>\n","      <td>0.530226</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>bert_multi_cased_L-12_H-768_A-12_batch_size_16...</td>\n","      <td>0.483142</td>\n","      <td>28944</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.575017</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.297282</td>\n","      <td>52056</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.474878</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.413638</td>\n","      <td>43872</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-512_A-8_batc...</td>\n","      <td>0.386469</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.505238</td>\n","      <td>34024</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.498863</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-12_H-768_A-12_bat...</td>\n","      <td>0.493317</td>\n","      <td>25960</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-2_H-256_A-4_batch...</td>\n","      <td>0.463368</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-256_A-4_batch...</td>\n","      <td>0.587524</td>\n","      <td>94600</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-512_A-8_batch...</td>\n","      <td>0.392345</td>\n","      <td>65368</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.440021</td>\n","      <td>82152</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.464051</td>\n","      <td>64968</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-512_A-8_batch...</td>\n","      <td>0.471855</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.518239</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.505281</td>\n","      <td>43368</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>talking-heads_large_batch_size_16_train</td>\n","      <td>0.562921</td>\n","      <td>11168</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>albert_en_xlarge_batch_size_16_test</td>\n","      <td>0.824346</td>\n","      <td>56032</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>bert_en_cased_L-12_H-768_A-12_batch_size_16_test</td>\n","      <td>0.689554</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>bert_multi_cased_L-12_H-768_A-12_batch_size_16...</td>\n","      <td>0.909351</td>\n","      <td>27752</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.818789</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.639387</td>\n","      <td>49528</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.860912</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.750328</td>\n","      <td>42104</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-512_A-8_batc...</td>\n","      <td>0.869385</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.909620</td>\n","      <td>31632</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.805464</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-12_H-768_A-12_bat...</td>\n","      <td>0.885143</td>\n","      <td>24608</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-2_H-256_A-4_batch...</td>\n","      <td>0.834586</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-256_A-4_batch...</td>\n","      <td>0.902449</td>\n","      <td>88000</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-512_A-8_batch...</td>\n","      <td>0.924395</td>\n","      <td>63088</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.893955</td>\n","      <td>76272</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.758090</td>\n","      <td>61744</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-512_A-8_batch...</td>\n","      <td>0.872599</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.664804</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.930420</td>\n","      <td>42784</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>talking-heads_large_batch_size_16_train</td>\n","      <td>0.758095</td>\n","      <td>10750</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>bert_pretraining.4x4.fp16</td>\n","      <td>0.429695</td>\n","      <td>19232</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>inception_v3_batch_128_train</td>\n","      <td>0.508935</td>\n","      <td>5984</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>mlperf_bert_batch_24_2x2</td>\n","      <td>0.096208</td>\n","      <td>6048</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>resnet50.4x4.fp16</td>\n","      <td>0.350970</td>\n","      <td>6120</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>resnet_v1_50_official_batch_128_bf16</td>\n","      <td>0.103483</td>\n","      <td>8512</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>tf2_bert_pretrain_dynamic_batch_size</td>\n","      <td>0.315040</td>\n","      <td>18160</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>unet_3d.4x4.bf16</td>\n","      <td>0.211117</td>\n","      <td>1965</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>bert_pretraining.4x4.fp16</td>\n","      <td>0.835299</td>\n","      <td>19040</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>inception_v3_batch_128_train</td>\n","      <td>0.754436</td>\n","      <td>4688</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>mlperf_bert_batch_24_2x2</td>\n","      <td>0.414462</td>\n","      <td>5288</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>resnet50.4x4.fp16</td>\n","      <td>0.194149</td>\n","      <td>5704</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>resnet_v1_50_official_batch_128_bf16</td>\n","      <td>0.630747</td>\n","      <td>7248</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>tf2_bert_pretrain_dynamic_batch_size</td>\n","      <td>0.708865</td>\n","      <td>17360</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>unet_3d.4x4.bf16</td>\n","      <td>0.487189</td>\n","      <td>481</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   arch     perm                                           filename     score  \\\n","0   nlp  default                albert_en_xlarge_batch_size_16_test  0.445358   \n","1   nlp  default   bert_en_cased_L-12_H-768_A-12_batch_size_16_test  0.530226   \n","2   nlp  default  bert_multi_cased_L-12_H-768_A-12_batch_size_16...  0.483142   \n","3   nlp  default  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.575017   \n","4   nlp  default  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.297282   \n","5   nlp  default  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.474878   \n","6   nlp  default  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.413638   \n","7   nlp  default  small_bert_bert_en_uncased_L-10_H-512_A-8_batc...  0.386469   \n","8   nlp  default  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.505238   \n","9   nlp  default  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.498863   \n","10  nlp  default  small_bert_bert_en_uncased_L-12_H-768_A-12_bat...  0.493317   \n","11  nlp  default  small_bert_bert_en_uncased_L-2_H-256_A-4_batch...  0.463368   \n","12  nlp  default  small_bert_bert_en_uncased_L-4_H-256_A-4_batch...  0.587524   \n","13  nlp  default  small_bert_bert_en_uncased_L-4_H-512_A-8_batch...  0.392345   \n","14  nlp  default  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.440021   \n","15  nlp  default  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.464051   \n","16  nlp  default  small_bert_bert_en_uncased_L-6_H-512_A-8_batch...  0.471855   \n","17  nlp  default  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.518239   \n","18  nlp  default  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.505281   \n","19  nlp  default            talking-heads_large_batch_size_16_train  0.562921   \n","20  nlp   random                albert_en_xlarge_batch_size_16_test  0.824346   \n","21  nlp   random   bert_en_cased_L-12_H-768_A-12_batch_size_16_test  0.689554   \n","22  nlp   random  bert_multi_cased_L-12_H-768_A-12_batch_size_16...  0.909351   \n","23  nlp   random  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.818789   \n","24  nlp   random  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.639387   \n","25  nlp   random  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.860912   \n","26  nlp   random  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.750328   \n","27  nlp   random  small_bert_bert_en_uncased_L-10_H-512_A-8_batc...  0.869385   \n","28  nlp   random  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.909620   \n","29  nlp   random  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.805464   \n","30  nlp   random  small_bert_bert_en_uncased_L-12_H-768_A-12_bat...  0.885143   \n","31  nlp   random  small_bert_bert_en_uncased_L-2_H-256_A-4_batch...  0.834586   \n","32  nlp   random  small_bert_bert_en_uncased_L-4_H-256_A-4_batch...  0.902449   \n","33  nlp   random  small_bert_bert_en_uncased_L-4_H-512_A-8_batch...  0.924395   \n","34  nlp   random  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.893955   \n","35  nlp   random  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.758090   \n","36  nlp   random  small_bert_bert_en_uncased_L-6_H-512_A-8_batch...  0.872599   \n","37  nlp   random  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.664804   \n","38  nlp   random  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.930420   \n","39  nlp   random            talking-heads_large_batch_size_16_train  0.758095   \n","40  xla  default                          bert_pretraining.4x4.fp16  0.429695   \n","41  xla  default                       inception_v3_batch_128_train  0.508935   \n","42  xla  default                           mlperf_bert_batch_24_2x2  0.096208   \n","43  xla  default                                  resnet50.4x4.fp16  0.350970   \n","44  xla  default               resnet_v1_50_official_batch_128_bf16  0.103483   \n","45  xla  default               tf2_bert_pretrain_dynamic_batch_size  0.315040   \n","46  xla  default                                   unet_3d.4x4.bf16  0.211117   \n","47  xla   random                          bert_pretraining.4x4.fp16  0.835299   \n","48  xla   random                       inception_v3_batch_128_train  0.754436   \n","49  xla   random                           mlperf_bert_batch_24_2x2  0.414462   \n","50  xla   random                                  resnet50.4x4.fp16  0.194149   \n","51  xla   random               resnet_v1_50_official_batch_128_bf16  0.630747   \n","52  xla   random               tf2_bert_pretrain_dynamic_batch_size  0.708865   \n","53  xla   random                                   unet_3d.4x4.bf16  0.487189   \n","\n","    num_config  \n","0        54584  \n","1       100040  \n","2        28944  \n","3       100040  \n","4        52056  \n","5       100040  \n","6        43872  \n","7       100040  \n","8        34024  \n","9       100040  \n","10       25960  \n","11      100040  \n","12       94600  \n","13       65368  \n","14       82152  \n","15       64968  \n","16      100040  \n","17      100040  \n","18       43368  \n","19       11168  \n","20       56032  \n","21      100040  \n","22       27752  \n","23      100040  \n","24       49528  \n","25      100040  \n","26       42104  \n","27      100040  \n","28       31632  \n","29      100040  \n","30       24608  \n","31      100040  \n","32       88000  \n","33       63088  \n","34       76272  \n","35       61744  \n","36      100040  \n","37      100040  \n","38       42784  \n","39       10750  \n","40       19232  \n","41        5984  \n","42        6048  \n","43        6120  \n","44        8512  \n","45       18160  \n","46        1965  \n","47       19040  \n","48        4688  \n","49        5288  \n","50        5704  \n","51        7248  \n","52       17360  \n","53         481  "]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["checkpoint_filepath = workdir / \"ranknet-random\" / \"epoch20_model.pt\"\n","\n","dftest = dataset_dict[\"valid\"].reset_index(drop=True)\n","\n","test_layout_dataset = LayoutDataset(\n","    dataset=dftest,\n","    params=params,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model = SimpleLayoutModel(\n","    params=params,\n","    const=const,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model.load_state_dict(torch.load(checkpoint_filepath))\n","model.eval()\n","\n","records = []\n","with tqdm(range(len(test_layout_dataset))) as pbar:\n","    for i in pbar:\n","        file_info = test_layout_dataset.get_ith_file_info(i)\n","\n","        pred_list = []\n","        for (\n","            node_opcode,\n","            node_flag_feat,\n","            node_cont_feat,\n","            node_cat_feat,\n","            node_config_feat,\n","            node_config_cont_feat,\n","            edge_index,\n","            node_splits,\n","            node_config_ids,\n","            target,\n","        ) in test_layout_dataset.getitem_as_batch(i):\n","            pred_batch = model(\n","                node_opcode=node_opcode,\n","                node_flag_feat=node_flag_feat,\n","                node_cont_feat=node_cont_feat,\n","                node_cat_feat=node_cat_feat,\n","                node_config_feat=node_config_feat,\n","                node_config_cont_feat=node_config_cont_feat,\n","                edge_index=edge_index,\n","                node_splits=node_splits,\n","                node_config_ids=node_config_ids,\n","            )\n","            if params.device == GPU:\n","                pred_batch = pred_batch.cpu().detach().numpy()\n","            else:\n","                pred_batch = pred_batch.detach().numpy()\n","            # pred_batchは高いものほどよい\n","            pred_batch = -pred_batch\n","            pred_list.append(pred_batch)\n","\n","        pred = np.hstack(pred_list)\n","\n","        # ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n","        # records.append(\n","        #     {\"ID\": ID, \"TopConfigs\": \";\".join(list(map(str, pred.argsort())))}\n","        # )\n","        records.append(\n","            {\n","                \"arch\": file_info[\"arch\"],\n","                \"perm\": file_info[\"perm\"],\n","                \"filename\": file_info[\"filename\"],\n","                \"pred\": pred.tolist(),\n","                \"target\": test_layout_dataset.create_layout_config(\n","                    idx=i\n","                ).config_runtime.tolist(),\n","            }\n","        )\n","\n","del test_layout_dataset, model\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","with open(workdir / f\"ranknet-random.json\", \"w\") as f:\n","    json.dump(records, f)\n","\n","results = []\n","for record in records:\n","    score = kendalltau(record[\"target\"], record[\"pred\"]).correlation\n","    result = record.copy()\n","    result[\"score\"] = score\n","    result[\"num_config\"] = len(result[\"target\"])\n","    del result[\"target\"], result[\"pred\"]\n","\n","    results.append(result)\n","\n","results = pd.DataFrame(results)\n","results.to_csv(workdir / f\"ranknet-random-score.csv\", index=False)\n","results\n","\n","# dfpred = pd.DataFrame(records)\n","# dfpred[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["### ranknet"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f825997248c4be2b6b6000c1ee0029d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/54 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arch</th>\n","      <th>perm</th>\n","      <th>filename</th>\n","      <th>score</th>\n","      <th>num_config</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>albert_en_xlarge_batch_size_16_test</td>\n","      <td>0.431057</td>\n","      <td>54584</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>bert_en_cased_L-12_H-768_A-12_batch_size_16_test</td>\n","      <td>0.520857</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>bert_multi_cased_L-12_H-768_A-12_batch_size_16...</td>\n","      <td>0.447280</td>\n","      <td>28944</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.569989</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.272996</td>\n","      <td>52056</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.463035</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.360506</td>\n","      <td>43872</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-512_A-8_batc...</td>\n","      <td>0.379060</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.479220</td>\n","      <td>34024</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.465872</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-12_H-768_A-12_bat...</td>\n","      <td>0.460421</td>\n","      <td>25960</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-2_H-256_A-4_batch...</td>\n","      <td>0.417184</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-256_A-4_batch...</td>\n","      <td>0.561033</td>\n","      <td>94600</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-512_A-8_batch...</td>\n","      <td>0.371206</td>\n","      <td>65368</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.421821</td>\n","      <td>82152</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.447383</td>\n","      <td>64968</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-512_A-8_batch...</td>\n","      <td>0.463337</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.485882</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.479156</td>\n","      <td>43368</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>nlp</td>\n","      <td>default</td>\n","      <td>talking-heads_large_batch_size_16_train</td>\n","      <td>0.554205</td>\n","      <td>11168</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>albert_en_xlarge_batch_size_16_test</td>\n","      <td>0.837467</td>\n","      <td>56032</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>bert_en_cased_L-12_H-768_A-12_batch_size_16_test</td>\n","      <td>0.625773</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>bert_multi_cased_L-12_H-768_A-12_batch_size_16...</td>\n","      <td>0.910091</td>\n","      <td>27752</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.806260</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n","      <td>0.602888</td>\n","      <td>49528</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.834998</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n","      <td>0.753907</td>\n","      <td>42104</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-512_A-8_batc...</td>\n","      <td>0.847978</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.910269</td>\n","      <td>31632</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n","      <td>0.748866</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-12_H-768_A-12_bat...</td>\n","      <td>0.879913</td>\n","      <td>24608</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-2_H-256_A-4_batch...</td>\n","      <td>0.755587</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-256_A-4_batch...</td>\n","      <td>0.855487</td>\n","      <td>88000</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-4_H-512_A-8_batch...</td>\n","      <td>0.909080</td>\n","      <td>63088</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.857252</td>\n","      <td>76272</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n","      <td>0.763785</td>\n","      <td>61744</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-512_A-8_batch...</td>\n","      <td>0.831428</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.595559</td>\n","      <td>100040</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n","      <td>0.921863</td>\n","      <td>42784</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>nlp</td>\n","      <td>random</td>\n","      <td>talking-heads_large_batch_size_16_train</td>\n","      <td>0.759973</td>\n","      <td>10750</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>bert_pretraining.4x4.fp16</td>\n","      <td>0.396819</td>\n","      <td>19232</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>inception_v3_batch_128_train</td>\n","      <td>0.274280</td>\n","      <td>5984</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>mlperf_bert_batch_24_2x2</td>\n","      <td>-0.035565</td>\n","      <td>6048</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>resnet50.4x4.fp16</td>\n","      <td>0.276527</td>\n","      <td>6120</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>resnet_v1_50_official_batch_128_bf16</td>\n","      <td>0.030902</td>\n","      <td>8512</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>tf2_bert_pretrain_dynamic_batch_size</td>\n","      <td>0.165689</td>\n","      <td>18160</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>xla</td>\n","      <td>default</td>\n","      <td>unet_3d.4x4.bf16</td>\n","      <td>0.262061</td>\n","      <td>1965</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>bert_pretraining.4x4.fp16</td>\n","      <td>0.826094</td>\n","      <td>19040</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>inception_v3_batch_128_train</td>\n","      <td>0.767852</td>\n","      <td>4688</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>mlperf_bert_batch_24_2x2</td>\n","      <td>0.395789</td>\n","      <td>5288</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>resnet50.4x4.fp16</td>\n","      <td>0.157330</td>\n","      <td>5704</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>resnet_v1_50_official_batch_128_bf16</td>\n","      <td>0.398296</td>\n","      <td>7248</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>tf2_bert_pretrain_dynamic_batch_size</td>\n","      <td>0.650223</td>\n","      <td>17360</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>xla</td>\n","      <td>random</td>\n","      <td>unet_3d.4x4.bf16</td>\n","      <td>0.571685</td>\n","      <td>481</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   arch     perm                                           filename     score  \\\n","0   nlp  default                albert_en_xlarge_batch_size_16_test  0.431057   \n","1   nlp  default   bert_en_cased_L-12_H-768_A-12_batch_size_16_test  0.520857   \n","2   nlp  default  bert_multi_cased_L-12_H-768_A-12_batch_size_16...  0.447280   \n","3   nlp  default  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.569989   \n","4   nlp  default  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.272996   \n","5   nlp  default  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.463035   \n","6   nlp  default  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.360506   \n","7   nlp  default  small_bert_bert_en_uncased_L-10_H-512_A-8_batc...  0.379060   \n","8   nlp  default  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.479220   \n","9   nlp  default  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.465872   \n","10  nlp  default  small_bert_bert_en_uncased_L-12_H-768_A-12_bat...  0.460421   \n","11  nlp  default  small_bert_bert_en_uncased_L-2_H-256_A-4_batch...  0.417184   \n","12  nlp  default  small_bert_bert_en_uncased_L-4_H-256_A-4_batch...  0.561033   \n","13  nlp  default  small_bert_bert_en_uncased_L-4_H-512_A-8_batch...  0.371206   \n","14  nlp  default  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.421821   \n","15  nlp  default  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.447383   \n","16  nlp  default  small_bert_bert_en_uncased_L-6_H-512_A-8_batch...  0.463337   \n","17  nlp  default  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.485882   \n","18  nlp  default  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.479156   \n","19  nlp  default            talking-heads_large_batch_size_16_train  0.554205   \n","20  nlp   random                albert_en_xlarge_batch_size_16_test  0.837467   \n","21  nlp   random   bert_en_cased_L-12_H-768_A-12_batch_size_16_test  0.625773   \n","22  nlp   random  bert_multi_cased_L-12_H-768_A-12_batch_size_16...  0.910091   \n","23  nlp   random  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.806260   \n","24  nlp   random  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  0.602888   \n","25  nlp   random  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.834998   \n","26  nlp   random  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  0.753907   \n","27  nlp   random  small_bert_bert_en_uncased_L-10_H-512_A-8_batc...  0.847978   \n","28  nlp   random  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.910269   \n","29  nlp   random  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  0.748866   \n","30  nlp   random  small_bert_bert_en_uncased_L-12_H-768_A-12_bat...  0.879913   \n","31  nlp   random  small_bert_bert_en_uncased_L-2_H-256_A-4_batch...  0.755587   \n","32  nlp   random  small_bert_bert_en_uncased_L-4_H-256_A-4_batch...  0.855487   \n","33  nlp   random  small_bert_bert_en_uncased_L-4_H-512_A-8_batch...  0.909080   \n","34  nlp   random  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.857252   \n","35  nlp   random  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  0.763785   \n","36  nlp   random  small_bert_bert_en_uncased_L-6_H-512_A-8_batch...  0.831428   \n","37  nlp   random  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.595559   \n","38  nlp   random  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  0.921863   \n","39  nlp   random            talking-heads_large_batch_size_16_train  0.759973   \n","40  xla  default                          bert_pretraining.4x4.fp16  0.396819   \n","41  xla  default                       inception_v3_batch_128_train  0.274280   \n","42  xla  default                           mlperf_bert_batch_24_2x2 -0.035565   \n","43  xla  default                                  resnet50.4x4.fp16  0.276527   \n","44  xla  default               resnet_v1_50_official_batch_128_bf16  0.030902   \n","45  xla  default               tf2_bert_pretrain_dynamic_batch_size  0.165689   \n","46  xla  default                                   unet_3d.4x4.bf16  0.262061   \n","47  xla   random                          bert_pretraining.4x4.fp16  0.826094   \n","48  xla   random                       inception_v3_batch_128_train  0.767852   \n","49  xla   random                           mlperf_bert_batch_24_2x2  0.395789   \n","50  xla   random                                  resnet50.4x4.fp16  0.157330   \n","51  xla   random               resnet_v1_50_official_batch_128_bf16  0.398296   \n","52  xla   random               tf2_bert_pretrain_dynamic_batch_size  0.650223   \n","53  xla   random                                   unet_3d.4x4.bf16  0.571685   \n","\n","    num_config  \n","0        54584  \n","1       100040  \n","2        28944  \n","3       100040  \n","4        52056  \n","5       100040  \n","6        43872  \n","7       100040  \n","8        34024  \n","9       100040  \n","10       25960  \n","11      100040  \n","12       94600  \n","13       65368  \n","14       82152  \n","15       64968  \n","16      100040  \n","17      100040  \n","18       43368  \n","19       11168  \n","20       56032  \n","21      100040  \n","22       27752  \n","23      100040  \n","24       49528  \n","25      100040  \n","26       42104  \n","27      100040  \n","28       31632  \n","29      100040  \n","30       24608  \n","31      100040  \n","32       88000  \n","33       63088  \n","34       76272  \n","35       61744  \n","36      100040  \n","37      100040  \n","38       42784  \n","39       10750  \n","40       19232  \n","41        5984  \n","42        6048  \n","43        6120  \n","44        8512  \n","45       18160  \n","46        1965  \n","47       19040  \n","48        4688  \n","49        5288  \n","50        5704  \n","51        7248  \n","52       17360  \n","53         481  "]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["checkpoint_filepath = workdir / \"ranknet\" / \"epoch20_model.pt\"\n","\n","dftest = dataset_dict[\"valid\"].reset_index(drop=True)\n","\n","test_layout_dataset = LayoutDataset(\n","    dataset=dftest,\n","    params=params,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model = SimpleLayoutModel(\n","    params=params,\n","    const=const,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model.load_state_dict(torch.load(checkpoint_filepath))\n","model.eval()\n","\n","records = []\n","with tqdm(range(len(test_layout_dataset))) as pbar:\n","    for i in pbar:\n","        file_info = test_layout_dataset.get_ith_file_info(i)\n","\n","        pred_list = []\n","        for (\n","            node_opcode,\n","            node_flag_feat,\n","            node_cont_feat,\n","            node_cat_feat,\n","            node_config_feat,\n","            node_config_cont_feat,\n","            edge_index,\n","            node_splits,\n","            node_config_ids,\n","            target,\n","        ) in test_layout_dataset.getitem_as_batch(i):\n","            pred_batch = model(\n","                node_opcode=node_opcode,\n","                node_flag_feat=node_flag_feat,\n","                node_cont_feat=node_cont_feat,\n","                node_cat_feat=node_cat_feat,\n","                node_config_feat=node_config_feat,\n","                node_config_cont_feat=node_config_cont_feat,\n","                edge_index=edge_index,\n","                node_splits=node_splits,\n","                node_config_ids=node_config_ids,\n","            )\n","            if params.device == GPU:\n","                pred_batch = pred_batch.cpu().detach().numpy()\n","            else:\n","                pred_batch = pred_batch.detach().numpy()\n","            # pred_batchは高いものほどよい\n","            pred_batch = -pred_batch\n","            pred_list.append(pred_batch)\n","\n","        pred = np.hstack(pred_list)\n","\n","        # ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n","        # records.append(\n","        #     {\"ID\": ID, \"TopConfigs\": \";\".join(list(map(str, pred.argsort())))}\n","        # )\n","        records.append(\n","            {\n","                \"arch\": file_info[\"arch\"],\n","                \"perm\": file_info[\"perm\"],\n","                \"filename\": file_info[\"filename\"],\n","                \"pred\": pred.tolist(),\n","                \"target\": test_layout_dataset.create_layout_config(\n","                    idx=i\n","                ).config_runtime.tolist(),\n","            }\n","        )\n","\n","del test_layout_dataset, model\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","with open(workdir / f\"ranknet.json\", \"w\") as f:\n","    json.dump(records, f)\n","\n","results = []\n","for record in records:\n","    score = kendalltau(record[\"target\"], record[\"pred\"]).correlation\n","    result = record.copy()\n","    result[\"score\"] = score\n","    result[\"num_config\"] = len(result[\"target\"])\n","    del result[\"target\"], result[\"pred\"]\n","\n","    results.append(result)\n","\n","results = pd.DataFrame(results)\n","results.to_csv(workdir / f\"ranknet-score.csv\", index=False)\n","results\n","\n","# dfpred = pd.DataFrame(records)\n","# dfpred[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## 推論\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["arch  perm   \n","nlp   default    0.452575\n","      random     0.800421\n","xla   default    0.195816\n","      random     0.538181\n","Name: score, dtype: float64"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["results.groupby([\"arch\", \"perm\"])[\"score\"].mean()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c40e41bde57401e86d3a5e8cd741f6f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["checkpoint_filepath = workdir / \"ranknet-random\" / \"epoch20_model.pt\"\n","inferdir = workdir / \"inference\"\n","inferdir.mkdir(exist_ok=True, parents=True)\n","\n","dftest = dataset_dict[\"test\"].copy()\n","\n","test_layout_dataset = LayoutDataset(\n","    dataset=dftest,\n","    params=params,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model = SimpleLayoutModel(\n","    params=params,\n","    const=const,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model.load_state_dict(torch.load(checkpoint_filepath))\n","model.eval()\n","\n","records = []\n","with tqdm(range(len(test_layout_dataset))) as pbar:\n","    for i in pbar:\n","        file_info = test_layout_dataset.get_ith_file_info(i)\n","\n","        pred_list = []\n","        for (\n","            node_opcode,\n","            node_flag_feat,\n","            node_cont_feat,\n","            node_cat_feat,\n","            node_config_feat,\n","            node_config_cont_feat,\n","            edge_index,\n","            node_splits,\n","            node_config_ids,\n","            target,\n","        ) in test_layout_dataset.getitem_as_batch(i):\n","            pred_batch = model(\n","                node_opcode=node_opcode,\n","                node_flag_feat=node_flag_feat,\n","                node_cont_feat=node_cont_feat,\n","                node_cat_feat=node_cat_feat,\n","                node_config_feat=node_config_feat,\n","                node_config_cont_feat=node_config_cont_feat,\n","                edge_index=edge_index,\n","                node_splits=node_splits,\n","                node_config_ids=node_config_ids,\n","            )\n","            if params.device == GPU:\n","                pred_batch = pred_batch.cpu().detach().numpy()\n","            else:\n","                pred_batch = pred_batch.detach().numpy()\n","            # pred_batchは高いものほどよい\n","            pred_batch = -pred_batch\n","            pred_list.append(pred_batch)\n","\n","        pred = np.hstack(pred_list)\n","\n","        # ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n","        # records.append(\n","        #     {\"ID\": ID, \"TopConfigs\": \";\".join(list(map(str, pred.argsort())))}\n","        # )\n","        records.append(\n","            {\n","                \"arch\": file_info[\"arch\"],\n","                \"perm\": file_info[\"perm\"],\n","                \"filename\": file_info[\"filename\"],\n","                \"pred\": pred.tolist(),\n","                \"target\": test_layout_dataset.create_layout_config(\n","                    idx=i\n","                ).config_runtime.tolist(),\n","            }\n","        )\n","\n","del test_layout_dataset, model\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","with open(inferdir / f\"ranknet-random.json\", \"w\") as f:\n","    json.dump(records, f)\n","\n","# dfpred = pd.DataFrame(records)\n","# dfpred[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53c9fb04fcf74c34b7de9954b2e43ecd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["checkpoint_filepath = workdir / \"ranknet\" / \"epoch20_model.pt\"\n","inferdir = workdir / \"inference\"\n","inferdir.mkdir(exist_ok=True, parents=True)\n","\n","dftest = dataset_dict[\"test\"].copy()\n","\n","test_layout_dataset = LayoutDataset(\n","    dataset=dftest,\n","    params=params,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model = SimpleLayoutModel(\n","    params=params,\n","    const=const,\n","    cat_status=cat_status,\n","    cat_config_status=cat_config_status,\n",")\n","model.load_state_dict(torch.load(checkpoint_filepath))\n","model.eval()\n","\n","records = []\n","with tqdm(range(len(test_layout_dataset))) as pbar:\n","    for i in pbar:\n","        file_info = test_layout_dataset.get_ith_file_info(i)\n","\n","        pred_list = []\n","        for (\n","            node_opcode,\n","            node_flag_feat,\n","            node_cont_feat,\n","            node_cat_feat,\n","            node_config_feat,\n","            node_config_cont_feat,\n","            edge_index,\n","            node_splits,\n","            node_config_ids,\n","            target,\n","        ) in test_layout_dataset.getitem_as_batch(i):\n","            pred_batch = model(\n","                node_opcode=node_opcode,\n","                node_flag_feat=node_flag_feat,\n","                node_cont_feat=node_cont_feat,\n","                node_cat_feat=node_cat_feat,\n","                node_config_feat=node_config_feat,\n","                node_config_cont_feat=node_config_cont_feat,\n","                edge_index=edge_index,\n","                node_splits=node_splits,\n","                node_config_ids=node_config_ids,\n","            )\n","            if params.device == GPU:\n","                pred_batch = pred_batch.cpu().detach().numpy()\n","            else:\n","                pred_batch = pred_batch.detach().numpy()\n","            # pred_batchは高いものほどよい\n","            pred_batch = -pred_batch\n","            pred_list.append(pred_batch)\n","\n","        pred = np.hstack(pred_list)\n","\n","        # ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n","        # records.append(\n","        #     {\"ID\": ID, \"TopConfigs\": \";\".join(list(map(str, pred.argsort())))}\n","        # )\n","        records.append(\n","            {\n","                \"arch\": file_info[\"arch\"],\n","                \"perm\": file_info[\"perm\"],\n","                \"filename\": file_info[\"filename\"],\n","                \"pred\": pred.tolist(),\n","                \"target\": test_layout_dataset.create_layout_config(\n","                    idx=i\n","                ).config_runtime.tolist(),\n","            }\n","        )\n","\n","del test_layout_dataset, model\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","with open(inferdir / f\"ranknet.json\", \"w\") as f:\n","    json.dump(records, f, indent=4)\n","\n","# dfpred = pd.DataFrame(records)\n","# dfpred[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"environment":{"kernel":"python3","name":"pytorch-gpu.2-0.m112","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
