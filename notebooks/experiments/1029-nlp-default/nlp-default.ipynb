{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "sns.set()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n",
    "node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fast-layout-7-85\"\n",
    "trans_node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout6-92-dataset\"\n",
    "trans_node_config_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout7-81-dataset\"\n",
    "workdir = Path().resolve() / \"out\"\n",
    "workdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "ignores = []\n",
    "for ds in [\"train\", \"valid\", \"test\"]:\n",
    "    records = []\n",
    "    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n",
    "        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n",
    "        for filepath in sorted(datadir.glob(\"*.npz\")):\n",
    "            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "\n",
    "            if (ds != \"test\") and ((\"mlperf\" in filename) or (\"openai\" in filename)):\n",
    "                ignores.append(filepath)\n",
    "                continue\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arch\": arch,\n",
    "                    \"perm\": perm,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": filepath,\n",
    "                    \"node_feat_filepath\": str(\n",
    "                        node_feat_dir / arch / perm / ds / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_feat_filepath\": str(\n",
    "                        trans_node_feat_dir\n",
    "                        / \"layout\"\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_config_filepath\": str(\n",
    "                        trans_node_config_feat_dir\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "    dataset_dict[ds] = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for filepath in tqdm(ignores):\n",
    "#     node_config_feat = np.load(filepath)[\"node_config_feat\"]\n",
    "\n",
    "#     for i in range(1, node_config_feat.shape[0]):\n",
    "#         if not (node_config_feat[0] == node_config_feat[i]).all():\n",
    "#             filepath\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "      <th>cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats  \\\n",
       "0       0         1        19   \n",
       "1       1        68         6   \n",
       "\n",
       "                                                cats  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "1                                 [0, 1, 2, 3, 4, 5]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat = pd.DataFrame(\n",
    "    [\n",
    "        {\"number\": 0, \"num_dims\": 1, \"num_cats\": 19, \"cats\": list(range(19))},\n",
    "        {\"number\": 1, \"num_dims\": 54 + 14, \"num_cats\": 6, \"cats\": list(range(6))},\n",
    "    ]\n",
    ")\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats\n",
       "0       0        18         8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat_config = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"number\": 0,\n",
    "            \"num_dims\": 18,\n",
    "            \"num_cats\": 8,\n",
    "        },  # output_layout, input_layout, kernel_layout\n",
    "    ]\n",
    ")\n",
    "dfcat_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset_dict:\n",
    "    for i, row in dataset_dict[ds].iterrows():\n",
    "        np.load(row[\"filepath\"])\n",
    "        np.load(row[\"node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_config_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データクラスを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CatStatus:\n",
    "    dfcat: pd.DataFrame\n",
    "    prefix: str\n",
    "    num_cat_dict: dict[str, int] = field(init=False)\n",
    "    index_dict: dict[str, list[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.num_cat_dict, self.index_dict = {}, {}\n",
    "        dim_start = 0\n",
    "        for i, row in self.dfcat.iterrows():\n",
    "            self.num_cat_dict[f\"{self.prefix}cat_feat{i + 1}\"] = row[\"num_cats\"]\n",
    "            self.index_dict[f\"{self.prefix}cat_feat{i + 1}\"] = list(\n",
    "                range(dim_start, dim_start + row[\"num_dims\"])\n",
    "            )\n",
    "            dim_start += row[\"num_dims\"]\n",
    "\n",
    "\n",
    "cat_status = CatStatus(dfcat=dfcat, prefix=\"\")\n",
    "cat_config_status = CatStatus(dfcat=dfcat_config, prefix=\"config_\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Const:\n",
    "    num_node_flag_feat_dim: int\n",
    "    num_node_cont_feat_dim: int\n",
    "    num_node_cat_feat_dim: int\n",
    "    num_node_config_cont_feat_dim: int\n",
    "\n",
    "    # 演算子の種類\n",
    "    num_operations: int = 120\n",
    "    # 各configの次元数\n",
    "    num_config_dims: int = 6\n",
    "\n",
    "\n",
    "fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"node_feat_filepath\"])\n",
    "trans_fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"trans_node_feat_filepath\"])\n",
    "trans_config_fileobj = np.load(\n",
    "    dataset_dict[\"train\"].iloc[0][\"trans_node_config_filepath\"]\n",
    ")\n",
    "\n",
    "node_flag_feat, node_cont_feat = fileobj[\"node_flag_feat\"], fileobj[\"node_cont_feat\"]\n",
    "node_enum_feat, node_dimension_number_feat = (\n",
    "    fileobj[\"node_enum_feat\"],\n",
    "    fileobj[\"node_dimension_number_feat\"],\n",
    ")\n",
    "trans_node_cont_feat, trans_node_cat_feat = (\n",
    "    trans_fileobj[\"node_cont_feat\"],\n",
    "    trans_fileobj[\"node_cat_feat\"],\n",
    ")\n",
    "trans_node_config_cont_feat = trans_config_fileobj[\"node_config_cont_feat\"]\n",
    "const = Const(\n",
    "    num_node_flag_feat_dim=node_flag_feat.shape[1] + 1,  # config_idsの分+1\n",
    "    num_node_cont_feat_dim=node_cont_feat.shape[1] + trans_node_cont_feat.shape[1],\n",
    "    num_node_cat_feat_dim=node_enum_feat.shape[1]\n",
    "    + node_dimension_number_feat.shape[1]\n",
    "    + trans_node_cat_feat.shape[1],\n",
    "    num_node_config_cont_feat_dim=trans_node_config_cont_feat.shape[2],\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NodeFeatExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope: float = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GNNExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CatEmbedding:\n",
    "    num_cat: int\n",
    "    embedding_dim: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    device: str\n",
    "    cat_embeddings: dict[str, CatEmbedding]\n",
    "    # num_random_samples: int = 30\n",
    "    # num_sampling: int = 33  # 1グラフ毎に1000件（テストと同じ）\n",
    "    # train_batch_size: int = 8\n",
    "    # batch_size: int = 30\n",
    "    num_random_samples: int = 15\n",
    "    num_sampling: int = 10  # 1グラフ毎に1000件（テストと同じ）\n",
    "    train_batch_size: int = 1\n",
    "    batch_size: int = 30\n",
    "    node_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    node_config_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    gnn_extractor: GNNExtractor = field(default_factory=lambda: GNNExtractor())\n",
    "    subgraph_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    epoch: int = 50\n",
    "    T_max: int = 50\n",
    "    eta_min: float = 1e-5\n",
    "    lr: float = 1e-2\n",
    "    weight_decay: float = 0\n",
    "\n",
    "\n",
    "cat_embeddings = {}\n",
    "cat_embeddings.update(\n",
    "    {\"op\": CatEmbedding(num_cat=const.num_operations, embedding_dim=16)}\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_config_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "params = Params(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    cat_embeddings=cat_embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LayoutConfigs:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    node_cont_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 108)\n",
    "\n",
    "    node_cat_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 3)\n",
    "\n",
    "    node_opcode: np.ndarray\n",
    "        ノード演算子、(ノード数,)\n",
    "    edge_index: np.ndarray\n",
    "        エッジ、(エッジ数, 2)\n",
    "\n",
    "    node_config_feat: np.ndarray\n",
    "        設定毎のノード特徴量、(設定数, 設定可能なノード数, 3)\n",
    "\n",
    "    node_config_ids: np.ndarray\n",
    "        設定可能なノードのIndex、(設定可能なノード数,)\n",
    "    config_runtime: np.ndarray\n",
    "        実行時間、(設定数,)\n",
    "    node_splits: np.ndarray\n",
    "        同じパーティションでの計算を意味する。今回は使用しない。(パーティション数, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    node_flag_feat: np.ndarray\n",
    "    node_cont_feat: np.ndarray\n",
    "    node_cat_feat: np.ndarray\n",
    "    node_opcode: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    node_config_feat: np.ndarray\n",
    "    node_config_cont_feat: np.ndarray\n",
    "    node_config_ids: np.ndarray\n",
    "    config_runtime: np.ndarray\n",
    "    node_splits: np.ndarray\n",
    "\n",
    "    cat_status: CatStatus\n",
    "    cat_config_status: CatStatus\n",
    "    target: np.ndarray = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # 設定が存在するノードのフラグ\n",
    "        node_active_feat = np.zeros((self.num_nodes, 1))\n",
    "        node_active_feat[self.node_config_ids, :] = 1\n",
    "        self.node_flag_feat = np.concatenate(\n",
    "            [self.node_flag_feat, node_active_feat], axis=1\n",
    "        )\n",
    "        self.node_cont_feat = self.apply_normalization(x=self.node_cont_feat)\n",
    "        self.node_config_feat = self.node_config_feat + 1  # カテゴリは0~7にする\n",
    "        self.node_splits = np.array(\n",
    "            [\n",
    "                [self.node_splits[0][i], self.node_splits[0][i + 1] - 1]\n",
    "                for i in range(self.node_splits.shape[1] - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.target = self.apply_target_normalization(x=self.config_runtime)\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        \"\"\"ノード数\"\"\"\n",
    "        return self.node_cont_feat.shape[0]\n",
    "\n",
    "    def get_random_config_idx(self, num: int) -> list[int]:\n",
    "        \"\"\"ランダムな設定をサンプルする\"\"\"\n",
    "        num_ = min(self.config_runtime.shape[0], num)\n",
    "        return random.sample(list(range(self.config_runtime.shape[0])), num_)\n",
    "\n",
    "    def get_filled_node_config_feat(\n",
    "        self, index_list: list[int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"指定された設定の設定毎のノード特徴量を取得する。設定がない場合は補完する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray [(len(index_list),ノード数, 18), (len(index_list),ノード数, 連続次元数)]\n",
    "        \"\"\"\n",
    "        # (サンプル数, ノード数) x 3\n",
    "        node_config_feat = np.full(\n",
    "            (len(index_list), self.num_nodes, Const.num_config_dims * 3),\n",
    "            Const.num_config_dims + 1,\n",
    "        )\n",
    "        node_config_feat[:, self.node_config_ids] = self.node_config_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "\n",
    "        node_config_cont_feat = np.zeros(\n",
    "            (len(index_list), self.num_nodes, self.node_config_cont_feat.shape[2])\n",
    "        )\n",
    "        node_config_cont_feat[:, self.node_config_ids] = self.node_config_cont_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "        return node_config_feat, node_config_cont_feat\n",
    "\n",
    "    def get_target(self, index_list: list[int]) -> np.ndarray:\n",
    "        \"\"\"指定された設定の目的変数を取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "        \"\"\"\n",
    "        return self.apply_target_ranking(x=self.config_runtime[index_list])\n",
    "\n",
    "    def apply_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"特徴量の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            2次元行列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            行方向に正規化された行列\n",
    "        \"\"\"\n",
    "        x /= 128\n",
    "        x = np.where(x >= 0, np.log1p(x / 128), -np.log1p(-x / 128))\n",
    "        return x\n",
    "\n",
    "    def apply_target_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"目的変数の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            ベクトル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            正規化されたベクトル\n",
    "        \"\"\"\n",
    "        return np.log(x / x.min())\n",
    "\n",
    "    def apply_target_ranking(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"降順でランキング\"\"\"\n",
    "        return np.argsort(np.argsort(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayoutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    rows: list[dict[str, np.ndarray]]\n",
    "        設定をリストでもつ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: pd.DataFrame,\n",
    "        params: Params,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        self.rows = dataset.to_dict(\"records\")\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "        self.cache_idx = None\n",
    "        self.cache_filepath = None\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return self.params.device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def create_layout_config(self, idx: int) -> LayoutConfigs:\n",
    "        if self.cache_idx != idx:\n",
    "            self.cache_idx = idx\n",
    "            fileobj = np.load(self.rows[self.cache_idx][\"filepath\"])\n",
    "            node_feat_fileobj = np.load(self.rows[self.cache_idx][\"node_feat_filepath\"])\n",
    "            trans_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_feat_filepath\"]\n",
    "            )\n",
    "            trans_config_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_config_filepath\"]\n",
    "            )\n",
    "\n",
    "            node_cont_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_cont_feat\"],\n",
    "                    trans_feat_fileobj[\"node_cont_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            node_cat_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_enum_feat\"],\n",
    "                    node_feat_fileobj[\"node_dimension_number_feat\"],\n",
    "                    trans_feat_fileobj[\"node_cat_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            self.cache_layout_config = LayoutConfigs(\n",
    "                node_opcode=fileobj[\"node_opcode\"],\n",
    "                edge_index=fileobj[\"edge_index\"],\n",
    "                node_config_ids=fileobj[\"node_config_ids\"],\n",
    "                config_runtime=fileobj[\"config_runtime\"],\n",
    "                node_splits=fileobj[\"node_splits\"],\n",
    "                node_flag_feat=node_feat_fileobj[\"node_flag_feat\"],\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=fileobj[\"node_config_feat\"],\n",
    "                node_config_cont_feat=trans_config_feat_fileobj[\n",
    "                    \"node_config_cont_feat\"\n",
    "                ],\n",
    "                cat_status=self.cat_status,\n",
    "                cat_config_status=self.cat_config_status,\n",
    "            )\n",
    "        return self.cache_layout_config\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"ランダムな設定を取得する\"\"\"\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "        config_index_list = layout_configs.get_random_config_idx(\n",
    "            num=self.params.num_random_samples\n",
    "        )\n",
    "        return self._get_tensors(\n",
    "            layout_configs=layout_configs, index_list=config_index_list\n",
    "        )\n",
    "\n",
    "    def getitem_as_batch(\n",
    "        self, idx: int\n",
    "    ) -> list[\n",
    "        tuple[\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "        ]\n",
    "    ]:\n",
    "        \"\"\"設定をバッチで取得する\"\"\"\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "\n",
    "        index_list = list(range(layout_configs.config_runtime.shape[0]))\n",
    "        for i_chunk in range(0, len(index_list), self.params.batch_size):\n",
    "            chunk_index_list = index_list[i_chunk : i_chunk + self.params.batch_size]\n",
    "            yield self._get_tensors(\n",
    "                layout_configs=layout_configs, index_list=chunk_index_list\n",
    "            )\n",
    "\n",
    "    def _get_tensors(\n",
    "        self, layout_configs: LayoutConfigs, index_list: list[int]\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"渡された設定のIndexのテンソルを取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layout_configs: LayoutConfigs\n",
    "            Layoutのデータクラス\n",
    "        index_list: list[int]\n",
    "            設定のインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            ノード特徴量(フラグ)\n",
    "        torch.Tensor\n",
    "            ノード特徴量(連続)\n",
    "        dict[str, torch.Tensor]\n",
    "            ノード特徴量(カテゴリ)\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量(連続)\n",
    "        torch.Tensor\n",
    "            ノード演算子\n",
    "        torch.Tensor\n",
    "            エッジ\n",
    "        torch.Tensor\n",
    "            目的変数\n",
    "        \"\"\"\n",
    "        # ノード特徴量(フラグ)\n",
    "        node_flag_feat = torch.tensor(\n",
    "            layout_configs.node_flag_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(連続)\n",
    "        node_cont_feat = torch.tensor(\n",
    "            layout_configs.node_cont_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(カテゴリ)\n",
    "        node_cat_feat = torch.tensor(\n",
    "            layout_configs.node_cat_feat,\n",
    "            dtype=torch.int64,\n",
    "        ).to(self.device)\n",
    "        # 設定毎のノード特徴量(カテゴリ)\n",
    "        (\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "        ) = layout_configs.get_filled_node_config_feat(index_list=index_list)\n",
    "        node_config_feat = torch.tensor(node_config_feat, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        node_config_cont_feat = torch.tensor(\n",
    "            node_config_cont_feat, dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        # ノード演算子\n",
    "        node_opcode = torch.tensor(layout_configs.node_opcode, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # エッジ\n",
    "        edge_index = torch.tensor(\n",
    "            np.swapaxes(layout_configs.edge_index, 0, 1), dtype=torch.int64\n",
    "        ).to(self.device)\n",
    "        # サブグラフ\n",
    "        node_splits = torch.tensor(layout_configs.node_splits, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # ターゲット\n",
    "        target = torch.tensor(\n",
    "            layout_configs.get_target(index_list=index_list),\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        )\n",
    "\n",
    "    def get_ith_file_info(self, i: int) -> dict[str, str]:\n",
    "        row = self.rows[i]\n",
    "        return {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "        }\n",
    "\n",
    "    def get_ith_runtime(self, i: int) -> np.ndarray:\n",
    "        layout_configs = self.create_layout_config(idx=i)\n",
    "        return layout_configs.config_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "\n",
    "class EdgeConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    ノード特徴 + 隣接ノード特徴 + 隣接ノード特徴の一致\n",
    "    参考： https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html#implementing-the-edge-convolution\n",
    "    補足: 集約関数はデフォルトでdim(axis) = -2。つまりノード方向で集約するので気にしなくてOK\n",
    "    https://github.com/pyg-team/pytorch_geometric/blob/1e12d41c28b1fb9793f17646b018071b508864d7/torch_geometric/nn/aggr/basic.py#L38\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_input_dim: int, x_output_dim: int, dropout_p: float):\n",
    "        # \"Add\" aggregation\n",
    "        super().__init__(aggr=\"max\")\n",
    "        self.mlp = nn.Sequential(\n",
    "            # nn.LayerNorm(x_input_dim * 2),\n",
    "            nn.Linear(x_input_dim * 2, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.ReLU(),\n",
    "            # nn.LayerNorm(x_output_dim),\n",
    "            nn.Linear(x_output_dim, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [設定数, N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"propagate()で渡された引数xから自動でx_i, x_jノードを取り出して随時処理を実装する関数\"\"\"\n",
    "        # x_i has shape [設定数, エッジ数, in_channels]\n",
    "        # x_j has shape [設定数, エッジ数, in_channels]\n",
    "        x_cat = torch.cat(\n",
    "            [x_i, x_i - x_j], dim=2\n",
    "        )  # tmp has shape [設定数, エッジ数, 2 * in_channels]\n",
    "        return self.mlp(x_cat)\n",
    "\n",
    "\n",
    "class SimpleLayoutModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params: Params\n",
    "        実験設定のデータクラス\n",
    "    node_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(ノード毎)\n",
    "    node_config_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(設定xノード毎)\n",
    "    node_feat_extractor: torch.nn.Module\n",
    "        ノードの特徴量を抽出するネットワーク\n",
    "    gnn_extractor: torch.nn.Module\n",
    "        グラフの特徴量を抽出するネットワーク\n",
    "    gc: torch.nn.Module\n",
    "        最終層の全結合層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Params,\n",
    "        const: Const,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "\n",
    "        # カテゴリ変数の埋め込み表現\n",
    "        self.embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                k: torch.nn.Embedding(v.num_cat, v.embedding_dim)\n",
    "                for k, v in self.params.cat_embeddings.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # node_featのfeature_extractorを定義\n",
    "        num_node_feat_extractor_input_dim = (\n",
    "            const.num_node_flag_feat_dim\n",
    "            + const.num_node_cont_feat_dim\n",
    "            + self.num_node_feat_embedding_dims\n",
    "        )\n",
    "\n",
    "        node_feat_extractor_layer = []\n",
    "        node_feat_extractor_dims = [\n",
    "            num_node_feat_extractor_input_dim\n",
    "        ] + self.params.node_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_feat_extractor_dims[i],\n",
    "                    out_features=node_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(params.node_feat_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "            self.node_feat_extractor = nn.Sequential(*node_feat_extractor_layer)\n",
    "\n",
    "        # node_config_featのfeature_extractorを定義\n",
    "        num_node_config_feat_extractor_input_dim = (\n",
    "            self.num_node_config_feat_embedding_dims\n",
    "            + const.num_node_config_cont_feat_dim\n",
    "        )\n",
    "\n",
    "        node_config_feat_extractor_layer = []\n",
    "        node_config_feat_extractor_dims = [\n",
    "            num_node_config_feat_extractor_input_dim\n",
    "        ] + self.params.node_config_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_config_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_config_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_config_feat_extractor_dims[i],\n",
    "                    out_features=node_config_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_config_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(\n",
    "                    params.node_config_feat_extractor.leakyrelu_negative_slope\n",
    "                ),\n",
    "            ]\n",
    "        self.node_config_feat_extractor = nn.Sequential(\n",
    "            *node_config_feat_extractor_layer\n",
    "        )\n",
    "\n",
    "        # ノード間のfeature_extractorの定義\n",
    "        num_gnn_extractor_input_dim = (\n",
    "            node_feat_extractor_dims[-1] + node_config_feat_extractor_dims[-1]\n",
    "        )\n",
    "\n",
    "        gnn_extractor_layer = []\n",
    "        gnn_extractor_dims = [\n",
    "            num_gnn_extractor_input_dim\n",
    "        ] + self.params.gnn_extractor.dims\n",
    "        for i in range(len(gnn_extractor_dims) - 1):\n",
    "            gnn_extractor_layer += [\n",
    "                (\n",
    "                    EdgeConv(\n",
    "                        x_input_dim=gnn_extractor_dims[i],\n",
    "                        x_output_dim=gnn_extractor_dims[i + 1],\n",
    "                        dropout_p=params.gnn_extractor.dropout_p,\n",
    "                    ),\n",
    "                    \"x, edge_index -> x\",\n",
    "                ),\n",
    "                nn.LeakyReLU(params.gnn_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "        self.gnn_extractor = Sequential(\"x, edge_index\", gnn_extractor_layer)\n",
    "\n",
    "        # サブグラフのfeature_extractorの定義\n",
    "        # num_subgraph_extractor_input_dim = (\n",
    "        #     self.params.gnn_extractor.dims[-1] + num_gnn_extractor_input_dim\n",
    "        # )\n",
    "\n",
    "        # subgraph_extractor_layer = []\n",
    "        # subgraph_extractor_dims = [\n",
    "        #     num_subgraph_extractor_input_dim\n",
    "        # ] + self.params.node_feat_extractor.dims\n",
    "        # for i in range(len(subgraph_extractor_dims) - 1):\n",
    "        #     subgraph_extractor_layer += [\n",
    "        #         # nn.LayerNorm(subgraph_extractor_dims[i]),\n",
    "        #         nn.Linear(\n",
    "        #             in_features=subgraph_extractor_dims[i],\n",
    "        #             out_features=subgraph_extractor_dims[i + 1],\n",
    "        #         ),\n",
    "        #         # nn.Dropout(params.subgraph_extractor.dropout_p),\n",
    "        #         nn.LeakyReLU(params.subgraph_extractor.leakyrelu_negative_slope),\n",
    "        #     ]\n",
    "        # self.subgraph_extractor = nn.Sequential(*subgraph_extractor_layer)\n",
    "\n",
    "        fc_layer = [\n",
    "            # nn.LayerNorm(subgraph_extractor_dims[-1]),\n",
    "            # nn.Linear(in_features=subgraph_extractor_dims[-1], out_features=1),\n",
    "            nn.Linear(\n",
    "                in_features=self.params.gnn_extractor.dims[-1]\n",
    "                + num_gnn_extractor_input_dim,\n",
    "                out_features=1,\n",
    "            ),\n",
    "        ]\n",
    "        self.fc = nn.Sequential(*fc_layer)\n",
    "        self.to(self.params.device)\n",
    "\n",
    "    @property\n",
    "    def num_node_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        num_embedding_dims += 1 * self.params.cat_embeddings[\"op\"].embedding_dim\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    @property\n",
    "    def num_node_config_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "        node_config_feat: torch.Tensor,\n",
    "        node_config_cont_feat: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        node_splits: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------\n",
    "        node_flag_feat:\n",
    "            ノードの特徴量(node数, フラグ次元数)\n",
    "        node_cont_feat:\n",
    "            ノードの特徴量(node数, 連続次元数)\n",
    "        node_cat_feat:\n",
    "            ノードの特徴量(node数, カテゴリ次元数*埋め込み次元数)\n",
    "        node_config_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 特徴次元数)\n",
    "        node_config_cont_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 連続次元数)\n",
    "        edge_index:\n",
    "            エッジ(2, エッジ数)\n",
    "        node_splits:\n",
    "            サブグラフのインデックス（サブグラフ数, 2)\n",
    "\n",
    "        Returns:\n",
    "        torch.tensor: (設定数)\n",
    "        \"\"\"\n",
    "        # (ノード数,特徴数)のテンソルを作成\n",
    "        node_feat = self._join_node_feature(\n",
    "            node_opcode=node_opcode,\n",
    "            node_flag_feat=node_flag_feat,\n",
    "            node_cont_feat=node_cont_feat,\n",
    "            node_cat_feat=node_cat_feat,\n",
    "        )\n",
    "\n",
    "        # (設定数,ノード数,特徴数)のテンソルを作成\n",
    "        node_config_feat = self._join_node_config_feature(\n",
    "            node_config_feat=node_config_feat,\n",
    "            node_config_cont_feat=node_config_cont_feat,\n",
    "        )\n",
    "\n",
    "        # node_featの抽出器を通す\n",
    "        extracted_node_feat = self.node_feat_extractor(node_feat)\n",
    "\n",
    "        # node_config_featの抽出器を通す\n",
    "        extracted_node_config_feat = self.node_config_feat_extractor(node_config_feat)\n",
    "\n",
    "        # 設定毎のノード特徴に結合する\n",
    "        extracted_feat = self._join_entire_node_config_feat(\n",
    "            node_feat=extracted_node_feat,\n",
    "            node_config_feat=extracted_node_config_feat,\n",
    "        )\n",
    "\n",
    "        # GNN抽出器を通す\n",
    "        conved_extracted_feat = self.gnn_extractor(\n",
    "            x=extracted_feat,\n",
    "            edge_index=edge_index,\n",
    "        )\n",
    "\n",
    "        # 残差を足すイメージ\n",
    "        concat_feat = torch.concat([extracted_feat, conved_extracted_feat], 2)\n",
    "\n",
    "        # subgraph_global_pool_feat_list = []\n",
    "        # for subgraph_start_node_idx, subgraph_end_node_idx in node_splits:\n",
    "        #     subgraph_concat_feat = concat_feat[\n",
    "        #         :, subgraph_start_node_idx : subgraph_end_node_idx + 1, :\n",
    "        #     ]\n",
    "        #     # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        #     subgraph_global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "        #     subgraph_global_pool_feat_list.append(\n",
    "        #         torch.reshape(\n",
    "        #             subgraph_global_pool_feat,\n",
    "        #             (\n",
    "        #                 subgraph_global_pool_feat.shape[0],\n",
    "        #                 1,\n",
    "        #                 subgraph_global_pool_feat.shape[1],\n",
    "        #             ),\n",
    "        #         )\n",
    "        #     )\n",
    "        # # （設定数,サブグラフ数,特徴数)\n",
    "        # subgraph_global_pool_feat = torch.concat(subgraph_global_pool_feat_list, 1)\n",
    "        # subgraph_extracted_feat = self.subgraph_extractor(subgraph_global_pool_feat)\n",
    "\n",
    "        # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        # global_pool_feat = torch.mean(subgraph_extracted_feat, dim=1)\n",
    "        global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "\n",
    "        return torch.squeeze(self.fc(global_pool_feat))\n",
    "\n",
    "    def _join_node_feature(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_featのテンソルを作成\"\"\"\n",
    "        # ノードの埋め込み表現\n",
    "        node_embeddings_list = []\n",
    "        node_embeddings_list.append(self.embeddings[\"op\"](node_opcode))\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](node_cat_feat[:, cat_index])\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (-1, node_embeddings.shape[-2] * node_embeddings.shape[-1]),\n",
    "            )\n",
    "            node_embeddings_list.append(node_embeddings)\n",
    "\n",
    "        # ノード毎で埋め込み、結合(ノード数, 特徴数)\n",
    "        node_embedding_feat = torch.concat(node_embeddings_list, 1)\n",
    "        node_feat = torch.concat(\n",
    "            [node_flag_feat, node_cont_feat, node_embedding_feat], 1\n",
    "        )\n",
    "        return node_feat\n",
    "\n",
    "    def _join_node_config_feature(\n",
    "        self, node_config_feat: torch.Tensor, node_config_cont_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_config_featのテンソルを作成\"\"\"\n",
    "        # 設定xノード毎で埋め込み(設定数, ノード数, 特徴数)\n",
    "        node_config_embeddings_list = []\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](\n",
    "                node_config_feat[:, :, cat_index]\n",
    "            )\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (\n",
    "                    node_embeddings.shape[0],\n",
    "                    -1,\n",
    "                    node_embeddings.shape[-2] * node_embeddings.shape[-1],\n",
    "                ),\n",
    "            )\n",
    "            node_config_embeddings_list.append(node_embeddings)\n",
    "        node_config_feat = torch.concat(\n",
    "            node_config_embeddings_list + [node_config_cont_feat], 2\n",
    "        )\n",
    "        return node_config_feat\n",
    "\n",
    "    def _join_entire_node_config_feat(\n",
    "        self, node_feat: torch.Tensor, node_config_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # ノード毎の特徴量を設定数だけ縦に並べる\n",
    "        node_tiled_feat = torch.tile(\n",
    "            torch.reshape(node_feat, (1, node_feat.shape[0], node_feat.shape[1])),\n",
    "            (node_config_feat.shape[0], 1, 1),\n",
    "        )\n",
    "        return torch.concat([node_tiled_feat, node_config_feat], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ListMLE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.Tensor\n",
    "            予測（要素数, ）\n",
    "        labels: torch.Tensor\n",
    "            目的変数（要素数, ）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        # 正解をソート\n",
    "        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n",
    "        # 予測を正解順でソート\n",
    "        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n",
    "        # 予測値の最大値で予測値を引く（expの爆発予防）\n",
    "        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n",
    "        logits_sorted_by_true = logits_sorted_by_true - logits_max\n",
    "        # ランキングが低いものから累積する(その後正解順に戻す)\n",
    "        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n",
    "            dims=[1]\n",
    "        )\n",
    "        # 誤差\n",
    "        negative_log_likelihood = torch.sum(\n",
    "            torch.log(cumsums) - logits_sorted_by_true, dim=1\n",
    "        )\n",
    "        return torch.mean(negative_log_likelihood)\n",
    "\n",
    "\n",
    "def rankNet(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    # here we generate every pair of indices from the range of document length in the batch\n",
    "    document_pairs_candidates = list(\n",
    "        itertools.product(range(y_true.shape[1]), repeat=2)\n",
    "    )\n",
    "\n",
    "    pairs_true = y_true[:, document_pairs_candidates]\n",
    "    selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "    # here we calculate the relative true relevance of every candidate pair\n",
    "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "    # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
    "    # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
    "    # positive ones for a simpler loss function formulation\n",
    "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "\n",
    "    pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "    weight = None\n",
    "    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
    "    # whether one document is better than the other and not about the actual difference in\n",
    "    # their relevancy levels\n",
    "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "    true_diffs = true_diffs[the_mask]\n",
    "\n",
    "    return nn.BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)\n",
    "\n",
    "\n",
    "def to_cpu_numpy(\n",
    "    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if params.device == \"cuda\":\n",
    "        pred_ = pred.cpu().detach().numpy()\n",
    "        truth_ = truth.cpu().detach().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pred_ = pred.detach().numpy()\n",
    "        truth_ = truth.detach().numpy()\n",
    "    return pred_, truth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "def evaluate_score(dataset: LayoutDataset, model: torch.nn.Module) -> pd.DataFrame:\n",
    "    \"\"\"データセット全件に対してコンペの評価指標を算出する\n",
    "    https://www.kaggle.com/competitions/predict-ai-model-runtime/overview\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    records = []\n",
    "    # 各グラフ毎にスコアを算出\n",
    "    for i in range(len(dataset)):\n",
    "        # グラフ毎に複数サンプル\n",
    "        preds, truths = [], []\n",
    "        current_loss = 0\n",
    "        for _ in range(params.num_sampling):\n",
    "            (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            ) = dataset[i]\n",
    "            pred = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            # loss = criterion(\n",
    "            #     torch.reshape(pred, (1, pred.shape[0])),\n",
    "            #     torch.reshape(target, (1, target.shape[0])),\n",
    "            # )\n",
    "            loss = rankNet(\n",
    "                torch.reshape(pred, (1, pred.shape[0])),\n",
    "                torch.reshape(target, (1, target.shape[0])),\n",
    "            )\n",
    "            current_loss += loss.item()\n",
    "            pred, truth = to_cpu_numpy(params, pred, target)\n",
    "            preds.append(pred)\n",
    "            truths.append(truth)\n",
    "        preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "        current_loss /= params.num_sampling\n",
    "\n",
    "        score = kendalltau(truth, pred).correlation\n",
    "        record = dataset.get_ith_file_info(i)\n",
    "        record.update(\n",
    "            {\n",
    "                \"current_loss\": current_loss,\n",
    "                \"score\": score,\n",
    "            }\n",
    "        )\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_indexes_as_batch(num_data: int, batch_size: int) -> list[list[int]]:\n",
    "    random_indexes = random.sample(list(range(num_data)), num_data)\n",
    "    batches = []\n",
    "    batch_random_indexes = []\n",
    "    for i, index in enumerate(random_indexes):\n",
    "        batch_random_indexes.append(index)\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            batches.append(batch_random_indexes)\n",
    "            batch_random_indexes = []\n",
    "    if len(batch_random_indexes) > 0:\n",
    "        batches.append(\n",
    "            batch_random_indexes\n",
    "            + random_indexes[: (batch_size - len(batch_random_indexes))]\n",
    "        )\n",
    "    return batches\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    dftrain: pd.DataFrame,\n",
    "    dfvalid: pd.DataFrame,\n",
    "    params: Params,\n",
    "    const: Const,\n",
    "    cat_status: CatStatus,\n",
    "    cat_config_status: CatStatus,\n",
    "    savedir: Path,\n",
    "    checkpoint_dir: Path = None,\n",
    ") -> None:\n",
    "    train_layout_dataset = LayoutDataset(\n",
    "        dataset=dftrain,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    valid_layout_dataset = LayoutDataset(\n",
    "        dataset=dfvalid,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "\n",
    "    model = SimpleLayoutModel(\n",
    "        params=params,\n",
    "        const=const,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    if checkpoint_dir is not None:\n",
    "        print(\"学習済みモデルを読み込みます\")\n",
    "        model.load_state_dict(torch.load(checkpoint_dir / f\"final_model.pt\"))\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n",
    "    )\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    best_score = -np.inf\n",
    "    records = []\n",
    "    for epoch in range(params.epoch):\n",
    "        model.train()\n",
    "\n",
    "        num_data = len(train_layout_dataset)\n",
    "        pbar = tqdm(range(num_data))\n",
    "        batch_indexes_list = get_random_indexes_as_batch(\n",
    "            num_data=num_data, batch_size=params.train_batch_size\n",
    "        )\n",
    "        num_batch = len(batch_indexes_list)\n",
    "        num_total_data = sum(\n",
    "            [len(batch_indexes) for batch_indexes in batch_indexes_list]\n",
    "        )\n",
    "        i_total_data = 1\n",
    "\n",
    "        running_losses = []\n",
    "        for i_batch, batch_indexes in enumerate(batch_indexes_list):\n",
    "            # 各グラフのループ\n",
    "            for i_data in batch_indexes:\n",
    "                # グラフの中で複数サンプルする\n",
    "                preds, truths = [], []\n",
    "                current_loss = 0\n",
    "                for _ in range(params.num_sampling):\n",
    "                    (\n",
    "                        node_opcode,\n",
    "                        node_flag_feat,\n",
    "                        node_cont_feat,\n",
    "                        node_cat_feat,\n",
    "                        node_config_feat,\n",
    "                        node_config_cont_feat,\n",
    "                        edge_index,\n",
    "                        node_splits,\n",
    "                        target,\n",
    "                    ) = train_layout_dataset[i_data]\n",
    "                    out = model(\n",
    "                        node_opcode=node_opcode,\n",
    "                        node_flag_feat=node_flag_feat,\n",
    "                        node_cont_feat=node_cont_feat,\n",
    "                        node_cat_feat=node_cat_feat,\n",
    "                        node_config_feat=node_config_feat,\n",
    "                        node_config_cont_feat=node_config_cont_feat,\n",
    "                        edge_index=edge_index,\n",
    "                        node_splits=node_splits,\n",
    "                    )\n",
    "\n",
    "                    # loss = criterion(\n",
    "                    #     torch.reshape(out, (1, out.shape[0])),\n",
    "                    #     torch.reshape(target, (1, target.shape[0])),\n",
    "                    # )\n",
    "                    loss = rankNet(\n",
    "                        torch.reshape(out, (1, out.shape[0])),\n",
    "                        torch.reshape(target, (1, target.shape[0])),\n",
    "                    )\n",
    "                    current_loss += loss.item()\n",
    "                    # loss /= (params.num_sampling * params.train_batch_size)\n",
    "                    loss.backward()\n",
    "\n",
    "                    pred, truth = to_cpu_numpy(params, out, target)\n",
    "                    preds.append(pred)\n",
    "                    truths.append(truth)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step(epoch + i_total_data / num_total_data)\n",
    "                optimizer.zero_grad()\n",
    "                i_total_data += 1\n",
    "\n",
    "                preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "                score = kendalltau(truth, pred).correlation\n",
    "                current_loss /= params.num_sampling\n",
    "                running_losses.append(current_loss)\n",
    "                running_loss = np.mean(running_losses)\n",
    "\n",
    "                record = {\"epoch\": epoch, \"batch\": i_batch}\n",
    "                record.update(train_layout_dataset.get_ith_file_info(i_data))\n",
    "                record.update(\n",
    "                    {\n",
    "                        \"train/running_loss\": running_loss,\n",
    "                        \"train/current_loss\": current_loss,\n",
    "                        \"train/score\": score,\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    }\n",
    "                )\n",
    "                records.append(record)\n",
    "                wandb.log(record)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f\"running loss: {running_loss:.5f}, current loss: {current_loss:.5f} score: {score:.3f}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "            # optimizer.step()\n",
    "            # scheduler.step(epoch + i_batch / num_batch)\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        dfscore = evaluate_score(dataset=valid_layout_dataset, model=model)\n",
    "        avg_loss = dfscore[\"current_loss\"].mean()\n",
    "        avg_score = dfscore[\"score\"].mean()\n",
    "        for _, row_score in dfscore.iterrows():\n",
    "            record = {\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": -1,\n",
    "                \"arch\": row_score[\"arch\"],\n",
    "                \"perm\": row_score[\"perm\"],\n",
    "                \"filename\": row_score[\"filename\"],\n",
    "                \"valid/running_loss\": avg_loss,\n",
    "                \"valid/current_loss\": row_score[\"current_loss\"],\n",
    "                \"valid/score\": row_score[\"score\"],\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            records.append(record)\n",
    "            wandb.log(record)\n",
    "\n",
    "        print(f\"[valid] current loss: {avg_loss:.5f} score: {avg_score:.3f}\")\n",
    "\n",
    "        if best_score < avg_score:\n",
    "            best_score = avg_score\n",
    "            torch.save(model.state_dict(), savedir / \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n",
    "\n",
    "    dflog = pd.DataFrame(records)\n",
    "    dflog.to_csv(savedir / \"log.csv\", index=False)\n",
    "\n",
    "    torch.save(model.state_dict(), savedir / \"final_model.pt\")\n",
    "\n",
    "    del (\n",
    "        train_layout_dataset,\n",
    "        valid_layout_dataset,\n",
    "        model,\n",
    "        optimizer,\n",
    "        dfscore,\n",
    "        dflog,\n",
    "        records,\n",
    "    )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzuuuubo-tetsu\u001b[0m (\u001b[33msun-scan-clan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64090597d035447cb04685df6c0746cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113370621266465, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yamaguchi/kaggle/experiments/1029-nlp-default/wandb/run-20231029_012539-tbqn4szn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/tbqn4szn' target=\"_blank\">1029-nlp-default</a></strong> to <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/tbqn4szn' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/tbqn4szn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/tbqn4szn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f16c45595d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch, perm = \"nlp\", \"default\"\n",
    "exptname = str(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"params\": asdict(params),\n",
    "        \"const\": asdict(const),\n",
    "        \"arch\": arch,\n",
    "        \"perm\": perm,\n",
    "        \"validation\": \"hold-out\",\n",
    "    },\n",
    "    name=exptname,\n",
    "    tags=[arch, perm],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fd4fa261004d85840b4161ebf9b6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.67128 score: 0.106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d55e5203e5340de9173b98f94620bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.65355 score: 0.204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8746f3d5dec04d23a2a941196af018a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 1.05902 score: 0.111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485fafd0324843bb92b9e7b02b154967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.69071 score: 0.067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd18530933d483a9e1509d4114b7412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.65336 score: 0.169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a28497b7e1e44be8742e3b48455d053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.64853 score: 0.191\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042ab4296cf1487ab53c80590752be96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.64047 score: 0.196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b0e6a889414bb986489c7bf6ac3c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.62097 score: 0.189\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2425684221db47b195d72f0d1f08da59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.62803 score: 0.206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d521aab2bfcd4bec9c3badf4764cff00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.64568 score: 0.218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa9cc608174f50a9f099ae4f1e8483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.63939 score: 0.180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ee4685c8c74694b659cecac53017bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.59718 score: 0.289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b9b025cc484f709a0eb211e19a814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.59803 score: 0.155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12342df94ad488ea5ebf29cb849c167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.59482 score: 0.242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446cafdb6e3a48a2a7f8155b5b7133c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.59487 score: 0.285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc777765f3d402889e19ceac276305a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.57894 score: 0.310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf8d54bff6c456f82ae22505c1b6aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.60002 score: 0.285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c00529a9c546dd869f5ea8bfc67514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.60245 score: 0.252\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f689b64f04492dba9c24e38e97a77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.57521 score: 0.353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1191c8ca7f41a2b9944dc7bcfc4e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.57677 score: 0.288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99293a47a53a40e9a1d934d95dfde110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.59795 score: 0.364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b6a5dc79d94a8a9fa94b5a18841122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.56041 score: 0.355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4792b4133a324c0c80ded1e59543b0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.57781 score: 0.314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ecd6af9d4347ff90ab692b578c1ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.57124 score: 0.367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c759e629f844154b5646369338cfe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.57408 score: 0.398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7868e931531440c683218c1e0030d365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.56140 score: 0.336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d045110d551543eba421fc1ba2a58f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.53878 score: 0.468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d03ed5589ef4e6b8cd8b540ab2de2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.53969 score: 0.298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a8db7ad27b4dd3a75d4db1447baec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.53547 score: 0.300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab90c95a4c894771b1332ddf1bd93776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.52417 score: 0.457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e056bd76231c4665b6e96a31c9aa249a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.54089 score: 0.342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda38ff8d347425c91528c87421f2819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.53988 score: 0.385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed538a010f74348b0e53288bae0a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.53205 score: 0.305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89760eabb834bd19382b9abb16a1aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.50764 score: 0.450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6972bfe71478d91f9b7865fde51d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.51454 score: 0.381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c296d1635a34abd9bad87b3cfd2093c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.50734 score: 0.373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f860b7d41732487bb2b0af829bea842c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.50596 score: 0.352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2a82a9b6404890b468222f215aa619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.50434 score: 0.460\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ce942d26734907afe6b8c4d704e196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.48212 score: 0.437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f5e8bd842d4c418ba193da8564984d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.49427 score: 0.410\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b10b09918474e78967f26cf5c128b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.49534 score: 0.462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7b636181e3478a898cf16ac1cd652b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.47396 score: 0.456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457120bc33754eb1bd9b69d22fbd302b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.48745 score: 0.431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa706af186ba4ec49026c4e81d9953f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.49503 score: 0.423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa5a895e9a64b838af29664f546f176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.48399 score: 0.431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c575f630db346839b0d967af72987f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.49255 score: 0.316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eceb66f094f2424ca14574b636eb2efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.48529 score: 0.452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b369782b53470eb5f0ae5f88b6770c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.48422 score: 0.427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45cedc3ce284f6da0343aadb5827a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.49028 score: 0.449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b78804ac7445f19d1cc412798bbd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] current loss: 0.47825 score: 0.413\n"
     ]
    }
   ],
   "source": [
    "dftrain = dataset_dict[\"train\"].query(f\"(arch == @arch) & (perm == @perm)\")\n",
    "dfvalid = dataset_dict[\"valid\"].query(f\"(arch == @arch) & (perm == @perm)\")\n",
    "params_opt = deepcopy(params)\n",
    "\n",
    "savedir = workdir / f\"{arch}-{perm}\"\n",
    "savedir.mkdir(exist_ok=True, parents=True)\n",
    "train_model(\n",
    "    dftrain=dftrain,\n",
    "    dfvalid=dfvalid,\n",
    "    params=params_opt,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    "    savedir=savedir,\n",
    "    checkpoint_dir=None,\n",
    ")\n",
    "wandb.alert(title=exptname, text=f\"Train End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedir = workdir / f\"{arch}-{perm}\"\n",
    "# dflog = pd.read_csv(savedir / \"log.csv\")\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "# for i, ds in enumerate([\"train\", \"valid\"]):\n",
    "#     dflog_ = dflog.query(\"(phase == @ds)\").groupby(\"epoch\")\n",
    "#     axes[i][0].plot(dflog_[\"current_loss\"].mean(), label=\"total\")\n",
    "#     axes[i][1].plot(dflog_[\"score\"].mean(), label=\"taotal\")\n",
    "#     if i == 0:\n",
    "#         axes[i][0].legend()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddda617d846472abcbeba9c789049ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "savedir = workdir / f\"{arch}-{perm}\"\n",
    "\n",
    "records = []\n",
    "\n",
    "dftest = dataset_dict[\"test\"].query(f\"(arch == @arch) & (perm == @perm)\")\n",
    "params_opt = deepcopy(params)\n",
    "if arch == \"nlp\":\n",
    "    params_opt.num_random_samples = 30\n",
    "    params_opt.batch_size = 30\n",
    "\n",
    "test_layout_dataset = LayoutDataset(\n",
    "    dataset=dftest,\n",
    "    params=params_opt,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model = SimpleLayoutModel(\n",
    "    params=params_opt,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model.load_state_dict(torch.load(savedir / f\"final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "with tqdm(range(len(test_layout_dataset))) as pbar:\n",
    "    for i in pbar:\n",
    "        file_info = test_layout_dataset.get_ith_file_info(i)\n",
    "\n",
    "        pred_list = []\n",
    "        for (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        ) in test_layout_dataset.getitem_as_batch(i):\n",
    "            pred_batch = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            if params_opt.device == \"cuda\":\n",
    "                pred_batch = pred_batch.cpu().detach().numpy()\n",
    "            else:\n",
    "                pred_batch = pred_batch.detach().numpy()\n",
    "            # pred_batchは高いものほどよい\n",
    "            pred_batch = -pred_batch\n",
    "            pred_list.append(pred_batch)\n",
    "\n",
    "            del (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            )\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        pred = np.hstack(pred_list)\n",
    "\n",
    "        ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n",
    "        records.append({\"ID\": ID, \"pred\": \";\".join(list(map(str, pred.argsort())))})\n",
    "\n",
    "del test_layout_dataset, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dfpred = pd.DataFrame(records)\n",
    "dfsub = pd.read_csv(inputdir / \"sample_submission.csv\")\n",
    "dfsub = dfsub.merge(dfpred, on=\"ID\", how=\"left\")\n",
    "dfsub[\"TopConfigs\"] = np.where(\n",
    "    dfsub[\"pred\"].isnull(), dfsub[\"TopConfigs\"], dfsub[\"pred\"]\n",
    ")\n",
    "dfsub[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission_final_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▆▇▁▂▃▅▆▄▅▇▁▂▃▁▃▄▅▇█▂▇▁▂▄▅▆▅▆▇▁▂▄▅▃▄▆▇▁▂█</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/current_loss</td><td>▃▃▂▃▃▂▃▃▂█▂▃▃▂▂▂▂▂▃▃▂▂▂▂▃▂▂▁▂▂▂▂▁▁▁▁▂▂▁▁</td></tr><tr><td>train/running_loss</td><td>▄▃▄█▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/score</td><td>▅▅▄▅▄▇▅▆▆▂▆▅▄▅▆▅▅▅▆▁▅▄▆▆▄▆▆▆▁▇▆▄█▇▄▇▆▇▆▆</td></tr><tr><td>valid/current_loss</td><td>▄▄▅▄▄▄▄▃▄▄▄▄▄▄▄▂▃▄▂▃▃▂▄▃▃▃▁▂▃▁▁▂▂▆▃▂█▁▂▁</td></tr><tr><td>valid/running_loss</td><td>▃▃█▄▃▃▃▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/score</td><td>▁▆▃▂▃▃▄▄▅▅▂▅▅▇▄▆▆▃█▄▄▇▃▆▆▇▆▄▆▇█▅▁▃▂█▄▃▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>arch</td><td>nlp</td></tr><tr><td>batch</td><td>-1</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>filename</td><td>talking-heads_large_...</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>perm</td><td>default</td></tr><tr><td>train/current_loss</td><td>0.38598</td></tr><tr><td>train/running_loss</td><td>0.49135</td></tr><tr><td>train/score</td><td>0.34786</td></tr><tr><td>valid/current_loss</td><td>0.64592</td></tr><tr><td>valid/running_loss</td><td>0.47825</td></tr><tr><td>valid/score</td><td>0.16508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1029-nlp-default</strong> at: <a href='https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/tbqn4szn' target=\"_blank\">https://wandb.ai/sun-scan-clan/predict-ai-model-runtime-for-sun-scan-clan/runs/tbqn4szn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231029_012539-tbqn4szn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.alert(title=exptname, text=f\"Inference End\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
