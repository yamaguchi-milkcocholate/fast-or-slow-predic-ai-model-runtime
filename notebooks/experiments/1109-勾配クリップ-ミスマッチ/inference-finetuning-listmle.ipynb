{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "from typing import Any\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "\n",
    "\n",
    "GPU = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n",
    "embeddir = Path().resolve() / \"out\" / \"ranknet\" / \"embeddings\"\n",
    "workdir = Path().resolve() / \"out\" / \"finetuning-listmle\"\n",
    "workdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "\n",
    "for ds in [\"train\", \"valid\", \"test\"]:\n",
    "    records = []\n",
    "    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n",
    "        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n",
    "        for filepath in sorted(datadir.glob(\"*.npz\")):\n",
    "            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arch\": arch,\n",
    "                    \"perm\": perm,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": filepath,\n",
    "                    \"embed_filepath\": embeddir / arch / perm / ds / f\"{filename}.npz\",\n",
    "                }\n",
    "            )\n",
    "    dataset_dict[ds] = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset_dict:\n",
    "    indexes = []\n",
    "    for i, row in dataset_dict[ds].iterrows():\n",
    "        try:\n",
    "            np.load(row[\"filepath\"])\n",
    "            np.load(row[\"embed_filepath\"])\n",
    "            indexes.append(i)\n",
    "        except FileNotFoundError as e:\n",
    "            print(row[\"embed_filepath\"])\n",
    "\n",
    "    dataset_dict[ds] = dataset_dict[ds].iloc[indexes].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group17 (18, 5) (18, 3)\n",
      "group16 (30, 5) (30, 3)\n",
      "group15 (50, 5) (50, 3)\n",
      "group10 (22, 5) (22, 3)\n",
      "group9 (19, 5) (19, 3)\n",
      "group8 (21, 5) (21, 3)\n",
      "group7 (10, 5) (10, 3)\n",
      "group6 (10, 5) (10, 3)\n",
      "group5 (53, 5) (53, 3)\n",
      "group4 (20, 5) (20, 3)\n",
      "group3 (20, 5) (20, 3)\n",
      "group2 (18, 5) (18, 3)\n",
      "group1 (64, 5) (64, 3)\n",
      "group0 (17, 5) (17, 3)\n"
     ]
    }
   ],
   "source": [
    "sheet_dict = pd.read_excel(\n",
    "    \"/home/yamaguchi/kaggle/data/clustering-1112.xlsx\", sheet_name=None\n",
    ")\n",
    "train_valid_dataset = pd.concat(\n",
    "    [dataset_dict[\"train\"], dataset_dict[\"valid\"]], axis=0, ignore_index=True\n",
    ")\n",
    "group_dict = {}\n",
    "for group_name in sheet_dict:\n",
    "    if not group_name.startswith(\"group\"):\n",
    "        continue\n",
    "    dfgroup = sheet_dict[group_name].query(\"use == 1\")[[\"arch\", \"perm\", \"filename\"]]\n",
    "    dataset = dfgroup.merge(train_valid_dataset, on=[\"arch\", \"perm\", \"filename\"])\n",
    "    sheet_dict[group_name]\n",
    "    print(group_name, dataset.shape, dfgroup.shape)\n",
    "    if dataset.shape[0] > 0:\n",
    "        group_dict[group_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_tensor(dataset, filename):\n",
    "    dataset_as_dict = {}\n",
    "    embeddings_list = []\n",
    "    for i, row in dataset.iterrows():\n",
    "        fileobj = np.load(row[\"filepath\"])\n",
    "        embed_fileobj = np.load(row[\"embed_filepath\"])\n",
    "        config_runtime = fileobj[\"config_runtime\"]\n",
    "        target = np.argsort(np.argsort(-config_runtime))\n",
    "        embeddings = embed_fileobj[\"embeddings\"]\n",
    "\n",
    "        dataset_as_dict[i] = {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"target\": target,\n",
    "            \"X\": embeddings,\n",
    "        }\n",
    "        embeddings_list.append(embeddings)\n",
    "    embeddings_list = np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "    emb_scl = embeddings_list.max(axis=0) - embeddings_list.min(axis=0)\n",
    "    emb_mean = embeddings_list.mean(axis=0)\n",
    "    del embeddings_list\n",
    "\n",
    "    for i in dataset_as_dict:\n",
    "        dataset_as_dict[i][\"X\"] = (dataset_as_dict[i][\"X\"] - emb_mean) / emb_scl\n",
    "\n",
    "    with open(workdir / f\"{filename}.json\", \"w\") as f:\n",
    "        json.dump({\"xmean\": emb_mean.tolist(), \"xscl\": emb_scl.tolist()}, f, indent=4)\n",
    "    return dataset_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Params:\n",
    "    device: str\n",
    "    dims: list[int] = field(default_factory=lambda: [512, 512, 512])\n",
    "    epoch: int = 500\n",
    "    T_max: int = 500\n",
    "    eta_min: float = 1e-6\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0\n",
    "    grad_clip_max_norm: float = 1.0\n",
    "    grad_clip_norm_type: float = 2.0\n",
    "\n",
    "    sample_size: int = 1000\n",
    "    batch_size: int = 8\n",
    "\n",
    "    num_feats: int = 192\n",
    "\n",
    "\n",
    "params = Params(device=GPU if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_as_dict: dict[str, Any],\n",
    "        params: Params,\n",
    "    ) -> None:\n",
    "        self.dataset_as_dict = dataset_as_dict\n",
    "        self.params = params\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return self.params.device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset_as_dict)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dataset = self.dataset_as_dict[idx]\n",
    "        num_configs = dataset[\"target\"].shape[0]\n",
    "        indexes = random.choices(list(range(num_configs)), k=self.params.sample_size)\n",
    "\n",
    "        embeddings = torch.tensor(\n",
    "            dataset[\"X\"][indexes, :],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        target = torch.tensor(\n",
    "            dataset[\"target\"][indexes],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return embeddings, target\n",
    "\n",
    "    def get_entire(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dataset = self.dataset_as_dict[idx]\n",
    "        embeddings = torch.tensor(\n",
    "            dataset[\"X\"],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        target = torch.tensor(\n",
    "            dataset[\"target\"],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return embeddings, target\n",
    "\n",
    "    def get_info(self, idx):\n",
    "        dataset = self.dataset_as_dict[idx]\n",
    "        return dataset[\"arch\"], dataset[\"perm\"], dataset[\"filename\"]\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Params,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "\n",
    "        dims = [params.num_feats] + self.params.dims\n",
    "        fc_layer = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            fc_layer += [\n",
    "                nn.Linear(\n",
    "                    in_features=dims[i],\n",
    "                    out_features=dims[i + 1],\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        fc_layer += [\n",
    "            nn.Linear(\n",
    "                in_features=dims[-1],\n",
    "                out_features=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.net = nn.Sequential(*fc_layer)\n",
    "        self.to(self.params.device)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.net(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankNet(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    document_pairs_candidates = list(itertools.combinations(range(y_true.shape[1]), 2))\n",
    "\n",
    "    pairs_true = y_true[:, document_pairs_candidates]\n",
    "    selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "    pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "    true_diffs = true_diffs[the_mask]\n",
    "\n",
    "    return nn.BCEWithLogitsLoss()(pred_diffs, true_diffs)\n",
    "\n",
    "\n",
    "class ListMLE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.Tensor\n",
    "            予測（バッチサイズ, 要素数, ）\n",
    "        labels: torch.Tensor\n",
    "            目的変数（バッチサイズ, 要素数, ）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        # 正解をソート\n",
    "        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n",
    "        # 予測を正解順でソート\n",
    "        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n",
    "        # 予測値の最大値で予測値を引く（expの爆発予防）\n",
    "        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n",
    "        logits_sorted_by_true = logits_sorted_by_true - logits_max\n",
    "        # ランキングが低いものから累積する(その後正解順に戻す)\n",
    "        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n",
    "            dims=[1]\n",
    "        )\n",
    "        # 誤差\n",
    "        negative_log_likelihood = torch.sum(\n",
    "            torch.log(cumsums) - logits_sorted_by_true, dim=1\n",
    "        )\n",
    "        return torch.mean(negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def to_cpu_numpy(\n",
    "    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if params.device == GPU:\n",
    "        pred_ = pred.cpu().detach().numpy()\n",
    "        truth_ = truth.cpu().detach().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pred_ = pred.detach().numpy()\n",
    "        truth_ = truth.detach().numpy()\n",
    "    return pred_, truth_\n",
    "\n",
    "\n",
    "def train(params, dataset_as_dict, savedir):\n",
    "    train_dataset = FineTuningDataset(dataset_as_dict=dataset_as_dict, params=params)\n",
    "    valid_dataset = train_dataset\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=params.batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    model = MLP(params=params)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n",
    "    )\n",
    "    criterion = ListMLE()\n",
    "\n",
    "    pbar = tqdm(range(params.epoch))\n",
    "    num_train_log, num_valid_log = 0, 0\n",
    "    for epoch in range(params.epoch):\n",
    "        model.train()\n",
    "\n",
    "        num_iters = len(train_dataloader)\n",
    "        for i_iter, (X, target) in enumerate(train_dataloader):\n",
    "            pred = model(X)\n",
    "\n",
    "            if (len(pred.shape) == 1) or (len(target.shape) == 1):\n",
    "                pred, target = pred.reshape(1, -1), target.reshape(1, -1)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                max_norm=params.grad_clip_max_norm,\n",
    "                norm_type=params.grad_clip_norm_type,\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + i_iter / num_iters)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred, target = to_cpu_numpy(params, pred, target)\n",
    "\n",
    "            scores = []\n",
    "            for pred_, target_ in zip(pred, target):\n",
    "                score = kendalltau(target_, pred_).correlation\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"iter\": i_iter,\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                        \"train/score\": score,\n",
    "                        \"train/loss\": loss,\n",
    "                        \"train/pred\": pred,\n",
    "                        \"train/target\": target,\n",
    "                        \"train/count\": num_train_log,\n",
    "                    }\n",
    "                )\n",
    "                num_train_log += 1\n",
    "                scores.append(score)\n",
    "            scores = np.array(scores)\n",
    "            # pbar.set_description(\n",
    "            #     f\"[{epoch + 1}] score={np.mean(scores[~np.isnan(scores)]):.3f} loss={loss.item():5f}\"\n",
    "            # )\n",
    "\n",
    "        model.eval()\n",
    "        losses, scores = [], []\n",
    "        for i_graph in range(len(valid_dataset)):\n",
    "            arch, perm, filename = valid_dataset.get_info(i_graph)\n",
    "            X, target = valid_dataset[i_graph]\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred.reshape(1, -1), target.reshape(1, -1))\n",
    "            graph_loss = loss.item()\n",
    "            pred, target = to_cpu_numpy(params, pred, target)\n",
    "            score = kendalltau(target, pred).correlation\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iter\": i_iter,\n",
    "                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                    \"valid/score\": score,\n",
    "                    \"valid/loss\": graph_loss,\n",
    "                    \"valid/pred\": pred,\n",
    "                    \"valid/target\": target,\n",
    "                    \"valid/count\": num_valid_log,\n",
    "                }\n",
    "            )\n",
    "            num_valid_log += 1\n",
    "            losses.append(graph_loss)\n",
    "            scores.append(score)\n",
    "        losses = np.array(losses)\n",
    "        scores = np.array(scores)\n",
    "        # print(\n",
    "        #     f\"[{epoch + 1}] valid score={np.mean(scores[~np.isnan(scores)]):.3f} valid loss={np.mean(losses[~np.isnan(losses)]):5f}\"\n",
    "        # )\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\n",
    "            f\"[{epoch + 1}] valid score={np.mean(scores[~np.isnan(scores)]):.3f} valid loss={np.mean(losses[~np.isnan(losses)]):5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 43\n",
    "# for group_name in group_dict:\n",
    "#     dataset = group_dict[group_name]\n",
    "#     dataset_as_dict = create_dataset_tensor(dataset=dataset, filename=group_name)\n",
    "\n",
    "#     exptname = f\"1108-勾配クリップ-ミスマッチ-listmle-finetuning-{group_name}-seed{seed}\"\n",
    "\n",
    "#     wandb.init(\n",
    "#         project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "#         config={\n",
    "#             \"params\": asdict(params),\n",
    "#         },\n",
    "#         name=exptname,\n",
    "#     )\n",
    "\n",
    "#     seed_everything(seed)\n",
    "\n",
    "#     savedir = workdir / f\"{group_name}\" / f\"seed{seed}\"\n",
    "#     savedir.mkdir(exist_ok=True, parents=True)\n",
    "#     train(params=params, dataset_as_dict=dataset_as_dict, savedir=savedir)\n",
    "\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset_tensor(dataset, filename):\n",
    "    dataset_as_dict = {}\n",
    "    embeddings_list = []\n",
    "    for i, row in dataset.iterrows():\n",
    "        fileobj = np.load(row[\"filepath\"])\n",
    "        embed_fileobj = np.load(row[\"embed_filepath\"])\n",
    "        config_runtime = fileobj[\"config_runtime\"]\n",
    "        target = np.argsort(np.argsort(-config_runtime))\n",
    "        embeddings = embed_fileobj[\"embeddings\"]\n",
    "\n",
    "        dataset_as_dict[i] = {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"target\": target,\n",
    "            \"X\": embeddings,\n",
    "        }\n",
    "        embeddings_list.append(embeddings)\n",
    "    embeddings_list = np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "    with open(workdir / f\"{filename}.json\", \"r\") as f:\n",
    "        scler = json.load(f)\n",
    "        emb_scl, emb_mean = np.array(scler[\"xscl\"]), np.array(scler[\"xmean\"])\n",
    "\n",
    "    for i in dataset_as_dict:\n",
    "        dataset_as_dict[i][\"X\"] = (dataset_as_dict[i][\"X\"] - emb_mean) / emb_scl\n",
    "\n",
    "    return dataset_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sheet = sheet_dict[\"test\"][[\"arch\", \"perm\", \"filename\", \"group\"]]\n",
    "seed = 43\n",
    "model_name = \"epoch500_model\"\n",
    "inferdir = workdir / f\"seed{seed}\" / model_name\n",
    "inferdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "records = []\n",
    "pred_records = []\n",
    "for group_name in group_dict:\n",
    "    i_group = int(group_name.replace(\"group\", \"\"))\n",
    "    dfgroup = test_sheet.query(\"group == @i_group\").reset_index(drop=True)\n",
    "    dataset = dfgroup.merge(dataset_dict[\"test\"], on=[\"arch\", \"perm\", \"filename\"])\n",
    "\n",
    "    assert dfgroup.shape[0] == dataset.shape[0]\n",
    "\n",
    "    dataset_as_dict = create_test_dataset_tensor(dataset, group_name)\n",
    "\n",
    "    test_dataset = FineTuningDataset(dataset_as_dict=dataset_as_dict, params=params)\n",
    "\n",
    "    model = MLP(params=params)\n",
    "    model.load_state_dict(\n",
    "        torch.load(workdir / group_name / f\"seed{seed}\" / f\"{model_name}.pt\")\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    for i_graph in range(len(test_dataset)):\n",
    "        arch, perm, filename = test_dataset.get_info(i_graph)\n",
    "\n",
    "        X, target = test_dataset.get_entire(i_graph)\n",
    "        pred = model(X)\n",
    "        pred, target = to_cpu_numpy(params, pred, target)\n",
    "\n",
    "        # 高い方が処理時間が短い\n",
    "        pred = -pred\n",
    "        ID = f\"layout:{arch}:{perm}:{filename}\"\n",
    "        records.append({\"ID\": ID, \"pred\": \";\".join(list(map(str, pred.argsort())))})\n",
    "        pred_records.append(\n",
    "            {\n",
    "                \"ID\": ID,\n",
    "                \"arch\": arch,\n",
    "                \"perm\": perm,\n",
    "                \"filename\": filename,\n",
    "                \"pred\": pred.tolist(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "with open(inferdir / \"pred.json\", \"w\") as f:\n",
    "    json.dump(pred_records, f, indent=4)\n",
    "\n",
    "dfpred = pd.DataFrame(records)\n",
    "dfpred.to_csv(inferdir / \"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
