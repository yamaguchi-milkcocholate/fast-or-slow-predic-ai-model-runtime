{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "from typing import Any\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "\n",
    "\n",
    "GPU = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n",
    "embeddir = Path().resolve() / \"out\" / \"ranknet\" / \"embeddings\"\n",
    "embeddir2 = rootdir / \"data\" / \"google-fast-vs-slowlayout9-embedding\"\n",
    "workdir = Path().resolve() / \"out\" / \"finetuning-listmle\"\n",
    "workdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "\n",
    "for ds in [\"train\", \"valid\", \"test\"]:\n",
    "    records = []\n",
    "    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n",
    "        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n",
    "        for filepath in sorted(datadir.glob(\"*.npz\")):\n",
    "            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arch\": arch,\n",
    "                    \"perm\": perm,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": filepath,\n",
    "                    \"embed_filepath\": embeddir / arch / perm / ds / f\"{filename}.npz\",\n",
    "                    \"embed_filepath2\": embeddir2 / arch / perm / ds / f\"{filename}.npz\",\n",
    "                }\n",
    "            )\n",
    "    dataset_dict[ds] = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset_dict:\n",
    "    indexes = []\n",
    "    for i, row in dataset_dict[ds].iterrows():\n",
    "        try:\n",
    "            np.load(row[\"filepath\"])\n",
    "            np.load(row[\"embed_filepath\"])\n",
    "            np.load(row[\"embed_filepath2\"])\n",
    "            indexes.append(i)\n",
    "        except FileNotFoundError as e:\n",
    "            print(row[\"embed_filepath2\"])\n",
    "\n",
    "    dataset_dict[ds] = dataset_dict[ds].iloc[indexes].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group17 (18, 6) (18, 3)\n",
      "group16 (30, 6) (30, 3)\n",
      "group15 (50, 6) (50, 3)\n",
      "group10 (22, 6) (22, 3)\n",
      "group9 (19, 6) (19, 3)\n",
      "group8 (21, 6) (21, 3)\n",
      "group7 (10, 6) (10, 3)\n",
      "group6 (10, 6) (10, 3)\n",
      "group5 (53, 6) (53, 3)\n",
      "group4 (20, 6) (20, 3)\n",
      "group3 (20, 6) (20, 3)\n",
      "group2 (18, 6) (18, 3)\n",
      "group1 (64, 6) (64, 3)\n",
      "group0 (17, 6) (17, 3)\n"
     ]
    }
   ],
   "source": [
    "sheet_dict = pd.read_excel(\n",
    "    \"/home/yamaguchi/kaggle/data/clustering-1112.xlsx\", sheet_name=None\n",
    ")\n",
    "train_valid_dataset = pd.concat(\n",
    "    [dataset_dict[\"train\"], dataset_dict[\"valid\"]], axis=0, ignore_index=True\n",
    ")\n",
    "group_dict = {}\n",
    "for group_name in sheet_dict:\n",
    "    if not group_name.startswith(\"group\"):\n",
    "        continue\n",
    "    dfgroup = sheet_dict[group_name].query(\"use == 1\")[[\"arch\", \"perm\", \"filename\"]]\n",
    "    dataset = dfgroup.merge(train_valid_dataset, on=[\"arch\", \"perm\", \"filename\"])\n",
    "    sheet_dict[group_name]\n",
    "    print(group_name, dataset.shape, dfgroup.shape)\n",
    "    if dataset.shape[0] > 0:\n",
    "        group_dict[group_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_tensor(dataset, filename):\n",
    "    dataset_as_dict = {}\n",
    "    embeddings_list = []\n",
    "    for i, row in dataset.iterrows():\n",
    "        fileobj = np.load(row[\"filepath\"])\n",
    "        embed_fileobj = np.load(row[\"embed_filepath\"])\n",
    "        embed_fileobj2 = np.load(row[\"embed_filepath2\"])\n",
    "        config_runtime = fileobj[\"config_runtime\"]\n",
    "        target = np.argsort(np.argsort(-config_runtime))\n",
    "        embeddings1 = embed_fileobj[\"embeddings\"]\n",
    "        embeddings2 = embed_fileobj[\"embeddings\"]\n",
    "        embeddings = np.concatenate([embeddings1, embeddings2], axis=1)\n",
    "\n",
    "        dataset_as_dict[i] = {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"target\": target,\n",
    "            \"X\": embeddings,\n",
    "        }\n",
    "        embeddings_list.append(embeddings)\n",
    "    embeddings_list = np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "    emb_scl = embeddings_list.max(axis=0) - embeddings_list.min(axis=0)\n",
    "    emb_mean = embeddings_list.mean(axis=0)\n",
    "    del embeddings_list\n",
    "\n",
    "    for i in dataset_as_dict:\n",
    "        dataset_as_dict[i][\"X\"] = (dataset_as_dict[i][\"X\"] - emb_mean) / emb_scl\n",
    "\n",
    "    with open(workdir / f\"{filename}.json\", \"w\") as f:\n",
    "        json.dump({\"xmean\": emb_mean.tolist(), \"xscl\": emb_scl.tolist()}, f, indent=4)\n",
    "    return dataset_as_dict\n",
    "\n",
    "\n",
    "def create_valid_dataset_tensor(dataset, filename):\n",
    "    dataset_as_dict = {}\n",
    "    for i, row in tqdm(dataset.iterrows()):\n",
    "        fileobj = np.load(row[\"filepath\"])\n",
    "        embed_fileobj = np.load(row[\"embed_filepath\"])\n",
    "        embed_fileobj2 = np.load(row[\"embed_filepath2\"])\n",
    "        config_runtime = fileobj[\"config_runtime\"]\n",
    "        target = np.argsort(np.argsort(-config_runtime))\n",
    "        embeddings1 = embed_fileobj[\"embeddings\"]\n",
    "        embeddings2 = embed_fileobj2[\"embeddings\"]\n",
    "        embeddings = np.concatenate([embeddings1, embeddings2], axis=1)\n",
    "\n",
    "        dataset_as_dict[i] = {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"target\": target,\n",
    "            \"X\": embeddings,\n",
    "        }\n",
    "\n",
    "    with open(workdir / f\"{filename}.json\", \"r\") as f:\n",
    "        scler = json.load(f)\n",
    "        emb_scl, emb_mean = np.array(scler[\"xscl\"]), np.array(scler[\"xmean\"])\n",
    "\n",
    "    for i in dataset_as_dict:\n",
    "        dataset_as_dict[i][\"X\"] = (dataset_as_dict[i][\"X\"] - emb_mean) / emb_scl\n",
    "\n",
    "    return dataset_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Params:\n",
    "    device: str\n",
    "    dims: list[int] = field(default_factory=lambda: [512, 512])\n",
    "    epoch: int = 500\n",
    "    T_max: int = 500\n",
    "    eta_min: float = 0\n",
    "    lr: float = 1e-5\n",
    "    weight_decay: float = 0\n",
    "    grad_clip_max_norm: float = 1.0\n",
    "    grad_clip_norm_type: float = 2.0\n",
    "    dropout_p: float = 0.05\n",
    "\n",
    "    sample_size: int = 1000\n",
    "    batch_size: int = 8\n",
    "\n",
    "    num_feats: int = 192 * 2\n",
    "\n",
    "\n",
    "params = Params(device=GPU if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_as_dict: dict[str, Any],\n",
    "        params: Params,\n",
    "    ) -> None:\n",
    "        self.dataset_as_dict = dataset_as_dict\n",
    "        self.params = params\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return self.params.device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset_as_dict)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dataset = self.dataset_as_dict[idx]\n",
    "        num_configs = dataset[\"target\"].shape[0]\n",
    "        indexes = random.choices(list(range(num_configs)), k=self.params.sample_size)\n",
    "\n",
    "        embeddings = torch.tensor(\n",
    "            dataset[\"X\"][indexes, :],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        target = torch.tensor(\n",
    "            dataset[\"target\"][indexes],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return embeddings, target\n",
    "\n",
    "    def get_entire(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        dataset = self.dataset_as_dict[idx]\n",
    "        embeddings = torch.tensor(\n",
    "            dataset[\"X\"],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        target = torch.tensor(\n",
    "            dataset[\"target\"],\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return embeddings, target\n",
    "\n",
    "    def get_info(self, idx):\n",
    "        dataset = self.dataset_as_dict[idx]\n",
    "        return dataset[\"arch\"], dataset[\"perm\"], dataset[\"filename\"]\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Params,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "\n",
    "        dims = [params.num_feats] + self.params.dims\n",
    "        fc_layer = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            fc_layer += [\n",
    "                nn.Dropout(self.params.dropout_p),\n",
    "                nn.Linear(\n",
    "                    in_features=dims[i],\n",
    "                    out_features=dims[i + 1],\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        fc_layer += [\n",
    "            nn.Dropout(self.params.dropout_p),\n",
    "            nn.Linear(\n",
    "                in_features=dims[-1],\n",
    "                out_features=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.net = nn.Sequential(*fc_layer)\n",
    "        self.to(self.params.device)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.net(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankNet(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    document_pairs_candidates = list(itertools.combinations(range(y_true.shape[1]), 2))\n",
    "\n",
    "    pairs_true = y_true[:, document_pairs_candidates]\n",
    "    selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "    pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "    true_diffs = true_diffs[the_mask]\n",
    "\n",
    "    return nn.BCEWithLogitsLoss()(pred_diffs, true_diffs)\n",
    "\n",
    "\n",
    "class ListMLE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.Tensor\n",
    "            予測（バッチサイズ, 要素数, ）\n",
    "        labels: torch.Tensor\n",
    "            目的変数（バッチサイズ, 要素数, ）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        # 正解をソート\n",
    "        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n",
    "        # 予測を正解順でソート\n",
    "        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n",
    "        # 予測値の最大値で予測値を引く（expの爆発予防）\n",
    "        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n",
    "        logits_sorted_by_true = logits_sorted_by_true - logits_max\n",
    "        # ランキングが低いものから累積する(その後正解順に戻す)\n",
    "        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n",
    "            dims=[1]\n",
    "        )\n",
    "        # 誤差\n",
    "        negative_log_likelihood = torch.sum(\n",
    "            torch.log(cumsums) - logits_sorted_by_true, dim=1\n",
    "        )\n",
    "        return torch.mean(negative_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def to_cpu_numpy(\n",
    "    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if params.device == GPU:\n",
    "        pred_ = pred.cpu().detach().numpy()\n",
    "        truth_ = truth.cpu().detach().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pred_ = pred.detach().numpy()\n",
    "        truth_ = truth.detach().numpy()\n",
    "    return pred_, truth_\n",
    "\n",
    "\n",
    "def train(params, train_dataset_as_dict, valid_dataset_as_dict, savedir):\n",
    "    train_dataset = FineTuningDataset(\n",
    "        dataset_as_dict=train_dataset_as_dict, params=params\n",
    "    )\n",
    "    valid_dataset = FineTuningDataset(\n",
    "        dataset_as_dict=valid_dataset_as_dict, params=params\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=params.batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    model = MLP(params=params)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n",
    "    )\n",
    "    criterion = ListMLE()\n",
    "\n",
    "    pbar = tqdm(range(params.epoch))\n",
    "    num_train_log, num_valid_log = 0, 0\n",
    "    best_score = -np.inf\n",
    "    best_epoch = None\n",
    "    for epoch in range(params.epoch):\n",
    "        is_log = (epoch + 1) % 10 == 0\n",
    "        model.train()\n",
    "\n",
    "        num_iters = len(train_dataloader)\n",
    "        train_scores = []\n",
    "        for i_iter, (X, target) in enumerate(train_dataloader):\n",
    "            pred = model(X)\n",
    "\n",
    "            if (len(pred.shape) == 1) or (len(target.shape) == 1):\n",
    "                pred, target = pred.reshape(1, -1), target.reshape(1, -1)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                max_norm=params.grad_clip_max_norm,\n",
    "                norm_type=params.grad_clip_norm_type,\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + i_iter / num_iters)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred, target = to_cpu_numpy(params, pred, target)\n",
    "\n",
    "            scores = []\n",
    "            for pred_, target_ in zip(pred, target):\n",
    "                score = kendalltau(target_, pred_).correlation\n",
    "                if is_log:\n",
    "                    # wandb.log(\n",
    "                    #     {\n",
    "                    #         \"epoch\": epoch,\n",
    "                    #         \"iter\": i_iter,\n",
    "                    #         \"lr\": scheduler.get_last_lr()[0],\n",
    "                    #         \"train/score\": score,\n",
    "                    #         \"train/loss\": loss,\n",
    "                    #         \"train/pred\": pred,\n",
    "                    #         \"train/target\": target,\n",
    "                    #         \"train/count\": num_train_log,\n",
    "                    #     }\n",
    "                    # )\n",
    "                    num_train_log += 1\n",
    "                scores.append(score)\n",
    "            scores = np.array(scores)\n",
    "            train_scores.append(scores)\n",
    "        train_scores = np.hstack(train_scores)\n",
    "\n",
    "        model.eval()\n",
    "        losses, scores = [], []\n",
    "        for i_graph in range(len(valid_dataset)):\n",
    "            arch, perm, filename = valid_dataset.get_info(i_graph)\n",
    "            X, target = valid_dataset[i_graph]\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred.reshape(1, -1), target.reshape(1, -1))\n",
    "            graph_loss = loss.item()\n",
    "            pred, target = to_cpu_numpy(params, pred, target)\n",
    "            score = kendalltau(target, pred).correlation\n",
    "\n",
    "            if is_log:\n",
    "                # wandb.log(\n",
    "                #     {\n",
    "                #         \"epoch\": epoch,\n",
    "                #         \"iter\": i_iter,\n",
    "                #         \"lr\": scheduler.get_last_lr()[0],\n",
    "                #         \"valid/arch\": arch,\n",
    "                #         \"valid/perm\": perm,\n",
    "                #         \"valid/filename\": filename,\n",
    "                #         \"valid/score\": score,\n",
    "                #         \"valid/loss\": graph_loss,\n",
    "                #         \"valid/pred\": pred,\n",
    "                #         \"valid/target\": target,\n",
    "                #         \"valid/count\": num_valid_log,\n",
    "                #     }\n",
    "                # )\n",
    "                num_valid_log += 1\n",
    "            losses.append(graph_loss)\n",
    "            scores.append(score)\n",
    "        losses = np.array(losses)\n",
    "        scores = np.array(scores)\n",
    "        score_ = np.mean(scores[~np.isnan(scores)])\n",
    "        # print(\n",
    "        #     f\"[{epoch + 1}] valid score={np.mean(scores[~np.isnan(scores)]):.3f} valid loss={np.mean(losses[~np.isnan(losses)]):5f}\"\n",
    "        # )\n",
    "\n",
    "        if best_score < score_:\n",
    "            best_score = score_\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), savedir / f\"best_model.pt\")\n",
    "\n",
    "        if is_log:\n",
    "            torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(\n",
    "            f\"[{epoch + 1}] train score={np.mean(train_scores[~np.isnan(train_scores)]):.3f} valid score={score_:.3f} best = {best_score:.3f} [{best_epoch}] valid loss={np.mean(losses[~np.isnan(losses)]):1f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccbe4b9a36843c1abef61f83c026e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892a9c25c4284fb3a69f490d76060cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamaguchi/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b144e3da170f43ffb96915ec5aa21002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84748d7b24884d3d8b359cb1e16bbbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214be0a56aa64f45ad1e2bb355ef3c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521f260a5c34c8782eb27aaf7f31897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c8c53f7b78405c914f3cf5eb1f2ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b19ae343842da8290e12498c3527c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9baa04050a44f1aa8c44354523da172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b95a92ccd247c1af3a204e60b0420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bae1cfd1fbe4b619a0b6583fa81a379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_name = \"group15\"\n",
    "seed = 0\n",
    "savedir = workdir / f\"{group_name}\" / f\"seed{seed}\"\n",
    "savedir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = group_dict[group_name].copy()\n",
    "\n",
    "valid_indexes = random.sample(list(range(dataset.shape[0])), dataset.shape[0] // 2)\n",
    "is_valids = np.zeros(dataset.shape[0]).astype(int)\n",
    "is_valids[valid_indexes] = 1\n",
    "dataset[\"valid\"] = is_valids\n",
    "dataset.to_csv(savedir / \"dataset.csv\", index=False)\n",
    "\n",
    "dftrain, dfvalid = dataset.query(\"valid == 0\").reset_index(drop=True), dataset.query(\n",
    "    \"valid == 1\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "train_dataset_as_dict = create_dataset_tensor(\n",
    "    dataset=dftrain, filename=f\"{group_name}-{seed}\"\n",
    ")\n",
    "valid_dataset_as_dict = create_valid_dataset_tensor(\n",
    "    dataset=dfvalid, filename=f\"{group_name}-{seed}\"\n",
    ")\n",
    "\n",
    "exptname = f\"1113-listmle-{group_name}-seed{seed}\"\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "#     config={\n",
    "#         \"params\": asdict(params),\n",
    "#     },\n",
    "#     name=exptname,\n",
    "# )\n",
    "\n",
    "for i in range(10):\n",
    "    model_savedir = savedir / f\"model{i + 1}\"\n",
    "    model_savedir.mkdir(exist_ok=True, parents=True)\n",
    "    seed_everything(i)\n",
    "    train(\n",
    "        params=params,\n",
    "        train_dataset_as_dict=train_dataset_as_dict,\n",
    "        valid_dataset_as_dict=valid_dataset_as_dict,\n",
    "        savedir=model_savedir,\n",
    "    )\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = FineTuningDataset(dataset_as_dict=valid_dataset_as_dict, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i_data in range(len(valid_dataset)):\n",
    "    X, target = valid_dataset.get_entire(i_data)\n",
    "    target = target.cpu().detach().numpy()\n",
    "\n",
    "    preds = []\n",
    "    record = {}\n",
    "    for i in range(10):\n",
    "        model_savedir = savedir / f\"model{i + 1}\"\n",
    "        model = MLP(params=params)\n",
    "        model.load_state_dict(torch.load(model_savedir / f\"best_model.pt\"))\n",
    "        model.eval()\n",
    "\n",
    "        pred = model(X)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        preds.append(pred)\n",
    "        entile_score = kendalltau(target, pred).correlation\n",
    "\n",
    "        sampled_score = []\n",
    "        for _ in range(100):\n",
    "            indexes = random.sample(list(range(pred.shape[0])), 1000)\n",
    "            score_ = kendalltau(target[indexes], pred[indexes]).correlation\n",
    "            sampled_score.append(score_)\n",
    "        sampled_score = np.mean(sampled_score)\n",
    "        record.update(\n",
    "            {\n",
    "                f\"entire{i + 1}\": entile_score,\n",
    "                f\"sampled{i + 1}\": sampled_score,\n",
    "            }\n",
    "        )\n",
    "    preds = np.vstack(preds)\n",
    "    preds_scl = (preds - preds.min(axis=1, keepdims=True)) / (\n",
    "        preds.max(axis=1, keepdims=True) - preds.min(axis=1, keepdims=True)\n",
    "    )\n",
    "    mean_ens_preds = preds_scl.mean(axis=0)\n",
    "    med_ens_preds = np.median(preds_scl, axis=0)\n",
    "\n",
    "    sampled_score_mean, sampled_score_med = [], []\n",
    "    for _ in range(100):\n",
    "        indexes = random.sample(list(range(pred.shape[0])), 1000)\n",
    "        score_mean_ = kendalltau(target[indexes], mean_ens_preds[indexes]).correlation\n",
    "        score_med_ = kendalltau(target[indexes], med_ens_preds[indexes]).correlation\n",
    "        sampled_score_mean.append(score_mean_)\n",
    "        sampled_score_med.append(score_med_)\n",
    "    sampled_score_mean = np.mean(sampled_score_mean)\n",
    "    sampled_score_med = np.mean(sampled_score_med)\n",
    "\n",
    "    record.update(\n",
    "        {\n",
    "            f\"entire_mean\": kendalltau(target, mean_ens_preds).correlation,\n",
    "            f\"entire_median\": kendalltau(target, med_ens_preds).correlation,\n",
    "            f\"sampled_mean\": sampled_score_mean,\n",
    "            f\"sampled_median\": sampled_score_med,\n",
    "        }\n",
    "    )\n",
    "    records.append(record)\n",
    "dfresult = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entire1</th>\n",
       "      <th>sampled1</th>\n",
       "      <th>entire2</th>\n",
       "      <th>sampled2</th>\n",
       "      <th>entire3</th>\n",
       "      <th>sampled3</th>\n",
       "      <th>entire4</th>\n",
       "      <th>sampled4</th>\n",
       "      <th>entire5</th>\n",
       "      <th>sampled5</th>\n",
       "      <th>...</th>\n",
       "      <th>entire8</th>\n",
       "      <th>sampled8</th>\n",
       "      <th>entire9</th>\n",
       "      <th>sampled9</th>\n",
       "      <th>entire10</th>\n",
       "      <th>sampled10</th>\n",
       "      <th>entire_mean</th>\n",
       "      <th>entire_median</th>\n",
       "      <th>sampled_mean</th>\n",
       "      <th>sampled_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341354</td>\n",
       "      <td>0.342989</td>\n",
       "      <td>0.331333</td>\n",
       "      <td>0.333615</td>\n",
       "      <td>0.317338</td>\n",
       "      <td>0.319576</td>\n",
       "      <td>0.354466</td>\n",
       "      <td>0.355693</td>\n",
       "      <td>0.332764</td>\n",
       "      <td>0.332677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345949</td>\n",
       "      <td>0.347477</td>\n",
       "      <td>0.352336</td>\n",
       "      <td>0.356040</td>\n",
       "      <td>0.422747</td>\n",
       "      <td>0.423232</td>\n",
       "      <td>0.354607</td>\n",
       "      <td>0.342401</td>\n",
       "      <td>0.356103</td>\n",
       "      <td>0.344025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.524238</td>\n",
       "      <td>0.524876</td>\n",
       "      <td>0.523609</td>\n",
       "      <td>0.524165</td>\n",
       "      <td>0.522498</td>\n",
       "      <td>0.523299</td>\n",
       "      <td>0.525656</td>\n",
       "      <td>0.525964</td>\n",
       "      <td>0.523192</td>\n",
       "      <td>0.525280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.524948</td>\n",
       "      <td>0.525359</td>\n",
       "      <td>0.523599</td>\n",
       "      <td>0.530481</td>\n",
       "      <td>0.530456</td>\n",
       "      <td>0.525682</td>\n",
       "      <td>0.524240</td>\n",
       "      <td>0.525487</td>\n",
       "      <td>0.524017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.556700</td>\n",
       "      <td>0.557642</td>\n",
       "      <td>0.556164</td>\n",
       "      <td>0.557075</td>\n",
       "      <td>0.554411</td>\n",
       "      <td>0.552543</td>\n",
       "      <td>0.558315</td>\n",
       "      <td>0.558633</td>\n",
       "      <td>0.555112</td>\n",
       "      <td>0.553745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558293</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.559344</td>\n",
       "      <td>0.560466</td>\n",
       "      <td>0.570771</td>\n",
       "      <td>0.569383</td>\n",
       "      <td>0.558576</td>\n",
       "      <td>0.556759</td>\n",
       "      <td>0.558897</td>\n",
       "      <td>0.557082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.227062</td>\n",
       "      <td>0.228606</td>\n",
       "      <td>0.220645</td>\n",
       "      <td>0.221563</td>\n",
       "      <td>0.201269</td>\n",
       "      <td>0.203408</td>\n",
       "      <td>0.226008</td>\n",
       "      <td>0.226568</td>\n",
       "      <td>0.211776</td>\n",
       "      <td>0.210387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234663</td>\n",
       "      <td>0.233910</td>\n",
       "      <td>0.241621</td>\n",
       "      <td>0.242529</td>\n",
       "      <td>0.242816</td>\n",
       "      <td>0.244233</td>\n",
       "      <td>0.236170</td>\n",
       "      <td>0.223772</td>\n",
       "      <td>0.239504</td>\n",
       "      <td>0.226853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117306</td>\n",
       "      <td>0.117956</td>\n",
       "      <td>0.116972</td>\n",
       "      <td>0.114313</td>\n",
       "      <td>0.112566</td>\n",
       "      <td>0.113661</td>\n",
       "      <td>0.124229</td>\n",
       "      <td>0.123438</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.113554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124980</td>\n",
       "      <td>0.123742</td>\n",
       "      <td>0.125152</td>\n",
       "      <td>0.125769</td>\n",
       "      <td>0.142887</td>\n",
       "      <td>0.145897</td>\n",
       "      <td>0.124762</td>\n",
       "      <td>0.120125</td>\n",
       "      <td>0.125475</td>\n",
       "      <td>0.120913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.131182</td>\n",
       "      <td>0.132426</td>\n",
       "      <td>0.130872</td>\n",
       "      <td>0.134556</td>\n",
       "      <td>0.127823</td>\n",
       "      <td>0.128243</td>\n",
       "      <td>0.138201</td>\n",
       "      <td>0.140684</td>\n",
       "      <td>0.128870</td>\n",
       "      <td>0.127882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137673</td>\n",
       "      <td>0.138123</td>\n",
       "      <td>0.138535</td>\n",
       "      <td>0.137059</td>\n",
       "      <td>0.154532</td>\n",
       "      <td>0.153192</td>\n",
       "      <td>0.138370</td>\n",
       "      <td>0.133729</td>\n",
       "      <td>0.140402</td>\n",
       "      <td>0.135769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.167834</td>\n",
       "      <td>0.168709</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>0.169579</td>\n",
       "      <td>0.163244</td>\n",
       "      <td>0.161303</td>\n",
       "      <td>0.174393</td>\n",
       "      <td>0.173594</td>\n",
       "      <td>0.165423</td>\n",
       "      <td>0.162420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177137</td>\n",
       "      <td>0.175541</td>\n",
       "      <td>0.177514</td>\n",
       "      <td>0.177009</td>\n",
       "      <td>0.196480</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.176387</td>\n",
       "      <td>0.170041</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.170196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.202355</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.199156</td>\n",
       "      <td>0.201235</td>\n",
       "      <td>0.181823</td>\n",
       "      <td>0.181454</td>\n",
       "      <td>0.207040</td>\n",
       "      <td>0.208180</td>\n",
       "      <td>0.187556</td>\n",
       "      <td>0.187764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198570</td>\n",
       "      <td>0.197626</td>\n",
       "      <td>0.196908</td>\n",
       "      <td>0.192841</td>\n",
       "      <td>0.211638</td>\n",
       "      <td>0.213956</td>\n",
       "      <td>0.209651</td>\n",
       "      <td>0.196571</td>\n",
       "      <td>0.210447</td>\n",
       "      <td>0.197447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.216697</td>\n",
       "      <td>0.216497</td>\n",
       "      <td>0.216262</td>\n",
       "      <td>0.215565</td>\n",
       "      <td>0.218955</td>\n",
       "      <td>0.217388</td>\n",
       "      <td>0.218272</td>\n",
       "      <td>0.215152</td>\n",
       "      <td>0.215954</td>\n",
       "      <td>0.217160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218747</td>\n",
       "      <td>0.217120</td>\n",
       "      <td>0.216440</td>\n",
       "      <td>0.219369</td>\n",
       "      <td>0.222786</td>\n",
       "      <td>0.222013</td>\n",
       "      <td>0.218867</td>\n",
       "      <td>0.217590</td>\n",
       "      <td>0.219458</td>\n",
       "      <td>0.218163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.513994</td>\n",
       "      <td>0.515226</td>\n",
       "      <td>0.514476</td>\n",
       "      <td>0.513908</td>\n",
       "      <td>0.499640</td>\n",
       "      <td>0.498421</td>\n",
       "      <td>0.526237</td>\n",
       "      <td>0.526517</td>\n",
       "      <td>0.499519</td>\n",
       "      <td>0.498867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524726</td>\n",
       "      <td>0.525678</td>\n",
       "      <td>0.526726</td>\n",
       "      <td>0.525706</td>\n",
       "      <td>0.591176</td>\n",
       "      <td>0.591185</td>\n",
       "      <td>0.525814</td>\n",
       "      <td>0.516360</td>\n",
       "      <td>0.527169</td>\n",
       "      <td>0.517838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.251906</td>\n",
       "      <td>0.251118</td>\n",
       "      <td>0.252638</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.244176</td>\n",
       "      <td>0.244469</td>\n",
       "      <td>0.257619</td>\n",
       "      <td>0.256278</td>\n",
       "      <td>0.244569</td>\n",
       "      <td>0.245220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257330</td>\n",
       "      <td>0.257867</td>\n",
       "      <td>0.257267</td>\n",
       "      <td>0.255060</td>\n",
       "      <td>0.280864</td>\n",
       "      <td>0.281616</td>\n",
       "      <td>0.258322</td>\n",
       "      <td>0.253316</td>\n",
       "      <td>0.255324</td>\n",
       "      <td>0.250204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.262456</td>\n",
       "      <td>0.263830</td>\n",
       "      <td>0.262954</td>\n",
       "      <td>0.263255</td>\n",
       "      <td>0.209119</td>\n",
       "      <td>0.208142</td>\n",
       "      <td>0.326462</td>\n",
       "      <td>0.325054</td>\n",
       "      <td>0.226213</td>\n",
       "      <td>0.226475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319077</td>\n",
       "      <td>0.317174</td>\n",
       "      <td>0.279891</td>\n",
       "      <td>0.278344</td>\n",
       "      <td>0.431660</td>\n",
       "      <td>0.432858</td>\n",
       "      <td>0.302686</td>\n",
       "      <td>0.283266</td>\n",
       "      <td>0.301729</td>\n",
       "      <td>0.281866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.502710</td>\n",
       "      <td>0.502677</td>\n",
       "      <td>0.501102</td>\n",
       "      <td>0.498838</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>0.488498</td>\n",
       "      <td>0.511303</td>\n",
       "      <td>0.510545</td>\n",
       "      <td>0.492875</td>\n",
       "      <td>0.493312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511461</td>\n",
       "      <td>0.512109</td>\n",
       "      <td>0.508461</td>\n",
       "      <td>0.507969</td>\n",
       "      <td>0.540223</td>\n",
       "      <td>0.541023</td>\n",
       "      <td>0.508777</td>\n",
       "      <td>0.502135</td>\n",
       "      <td>0.506784</td>\n",
       "      <td>0.500154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.312864</td>\n",
       "      <td>0.315651</td>\n",
       "      <td>0.296709</td>\n",
       "      <td>0.297265</td>\n",
       "      <td>0.247038</td>\n",
       "      <td>0.245259</td>\n",
       "      <td>0.321290</td>\n",
       "      <td>0.324011</td>\n",
       "      <td>0.270912</td>\n",
       "      <td>0.271589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>0.334601</td>\n",
       "      <td>0.302162</td>\n",
       "      <td>0.302516</td>\n",
       "      <td>0.416310</td>\n",
       "      <td>0.414867</td>\n",
       "      <td>0.335395</td>\n",
       "      <td>0.312881</td>\n",
       "      <td>0.339596</td>\n",
       "      <td>0.317331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.534757</td>\n",
       "      <td>0.534064</td>\n",
       "      <td>0.533308</td>\n",
       "      <td>0.535621</td>\n",
       "      <td>0.527877</td>\n",
       "      <td>0.528764</td>\n",
       "      <td>0.536806</td>\n",
       "      <td>0.538683</td>\n",
       "      <td>0.529142</td>\n",
       "      <td>0.527953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537169</td>\n",
       "      <td>0.536896</td>\n",
       "      <td>0.536474</td>\n",
       "      <td>0.534961</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>0.553259</td>\n",
       "      <td>0.537428</td>\n",
       "      <td>0.533111</td>\n",
       "      <td>0.536450</td>\n",
       "      <td>0.532161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.177371</td>\n",
       "      <td>0.175503</td>\n",
       "      <td>0.143172</td>\n",
       "      <td>0.141451</td>\n",
       "      <td>0.095197</td>\n",
       "      <td>0.095270</td>\n",
       "      <td>0.223103</td>\n",
       "      <td>0.222468</td>\n",
       "      <td>0.135789</td>\n",
       "      <td>0.134699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.197726</td>\n",
       "      <td>0.185288</td>\n",
       "      <td>0.185009</td>\n",
       "      <td>0.307437</td>\n",
       "      <td>0.307786</td>\n",
       "      <td>0.225374</td>\n",
       "      <td>0.160935</td>\n",
       "      <td>0.225590</td>\n",
       "      <td>0.160731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.289870</td>\n",
       "      <td>0.288562</td>\n",
       "      <td>0.289942</td>\n",
       "      <td>0.288201</td>\n",
       "      <td>0.281627</td>\n",
       "      <td>0.279935</td>\n",
       "      <td>0.295291</td>\n",
       "      <td>0.295468</td>\n",
       "      <td>0.283504</td>\n",
       "      <td>0.282952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295373</td>\n",
       "      <td>0.296302</td>\n",
       "      <td>0.293971</td>\n",
       "      <td>0.296031</td>\n",
       "      <td>0.308926</td>\n",
       "      <td>0.309936</td>\n",
       "      <td>0.295670</td>\n",
       "      <td>0.290118</td>\n",
       "      <td>0.295523</td>\n",
       "      <td>0.290148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.206795</td>\n",
       "      <td>0.209089</td>\n",
       "      <td>0.218270</td>\n",
       "      <td>0.219331</td>\n",
       "      <td>0.183462</td>\n",
       "      <td>0.181995</td>\n",
       "      <td>0.234446</td>\n",
       "      <td>0.236231</td>\n",
       "      <td>0.197655</td>\n",
       "      <td>0.196602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228075</td>\n",
       "      <td>0.227930</td>\n",
       "      <td>0.252599</td>\n",
       "      <td>0.251687</td>\n",
       "      <td>0.349733</td>\n",
       "      <td>0.350704</td>\n",
       "      <td>0.255977</td>\n",
       "      <td>0.220737</td>\n",
       "      <td>0.256132</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.251949</td>\n",
       "      <td>0.253351</td>\n",
       "      <td>0.253851</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>0.233206</td>\n",
       "      <td>0.234955</td>\n",
       "      <td>0.264907</td>\n",
       "      <td>0.262834</td>\n",
       "      <td>0.240477</td>\n",
       "      <td>0.241571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265564</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.270754</td>\n",
       "      <td>0.271898</td>\n",
       "      <td>0.336575</td>\n",
       "      <td>0.334574</td>\n",
       "      <td>0.269559</td>\n",
       "      <td>0.258147</td>\n",
       "      <td>0.266776</td>\n",
       "      <td>0.255399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.134350</td>\n",
       "      <td>0.136780</td>\n",
       "      <td>0.131010</td>\n",
       "      <td>0.129873</td>\n",
       "      <td>0.096312</td>\n",
       "      <td>0.093433</td>\n",
       "      <td>0.131533</td>\n",
       "      <td>0.129787</td>\n",
       "      <td>0.110811</td>\n",
       "      <td>0.109367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130425</td>\n",
       "      <td>0.128375</td>\n",
       "      <td>0.145622</td>\n",
       "      <td>0.145855</td>\n",
       "      <td>0.147008</td>\n",
       "      <td>0.147811</td>\n",
       "      <td>0.140479</td>\n",
       "      <td>0.124144</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>0.124871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.014055</td>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>-0.001097</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>-0.026238</td>\n",
       "      <td>-0.026837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.012290</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>-0.015427</td>\n",
       "      <td>-0.016661</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>-0.008494</td>\n",
       "      <td>0.013195</td>\n",
       "      <td>-0.006857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.552908</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.543588</td>\n",
       "      <td>0.543042</td>\n",
       "      <td>0.547072</td>\n",
       "      <td>0.546523</td>\n",
       "      <td>0.560896</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.549693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557181</td>\n",
       "      <td>0.559045</td>\n",
       "      <td>0.556802</td>\n",
       "      <td>0.558183</td>\n",
       "      <td>0.580328</td>\n",
       "      <td>0.579537</td>\n",
       "      <td>0.559633</td>\n",
       "      <td>0.554401</td>\n",
       "      <td>0.559136</td>\n",
       "      <td>0.553913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.208316</td>\n",
       "      <td>-0.209860</td>\n",
       "      <td>-0.171594</td>\n",
       "      <td>-0.171595</td>\n",
       "      <td>-0.207216</td>\n",
       "      <td>-0.207944</td>\n",
       "      <td>-0.220232</td>\n",
       "      <td>-0.220509</td>\n",
       "      <td>-0.226948</td>\n",
       "      <td>-0.225174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171699</td>\n",
       "      <td>-0.172494</td>\n",
       "      <td>-0.209710</td>\n",
       "      <td>-0.209665</td>\n",
       "      <td>0.081743</td>\n",
       "      <td>0.081105</td>\n",
       "      <td>-0.176419</td>\n",
       "      <td>-0.191588</td>\n",
       "      <td>-0.177893</td>\n",
       "      <td>-0.193216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.156483</td>\n",
       "      <td>0.157105</td>\n",
       "      <td>0.150978</td>\n",
       "      <td>0.151693</td>\n",
       "      <td>0.161611</td>\n",
       "      <td>0.159187</td>\n",
       "      <td>0.169146</td>\n",
       "      <td>0.168396</td>\n",
       "      <td>0.166026</td>\n",
       "      <td>0.166137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141450</td>\n",
       "      <td>0.142528</td>\n",
       "      <td>0.171104</td>\n",
       "      <td>0.170717</td>\n",
       "      <td>-0.109478</td>\n",
       "      <td>-0.107971</td>\n",
       "      <td>0.152979</td>\n",
       "      <td>0.158229</td>\n",
       "      <td>0.149188</td>\n",
       "      <td>0.153875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.313682</td>\n",
       "      <td>0.315953</td>\n",
       "      <td>0.304374</td>\n",
       "      <td>0.302549</td>\n",
       "      <td>0.313413</td>\n",
       "      <td>0.316543</td>\n",
       "      <td>0.321340</td>\n",
       "      <td>0.322666</td>\n",
       "      <td>0.308269</td>\n",
       "      <td>0.309638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330035</td>\n",
       "      <td>0.330634</td>\n",
       "      <td>0.319081</td>\n",
       "      <td>0.319084</td>\n",
       "      <td>0.325236</td>\n",
       "      <td>0.326489</td>\n",
       "      <td>0.325119</td>\n",
       "      <td>0.317679</td>\n",
       "      <td>0.325886</td>\n",
       "      <td>0.318471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     entire1  sampled1   entire2  sampled2   entire3  sampled3   entire4  \\\n",
       "0   0.341354  0.342989  0.331333  0.333615  0.317338  0.319576  0.354466   \n",
       "1   0.524238  0.524876  0.523609  0.524165  0.522498  0.523299  0.525656   \n",
       "2   0.556700  0.557642  0.556164  0.557075  0.554411  0.552543  0.558315   \n",
       "3   0.227062  0.228606  0.220645  0.221563  0.201269  0.203408  0.226008   \n",
       "4   0.117306  0.117956  0.116972  0.114313  0.112566  0.113661  0.124229   \n",
       "5   0.131182  0.132426  0.130872  0.134556  0.127823  0.128243  0.138201   \n",
       "6   0.167834  0.168709  0.167127  0.169579  0.163244  0.161303  0.174393   \n",
       "7   0.202355  0.202509  0.199156  0.201235  0.181823  0.181454  0.207040   \n",
       "8   0.216697  0.216497  0.216262  0.215565  0.218955  0.217388  0.218272   \n",
       "9   0.513994  0.515226  0.514476  0.513908  0.499640  0.498421  0.526237   \n",
       "10  0.251906  0.251118  0.252638  0.252645  0.244176  0.244469  0.257619   \n",
       "11  0.262456  0.263830  0.262954  0.263255  0.209119  0.208142  0.326462   \n",
       "12  0.502710  0.502677  0.501102  0.498838  0.491793  0.488498  0.511303   \n",
       "13  0.312864  0.315651  0.296709  0.297265  0.247038  0.245259  0.321290   \n",
       "14  0.534757  0.534064  0.533308  0.535621  0.527877  0.528764  0.536806   \n",
       "15  0.177371  0.175503  0.143172  0.141451  0.095197  0.095270  0.223103   \n",
       "16  0.289870  0.288562  0.289942  0.288201  0.281627  0.279935  0.295291   \n",
       "17  0.206795  0.209089  0.218270  0.219331  0.183462  0.181995  0.234446   \n",
       "18  0.251949  0.253351  0.253851  0.251412  0.233206  0.234955  0.264907   \n",
       "19  0.134350  0.136780  0.131010  0.129873  0.096312  0.093433  0.131533   \n",
       "20  0.014055  0.013216  0.002142  0.003855  0.001818  0.002524 -0.001097   \n",
       "21  0.552908  0.552651  0.543588  0.543042  0.547072  0.546523  0.560896   \n",
       "22 -0.208316 -0.209860 -0.171594 -0.171595 -0.207216 -0.207944 -0.220232   \n",
       "23  0.156483  0.157105  0.150978  0.151693  0.161611  0.159187  0.169146   \n",
       "24  0.313682  0.315953  0.304374  0.302549  0.313413  0.316543  0.321340   \n",
       "\n",
       "    sampled4   entire5  sampled5  ...   entire8  sampled8   entire9  sampled9  \\\n",
       "0   0.355693  0.332764  0.332677  ...  0.345949  0.347477  0.352336  0.356040   \n",
       "1   0.525964  0.523192  0.525280  ...  0.525400  0.524948  0.525359  0.523599   \n",
       "2   0.558633  0.555112  0.553745  ...  0.558293  0.557991  0.559344  0.560466   \n",
       "3   0.226568  0.211776  0.210387  ...  0.234663  0.233910  0.241621  0.242529   \n",
       "4   0.123438  0.114300  0.113554  ...  0.124980  0.123742  0.125152  0.125769   \n",
       "5   0.140684  0.128870  0.127882  ...  0.137673  0.138123  0.138535  0.137059   \n",
       "6   0.173594  0.165423  0.162420  ...  0.177137  0.175541  0.177514  0.177009   \n",
       "7   0.208180  0.187556  0.187764  ...  0.198570  0.197626  0.196908  0.192841   \n",
       "8   0.215152  0.215954  0.217160  ...  0.218747  0.217120  0.216440  0.219369   \n",
       "9   0.526517  0.499519  0.498867  ...  0.524726  0.525678  0.526726  0.525706   \n",
       "10  0.256278  0.244569  0.245220  ...  0.257330  0.257867  0.257267  0.255060   \n",
       "11  0.325054  0.226213  0.226475  ...  0.319077  0.317174  0.279891  0.278344   \n",
       "12  0.510545  0.492875  0.493312  ...  0.511461  0.512109  0.508461  0.507969   \n",
       "13  0.324011  0.270912  0.271589  ...  0.331195  0.334601  0.302162  0.302516   \n",
       "14  0.538683  0.529142  0.527953  ...  0.537169  0.536896  0.536474  0.534961   \n",
       "15  0.222468  0.135789  0.134699  ...  0.192000  0.197726  0.185288  0.185009   \n",
       "16  0.295468  0.283504  0.282952  ...  0.295373  0.296302  0.293971  0.296031   \n",
       "17  0.236231  0.197655  0.196602  ...  0.228075  0.227930  0.252599  0.251687   \n",
       "18  0.262834  0.240477  0.241571  ...  0.265564  0.265812  0.270754  0.271898   \n",
       "19  0.129787  0.110811  0.109367  ...  0.130425  0.128375  0.145622  0.145855   \n",
       "20  0.000422 -0.026238 -0.026837  ...  0.005126  0.004894  0.012290  0.011656   \n",
       "21  0.561700  0.548500  0.549693  ...  0.557181  0.559045  0.556802  0.558183   \n",
       "22 -0.220509 -0.226948 -0.225174  ... -0.171699 -0.172494 -0.209710 -0.209665   \n",
       "23  0.168396  0.166026  0.166137  ...  0.141450  0.142528  0.171104  0.170717   \n",
       "24  0.322666  0.308269  0.309638  ...  0.330035  0.330634  0.319081  0.319084   \n",
       "\n",
       "    entire10  sampled10  entire_mean  entire_median  sampled_mean  \\\n",
       "0   0.422747   0.423232     0.354607       0.342401      0.356103   \n",
       "1   0.530481   0.530456     0.525682       0.524240      0.525487   \n",
       "2   0.570771   0.569383     0.558576       0.556759      0.558897   \n",
       "3   0.242816   0.244233     0.236170       0.223772      0.239504   \n",
       "4   0.142887   0.145897     0.124762       0.120125      0.125475   \n",
       "5   0.154532   0.153192     0.138370       0.133729      0.140402   \n",
       "6   0.196480   0.197900     0.176387       0.170041      0.176543   \n",
       "7   0.211638   0.213956     0.209651       0.196571      0.210447   \n",
       "8   0.222786   0.222013     0.218867       0.217590      0.219458   \n",
       "9   0.591176   0.591185     0.525814       0.516360      0.527169   \n",
       "10  0.280864   0.281616     0.258322       0.253316      0.255324   \n",
       "11  0.431660   0.432858     0.302686       0.283266      0.301729   \n",
       "12  0.540223   0.541023     0.508777       0.502135      0.506784   \n",
       "13  0.416310   0.414867     0.335395       0.312881      0.339596   \n",
       "14  0.554306   0.553259     0.537428       0.533111      0.536450   \n",
       "15  0.307437   0.307786     0.225374       0.160935      0.225590   \n",
       "16  0.308926   0.309936     0.295670       0.290118      0.295523   \n",
       "17  0.349733   0.350704     0.255977       0.220737      0.256132   \n",
       "18  0.336575   0.334574     0.269559       0.258147      0.266776   \n",
       "19  0.147008   0.147811     0.140479       0.124144      0.141393   \n",
       "20 -0.015427  -0.016661     0.010855      -0.008494      0.013195   \n",
       "21  0.580328   0.579537     0.559633       0.554401      0.559136   \n",
       "22  0.081743   0.081105    -0.176419      -0.191588     -0.177893   \n",
       "23 -0.109478  -0.107971     0.152979       0.158229      0.149188   \n",
       "24  0.325236   0.326489     0.325119       0.317679      0.325886   \n",
       "\n",
       "    sampled_median  \n",
       "0         0.344025  \n",
       "1         0.524017  \n",
       "2         0.557082  \n",
       "3         0.226853  \n",
       "4         0.120913  \n",
       "5         0.135769  \n",
       "6         0.170196  \n",
       "7         0.197447  \n",
       "8         0.218163  \n",
       "9         0.517838  \n",
       "10        0.250204  \n",
       "11        0.281866  \n",
       "12        0.500154  \n",
       "13        0.317331  \n",
       "14        0.532161  \n",
       "15        0.160731  \n",
       "16        0.290148  \n",
       "17        0.220700  \n",
       "18        0.255399  \n",
       "19        0.124871  \n",
       "20       -0.006857  \n",
       "21        0.553913  \n",
       "22       -0.193216  \n",
       "23        0.153875  \n",
       "24        0.318471  \n",
       "\n",
       "[25 rows x 24 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.423232\n",
       "1     0.530481\n",
       "2     0.570771\n",
       "3     0.244233\n",
       "4     0.145897\n",
       "5     0.154532\n",
       "6     0.197900\n",
       "7     0.213956\n",
       "8     0.222786\n",
       "9     0.591185\n",
       "10    0.281616\n",
       "11    0.432858\n",
       "12    0.541023\n",
       "13    0.416310\n",
       "14    0.554306\n",
       "15    0.307786\n",
       "16    0.309936\n",
       "17    0.350704\n",
       "18    0.336575\n",
       "19    0.147811\n",
       "20    0.014055\n",
       "21    0.580328\n",
       "22    0.081743\n",
       "23    0.171104\n",
       "24    0.330634\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresult.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3546072749552976"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33974317241987534"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(43, 43 + 10):\n",
    "    savedir = workdir / f\"{group_name}\" / f\"seed{seed}\"\n",
    "    model_savedir = savedir / \"model\"\n",
    "\n",
    "    dataset = pd.read_csv(savedir / \"dataset.csv\")\n",
    "    # valid_indexes = random.sample(list(range(dataset.shape[0])), dataset.shape[0] // 2)\n",
    "    # is_valids = np.zeros(dataset.shape[0]).astype(int)\n",
    "    # is_valids[valid_indexes] = 1\n",
    "    # dataset[\"valid\"] = is_valids\n",
    "    # dataset.to_csv(savedir / \"dataset.csv\", index=False)\n",
    "\n",
    "    # dftrain, dfvalid = dataset.query(\"valid == 0\").reset_index(\n",
    "    #     drop=True\n",
    "    # ), dataset.query(\"valid == 1\").reset_index(drop=True)\n",
    "\n",
    "    # train_dataset_as_dict = create_dataset_tensor(dataset=dftrain, filename=group_name)\n",
    "    # valid_dataset_as_dict = create_valid_dataset_tensor(\n",
    "    #     dataset=dfvalid, filename=group_name\n",
    "    # )\n",
    "\n",
    "    # train(\n",
    "    #     params=params,\n",
    "    #     train_dataset_as_dict=train_dataset_as_dict,\n",
    "    #     valid_dataset_as_dict=valid_dataset_as_dict,\n",
    "    #     savedir=model_savedir,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 43\n",
    "# for group_name in group_dict:\n",
    "#     dataset = group_dict[group_name]\n",
    "#     dataset_as_dict = create_dataset_tensor(dataset=dataset, filename=group_name)\n",
    "\n",
    "#     exptname = f\"1113-勾配クリップ-ミスマッチ-listmle-finetuning-{group_name}-seed{seed}\"\n",
    "\n",
    "#     wandb.init(\n",
    "#         project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "#         config={\n",
    "#             \"params\": asdict(params),\n",
    "#         },\n",
    "#         name=exptname,\n",
    "#     )\n",
    "\n",
    "#     seed_everything(seed)\n",
    "\n",
    "#     savedir = workdir / f\"{group_name}\" / f\"seed{seed}\"\n",
    "#     savedir.mkdir(exist_ok=True, parents=True)\n",
    "#     train(params=params, dataset_as_dict=dataset_as_dict, savedir=savedir)\n",
    "\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
