{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.nn import GCNConv, Sequential\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from dataclasses import asdict\n",
    "\n",
    "sns.set()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rootdir = Path().resolve().parent.parent\n",
    "inputdir = rootdir / \"data\" / \"predict-ai-model-runtime\"\n",
    "node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fast-layout-7-85\"\n",
    "trans_node_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout6-92-dataset\"\n",
    "trans_node_config_feat_dir = rootdir / \"data\" / \"google-slow-vs-fastlayout7-81-dataset\"\n",
    "workdir = Path().resolve() / \"out\"\n",
    "workdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "ignores = []\n",
    "for ds in [\"train\", \"valid\", \"test\"]:\n",
    "    records = []\n",
    "    for arch, perm in itertools.product([\"nlp\", \"xla\"], [\"default\", \"random\"]):\n",
    "        datadir = inputdir / f\"npz_all/npz/layout/{arch}/{perm}/{ds}\"\n",
    "        for filepath in sorted(datadir.glob(\"*.npz\")):\n",
    "            filename = str(filepath).split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "\n",
    "            if (ds != \"test\") and ((\"mlperf\" in filename) or (\"openai\" in filename)):\n",
    "                ignores.append(filepath)\n",
    "                continue\n",
    "            records.append(\n",
    "                {\n",
    "                    \"arch\": arch,\n",
    "                    \"perm\": perm,\n",
    "                    \"filename\": filename,\n",
    "                    \"filepath\": filepath,\n",
    "                    \"node_feat_filepath\": str(\n",
    "                        node_feat_dir / arch / perm / ds / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_feat_filepath\": str(\n",
    "                        trans_node_feat_dir\n",
    "                        / \"layout\"\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                    \"trans_node_config_filepath\": str(\n",
    "                        trans_node_config_feat_dir\n",
    "                        / arch\n",
    "                        / perm\n",
    "                        / ds\n",
    "                        / f\"{filename}.npz\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "    dataset_dict[ds] = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for filepath in tqdm(ignores):\n",
    "#     node_config_feat = np.load(filepath)[\"node_config_feat\"]\n",
    "\n",
    "#     for i in range(1, node_config_feat.shape[0]):\n",
    "#         if not (node_config_feat[0] == node_config_feat[i]).all():\n",
    "#             filepath\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "      <th>cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats  \\\n",
       "0       0         1        19   \n",
       "1       1        68         6   \n",
       "\n",
       "                                                cats  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "1                                 [0, 1, 2, 3, 4, 5]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat = pd.DataFrame(\n",
    "    [\n",
    "        {\"number\": 0, \"num_dims\": 1, \"num_cats\": 19, \"cats\": list(range(19))},\n",
    "        {\"number\": 1, \"num_dims\": 54 + 14, \"num_cats\": 6, \"cats\": list(range(6))},\n",
    "    ]\n",
    ")\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num_dims</th>\n",
       "      <th>num_cats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num_dims  num_cats\n",
       "0       0        18         8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcat_config = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"number\": 0,\n",
    "            \"num_dims\": 18,\n",
    "            \"num_cats\": 8,\n",
    "        },  # output_layout, input_layout, kernel_layout\n",
    "    ]\n",
    ")\n",
    "dfcat_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in dataset_dict:\n",
    "    for i, row in dataset_dict[ds].iterrows():\n",
    "        np.load(row[\"filepath\"])\n",
    "        np.load(row[\"node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_feat_filepath\"])\n",
    "        np.load(row[\"trans_node_config_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データクラスを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CatStatus:\n",
    "    dfcat: pd.DataFrame\n",
    "    prefix: str\n",
    "    num_cat_dict: dict[str, int] = field(init=False)\n",
    "    index_dict: dict[str, list[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.num_cat_dict, self.index_dict = {}, {}\n",
    "        dim_start = 0\n",
    "        for i, row in self.dfcat.iterrows():\n",
    "            self.num_cat_dict[f\"{self.prefix}cat_feat{i + 1}\"] = row[\"num_cats\"]\n",
    "            self.index_dict[f\"{self.prefix}cat_feat{i + 1}\"] = list(\n",
    "                range(dim_start, dim_start + row[\"num_dims\"])\n",
    "            )\n",
    "            dim_start += row[\"num_dims\"]\n",
    "\n",
    "\n",
    "cat_status = CatStatus(dfcat=dfcat, prefix=\"\")\n",
    "cat_config_status = CatStatus(dfcat=dfcat_config, prefix=\"config_\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Const:\n",
    "    num_node_flag_feat_dim: int\n",
    "    num_node_cont_feat_dim: int\n",
    "    num_node_cat_feat_dim: int\n",
    "    num_node_config_cont_feat_dim: int\n",
    "\n",
    "    # 演算子の種類\n",
    "    num_operations: int = 120\n",
    "    # 各configの次元数\n",
    "    num_config_dims: int = 6\n",
    "\n",
    "\n",
    "fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"node_feat_filepath\"])\n",
    "trans_fileobj = np.load(dataset_dict[\"train\"].iloc[0][\"trans_node_feat_filepath\"])\n",
    "trans_config_fileobj = np.load(\n",
    "    dataset_dict[\"train\"].iloc[0][\"trans_node_config_filepath\"]\n",
    ")\n",
    "\n",
    "node_flag_feat, node_cont_feat = fileobj[\"node_flag_feat\"], fileobj[\"node_cont_feat\"]\n",
    "node_enum_feat, node_dimension_number_feat = (\n",
    "    fileobj[\"node_enum_feat\"],\n",
    "    fileobj[\"node_dimension_number_feat\"],\n",
    ")\n",
    "trans_node_cont_feat, trans_node_cat_feat = (\n",
    "    trans_fileobj[\"node_cont_feat\"],\n",
    "    trans_fileobj[\"node_cat_feat\"],\n",
    ")\n",
    "trans_node_config_cont_feat = trans_config_fileobj[\"node_config_cont_feat\"]\n",
    "const = Const(\n",
    "    num_node_flag_feat_dim=node_flag_feat.shape[1] + 1,  # config_idsの分+1\n",
    "    num_node_cont_feat_dim=node_cont_feat.shape[1] + trans_node_cont_feat.shape[1],\n",
    "    num_node_cat_feat_dim=node_enum_feat.shape[1]\n",
    "    + node_dimension_number_feat.shape[1]\n",
    "    + trans_node_cat_feat.shape[1],\n",
    "    num_node_config_cont_feat_dim=trans_node_config_cont_feat.shape[2],\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NodeFeatExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope: float = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GNNExtractor:\n",
    "    dims: list[int] = field(default_factory=lambda: [64, 64])\n",
    "    leakyrelu_negative_slope = 0.1\n",
    "    dropout_p: float = 0.2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CatEmbedding:\n",
    "    num_cat: int\n",
    "    embedding_dim: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    device: str\n",
    "    cat_embeddings: dict[str, CatEmbedding]\n",
    "    random_batch_size: int = 30\n",
    "    batch_size: int = 30\n",
    "    node_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    node_config_feat_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    gnn_extractor: GNNExtractor = field(default_factory=lambda: GNNExtractor())\n",
    "    subgraph_extractor: NodeFeatExtractor = field(\n",
    "        default_factory=lambda: NodeFeatExtractor()\n",
    "    )\n",
    "    epoch: int = 20\n",
    "    T_max: int = 20\n",
    "    eta_min: float = 1e-5\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0\n",
    "    grad_clip_max_norm: float = 1.0\n",
    "    grad_clip_norm_type: float = 2.0\n",
    "\n",
    "\n",
    "cat_embeddings = {}\n",
    "cat_embeddings.update(\n",
    "    {\"op\": CatEmbedding(num_cat=const.num_operations, embedding_dim=16)}\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "cat_embeddings.update(\n",
    "    {\n",
    "        k: CatEmbedding(num_cat=v, embedding_dim=16)\n",
    "        for k, v in cat_config_status.num_cat_dict.items()\n",
    "    }\n",
    ")\n",
    "params = Params(\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    cat_embeddings=cat_embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LayoutConfigs:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    node_cont_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 108)\n",
    "\n",
    "    node_cat_feat: np.ndarray\n",
    "        ノード特徴量、(ノード数, 3)\n",
    "\n",
    "    node_opcode: np.ndarray\n",
    "        ノード演算子、(ノード数,)\n",
    "    edge_index: np.ndarray\n",
    "        エッジ、(エッジ数, 2)\n",
    "\n",
    "    node_config_feat: np.ndarray\n",
    "        設定毎のノード特徴量、(設定数, 設定可能なノード数, 3)\n",
    "\n",
    "    node_config_ids: np.ndarray\n",
    "        設定可能なノードのIndex、(設定可能なノード数,)\n",
    "    config_runtime: np.ndarray\n",
    "        実行時間、(設定数,)\n",
    "    node_splits: np.ndarray\n",
    "        同じパーティションでの計算を意味する。今回は使用しない。(パーティション数, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    node_flag_feat: np.ndarray\n",
    "    node_cont_feat: np.ndarray\n",
    "    node_cat_feat: np.ndarray\n",
    "    node_opcode: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    node_config_feat: np.ndarray\n",
    "    node_config_cont_feat: np.ndarray\n",
    "    node_config_ids: np.ndarray\n",
    "    config_runtime: np.ndarray\n",
    "    node_splits: np.ndarray\n",
    "\n",
    "    cat_status: CatStatus\n",
    "    cat_config_status: CatStatus\n",
    "    target: np.ndarray = field(init=False)\n",
    "    argsorted_indexs: list[int] = field(init=False)\n",
    "\n",
    "    NUM_SAMPLES: int = 1000\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # 設定が存在するノードのフラグ\n",
    "        node_active_feat = np.zeros((self.num_nodes, 1))\n",
    "        node_active_feat[self.node_config_ids, :] = 1\n",
    "        self.node_flag_feat = np.concatenate(\n",
    "            [self.node_flag_feat, node_active_feat], axis=1\n",
    "        )\n",
    "        self.node_cont_feat = self.apply_normalization(x=self.node_cont_feat)\n",
    "        self.node_config_feat = self.node_config_feat + 1  # カテゴリは0~7にする\n",
    "        self.node_splits = np.array(\n",
    "            [\n",
    "                [self.node_splits[0][i], self.node_splits[0][i + 1] - 1]\n",
    "                for i in range(self.node_splits.shape[1] - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.target = self.apply_target_normalization(x=self.config_runtime)\n",
    "        self.argsorted_indexs = np.argsort(self.config_runtime).tolist()\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        \"\"\"ノード数\"\"\"\n",
    "        return self.node_cont_feat.shape[0]\n",
    "\n",
    "    def get_random_config_idxs(self) -> list[int]:\n",
    "        \"\"\"tpu_graphのサンプリング方法\n",
    "        https://github.com/google-research-datasets/tpu_graphs/blob/main/tpu_graphs/baselines/layout/data.py#L352\n",
    "        \"\"\"\n",
    "        num_configs = self.config_runtime.shape[0]\n",
    "        num_samples = min(self.NUM_SAMPLES, num_configs)\n",
    "        third = num_samples // 3\n",
    "\n",
    "        middle_samples = np.random.choice(\n",
    "            self.argsorted_indexs[third:-third], num_samples - 2 * third\n",
    "        ).tolist()\n",
    "        samples = (\n",
    "            self.argsorted_indexs[:third]\n",
    "            + self.argsorted_indexs[-third:]\n",
    "            + middle_samples\n",
    "        )\n",
    "        samples = random.sample(samples, len(samples))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def get_filled_node_config_feat(\n",
    "        self, index_list: list[int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"指定された設定の設定毎のノード特徴量を取得する。設定がない場合は補完する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray [(len(index_list),ノード数, 18), (len(index_list),ノード数, 連続次元数)]\n",
    "        \"\"\"\n",
    "        # (サンプル数, ノード数) x 3\n",
    "        node_config_feat = np.full(\n",
    "            (len(index_list), self.num_nodes, Const.num_config_dims * 3),\n",
    "            Const.num_config_dims + 1,\n",
    "        )\n",
    "        node_config_feat[:, self.node_config_ids] = self.node_config_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "\n",
    "        node_config_cont_feat = np.zeros(\n",
    "            (len(index_list), self.num_nodes, self.node_config_cont_feat.shape[2])\n",
    "        )\n",
    "        node_config_cont_feat[:, self.node_config_ids] = self.node_config_cont_feat[\n",
    "            index_list, :, :\n",
    "        ]\n",
    "        return node_config_feat, node_config_cont_feat\n",
    "\n",
    "    def get_target(self, index_list: list[int]) -> np.ndarray:\n",
    "        \"\"\"指定された設定の目的変数を取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_list: list[int]\n",
    "            設定のIndex\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "        \"\"\"\n",
    "        return self.apply_target_ranking(x=self.config_runtime[index_list])\n",
    "\n",
    "    def apply_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"特徴量の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            2次元行列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            行方向に正規化された行列\n",
    "        \"\"\"\n",
    "        x /= 128\n",
    "        x = np.where(x >= 0, np.log1p(x / 128), -np.log1p(-x / 128))\n",
    "        return x\n",
    "\n",
    "    def apply_target_normalization(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"目的変数の正規化\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            ベクトル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x: np.ndarray\n",
    "            正規化されたベクトル\n",
    "        \"\"\"\n",
    "        return np.log(x / x.min())\n",
    "\n",
    "    def apply_target_ranking(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"降順でランキング\"\"\"\n",
    "        return np.argsort(np.argsort(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayoutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    rows: list[dict[str, np.ndarray]]\n",
    "        設定をリストでもつ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: pd.DataFrame,\n",
    "        params: Params,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        self.rows = dataset.to_dict(\"records\")\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "        self.cache_idx = None\n",
    "        self.cache_filepath = None\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return self.params.device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def create_layout_config(self, idx: int) -> LayoutConfigs:\n",
    "        if self.cache_idx != idx:\n",
    "            self.cache_idx = idx\n",
    "            fileobj = np.load(self.rows[self.cache_idx][\"filepath\"])\n",
    "            node_feat_fileobj = np.load(self.rows[self.cache_idx][\"node_feat_filepath\"])\n",
    "            trans_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_feat_filepath\"]\n",
    "            )\n",
    "            trans_config_feat_fileobj = np.load(\n",
    "                self.rows[self.cache_idx][\"trans_node_config_filepath\"]\n",
    "            )\n",
    "\n",
    "            node_cont_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_cont_feat\"],\n",
    "                    trans_feat_fileobj[\"node_cont_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            node_cat_feat = np.concatenate(\n",
    "                [\n",
    "                    node_feat_fileobj[\"node_enum_feat\"],\n",
    "                    node_feat_fileobj[\"node_dimension_number_feat\"],\n",
    "                    trans_feat_fileobj[\"node_cat_feat\"],\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            self.cache_layout_config = LayoutConfigs(\n",
    "                node_opcode=fileobj[\"node_opcode\"],\n",
    "                edge_index=fileobj[\"edge_index\"],\n",
    "                node_config_ids=fileobj[\"node_config_ids\"],\n",
    "                config_runtime=fileobj[\"config_runtime\"],\n",
    "                node_splits=fileobj[\"node_splits\"],\n",
    "                node_flag_feat=node_feat_fileobj[\"node_flag_feat\"],\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=fileobj[\"node_config_feat\"],\n",
    "                node_config_cont_feat=trans_config_feat_fileobj[\n",
    "                    \"node_config_cont_feat\"\n",
    "                ],\n",
    "                cat_status=self.cat_status,\n",
    "                cat_config_status=self.cat_config_status,\n",
    "            )\n",
    "        return self.cache_layout_config\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def getitem_as_random_batch(\n",
    "        self, idx: int\n",
    "    ) -> list[\n",
    "        tuple[\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "        ]\n",
    "    ]:\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "\n",
    "        index_list = layout_configs.get_random_config_idxs()\n",
    "        for i_chunk in range(0, len(index_list), self.params.random_batch_size):\n",
    "            chunk_index_list = index_list[\n",
    "                i_chunk : i_chunk + self.params.random_batch_size\n",
    "            ]\n",
    "            yield self._get_tensors(\n",
    "                layout_configs=layout_configs, index_list=chunk_index_list\n",
    "            )\n",
    "\n",
    "    def getitem_as_batch(\n",
    "        self, idx: int\n",
    "    ) -> list[\n",
    "        tuple[\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "            torch.Tensor,\n",
    "        ]\n",
    "    ]:\n",
    "        \"\"\"設定をバッチで取得する\"\"\"\n",
    "        layout_configs = self.create_layout_config(idx=idx)\n",
    "\n",
    "        index_list = list(range(layout_configs.config_runtime.shape[0]))\n",
    "        for i_chunk in range(0, len(index_list), self.params.batch_size):\n",
    "            chunk_index_list = index_list[i_chunk : i_chunk + self.params.batch_size]\n",
    "            yield self._get_tensors(\n",
    "                layout_configs=layout_configs, index_list=chunk_index_list\n",
    "            )\n",
    "\n",
    "    def _get_tensors(\n",
    "        self, layout_configs: LayoutConfigs, index_list: list[int]\n",
    "    ) -> tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        \"\"\"渡された設定のIndexのテンソルを取得する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layout_configs: LayoutConfigs\n",
    "            Layoutのデータクラス\n",
    "        index_list: list[int]\n",
    "            設定のインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            ノード特徴量(フラグ)\n",
    "        torch.Tensor\n",
    "            ノード特徴量(連続)\n",
    "        dict[str, torch.Tensor]\n",
    "            ノード特徴量(カテゴリ)\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量\n",
    "        torch.Tensor\n",
    "            設定毎のノード特徴量(連続)\n",
    "        torch.Tensor\n",
    "            ノード演算子\n",
    "        torch.Tensor\n",
    "            エッジ\n",
    "        torch.Tensor\n",
    "            目的変数\n",
    "        \"\"\"\n",
    "        # ノード特徴量(フラグ)\n",
    "        node_flag_feat = torch.tensor(\n",
    "            layout_configs.node_flag_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(連続)\n",
    "        node_cont_feat = torch.tensor(\n",
    "            layout_configs.node_cont_feat,\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "        # ノード特徴量(カテゴリ)\n",
    "        node_cat_feat = torch.tensor(\n",
    "            layout_configs.node_cat_feat,\n",
    "            dtype=torch.int64,\n",
    "        ).to(self.device)\n",
    "        # 設定毎のノード特徴量(カテゴリ)\n",
    "        (\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "        ) = layout_configs.get_filled_node_config_feat(index_list=index_list)\n",
    "        node_config_feat = torch.tensor(node_config_feat, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        node_config_cont_feat = torch.tensor(\n",
    "            node_config_cont_feat, dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        # ノード演算子\n",
    "        node_opcode = torch.tensor(layout_configs.node_opcode, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # エッジ\n",
    "        edge_index = torch.tensor(\n",
    "            np.swapaxes(layout_configs.edge_index, 0, 1), dtype=torch.int64\n",
    "        ).to(self.device)\n",
    "        # サブグラフ\n",
    "        node_splits = torch.tensor(layout_configs.node_splits, dtype=torch.int64).to(\n",
    "            self.device\n",
    "        )\n",
    "        # ターゲット\n",
    "        target = torch.tensor(\n",
    "            layout_configs.get_target(index_list=index_list),\n",
    "            dtype=torch.float32,\n",
    "        ).to(self.device)\n",
    "\n",
    "        return (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        )\n",
    "\n",
    "    def get_ith_file_info(self, i: int) -> dict[str, str]:\n",
    "        row = self.rows[i]\n",
    "        return {\n",
    "            \"arch\": row[\"arch\"],\n",
    "            \"perm\": row[\"perm\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "        }\n",
    "\n",
    "    def get_ith_runtime(self, i: int) -> np.ndarray:\n",
    "        layout_configs = self.create_layout_config(idx=i)\n",
    "        return layout_configs.config_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "\n",
    "class EdgeConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    ノード特徴 + 隣接ノード特徴 + 隣接ノード特徴の一致\n",
    "    参考： https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html#implementing-the-edge-convolution\n",
    "    補足: 集約関数はデフォルトでdim(axis) = -2。つまりノード方向で集約するので気にしなくてOK\n",
    "    https://github.com/pyg-team/pytorch_geometric/blob/1e12d41c28b1fb9793f17646b018071b508864d7/torch_geometric/nn/aggr/basic.py#L38\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_input_dim: int, x_output_dim: int, dropout_p: float):\n",
    "        # \"Add\" aggregation\n",
    "        super().__init__(aggr=\"max\")\n",
    "        self.mlp = nn.Sequential(\n",
    "            # nn.LayerNorm(x_input_dim * 2),\n",
    "            nn.Linear(x_input_dim * 2, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.ReLU(),\n",
    "            # nn.LayerNorm(x_output_dim),\n",
    "            nn.Linear(x_output_dim, x_output_dim),\n",
    "            # nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [設定数, N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"propagate()で渡された引数xから自動でx_i, x_jノードを取り出して随時処理を実装する関数\"\"\"\n",
    "        # x_i has shape [設定数, エッジ数, in_channels]\n",
    "        # x_j has shape [設定数, エッジ数, in_channels]\n",
    "        x_cat = torch.cat(\n",
    "            [x_i, x_i - x_j], dim=2\n",
    "        )  # tmp has shape [設定数, エッジ数, 2 * in_channels]\n",
    "        return self.mlp(x_cat)\n",
    "\n",
    "\n",
    "class SimpleLayoutModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params: Params\n",
    "        実験設定のデータクラス\n",
    "    node_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(ノード毎)\n",
    "    node_config_embeddings: torch.Tensor\n",
    "        カテゴリ変数の埋め込み表現(設定xノード毎)\n",
    "    node_feat_extractor: torch.nn.Module\n",
    "        ノードの特徴量を抽出するネットワーク\n",
    "    gnn_extractor: torch.nn.Module\n",
    "        グラフの特徴量を抽出するネットワーク\n",
    "    gc: torch.nn.Module\n",
    "        最終層の全結合層\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Params,\n",
    "        const: Const,\n",
    "        cat_status: CatStatus,\n",
    "        cat_config_status: CatStatus,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.cat_status = cat_status\n",
    "        self.cat_config_status = cat_config_status\n",
    "\n",
    "        # カテゴリ変数の埋め込み表現\n",
    "        self.embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                k: torch.nn.Embedding(v.num_cat, v.embedding_dim)\n",
    "                for k, v in self.params.cat_embeddings.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # node_featのfeature_extractorを定義\n",
    "        num_node_feat_extractor_input_dim = (\n",
    "            const.num_node_flag_feat_dim\n",
    "            + const.num_node_cont_feat_dim\n",
    "            + self.num_node_feat_embedding_dims\n",
    "        )\n",
    "\n",
    "        node_feat_extractor_layer = []\n",
    "        node_feat_extractor_dims = [\n",
    "            num_node_feat_extractor_input_dim\n",
    "        ] + self.params.node_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_feat_extractor_dims[i],\n",
    "                    out_features=node_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(params.node_feat_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "            self.node_feat_extractor = nn.Sequential(*node_feat_extractor_layer)\n",
    "\n",
    "        # node_config_featのfeature_extractorを定義\n",
    "        num_node_config_feat_extractor_input_dim = (\n",
    "            self.num_node_config_feat_embedding_dims\n",
    "            + const.num_node_config_cont_feat_dim\n",
    "        )\n",
    "\n",
    "        node_config_feat_extractor_layer = []\n",
    "        node_config_feat_extractor_dims = [\n",
    "            num_node_config_feat_extractor_input_dim\n",
    "        ] + self.params.node_config_feat_extractor.dims\n",
    "        for i in range(len(node_feat_extractor_dims) - 1):\n",
    "            node_config_feat_extractor_layer += [\n",
    "                # nn.LayerNorm(node_config_feat_extractor_dims[i]),\n",
    "                nn.Linear(\n",
    "                    in_features=node_config_feat_extractor_dims[i],\n",
    "                    out_features=node_config_feat_extractor_dims[i + 1],\n",
    "                ),\n",
    "                # nn.Dropout(params.node_config_feat_extractor.dropout_p),\n",
    "                nn.LeakyReLU(\n",
    "                    params.node_config_feat_extractor.leakyrelu_negative_slope\n",
    "                ),\n",
    "            ]\n",
    "        self.node_config_feat_extractor = nn.Sequential(\n",
    "            *node_config_feat_extractor_layer\n",
    "        )\n",
    "\n",
    "        # ノード間のfeature_extractorの定義\n",
    "        num_gnn_extractor_input_dim = (\n",
    "            node_feat_extractor_dims[-1] + node_config_feat_extractor_dims[-1]\n",
    "        )\n",
    "\n",
    "        gnn_extractor_layer = []\n",
    "        gnn_extractor_dims = [\n",
    "            num_gnn_extractor_input_dim\n",
    "        ] + self.params.gnn_extractor.dims\n",
    "        for i in range(len(gnn_extractor_dims) - 1):\n",
    "            gnn_extractor_layer += [\n",
    "                (\n",
    "                    EdgeConv(\n",
    "                        x_input_dim=gnn_extractor_dims[i],\n",
    "                        x_output_dim=gnn_extractor_dims[i + 1],\n",
    "                        dropout_p=params.gnn_extractor.dropout_p,\n",
    "                    ),\n",
    "                    \"x, edge_index -> x\",\n",
    "                ),\n",
    "                nn.LeakyReLU(params.gnn_extractor.leakyrelu_negative_slope),\n",
    "            ]\n",
    "        self.gnn_extractor = Sequential(\"x, edge_index\", gnn_extractor_layer)\n",
    "\n",
    "        # # サブグラフのfeature_extractorの定義\n",
    "        # num_subgraph_extractor_input_dim = (\n",
    "        #     self.params.gnn_extractor.dims[-1] + num_gnn_extractor_input_dim\n",
    "        # )\n",
    "\n",
    "        # subgraph_extractor_layer = []\n",
    "        # subgraph_extractor_dims = [\n",
    "        #     num_subgraph_extractor_input_dim\n",
    "        # ] + self.params.node_feat_extractor.dims\n",
    "        # for i in range(len(subgraph_extractor_dims) - 1):\n",
    "        #     subgraph_extractor_layer += [\n",
    "        #         # nn.LayerNorm(subgraph_extractor_dims[i]),\n",
    "        #         nn.Linear(\n",
    "        #             in_features=subgraph_extractor_dims[i],\n",
    "        #             out_features=subgraph_extractor_dims[i + 1],\n",
    "        #         ),\n",
    "        #         # nn.Dropout(params.subgraph_extractor.dropout_p),\n",
    "        #         nn.LeakyReLU(params.subgraph_extractor.leakyrelu_negative_slope),\n",
    "        #     ]\n",
    "        # self.subgraph_extractor = nn.Sequential(*subgraph_extractor_layer)\n",
    "\n",
    "        fc_layer = [\n",
    "            # nn.LayerNorm(subgraph_extractor_dims[-1]),\n",
    "            # nn.Linear(in_features=subgraph_extractor_dims[-1], out_features=1),\n",
    "            nn.Linear(\n",
    "                in_features=self.params.gnn_extractor.dims[-1]\n",
    "                + num_gnn_extractor_input_dim,\n",
    "                out_features=1,\n",
    "            ),\n",
    "        ]\n",
    "        self.fc = nn.Sequential(*fc_layer)\n",
    "        self.to(self.params.device)\n",
    "\n",
    "    @property\n",
    "    def num_node_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        num_embedding_dims += 1 * self.params.cat_embeddings[\"op\"].embedding_dim\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    @property\n",
    "    def num_node_config_feat_embedding_dims(self) -> int:\n",
    "        num_embedding_dims = 0\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            num_embedding_dims += (\n",
    "                len(cat_index) * self.params.cat_embeddings[cat_name].embedding_dim\n",
    "            )\n",
    "        return num_embedding_dims\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "        node_config_feat: torch.Tensor,\n",
    "        node_config_cont_feat: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        node_splits: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ------\n",
    "        node_flag_feat:\n",
    "            ノードの特徴量(node数, フラグ次元数)\n",
    "        node_cont_feat:\n",
    "            ノードの特徴量(node数, 連続次元数)\n",
    "        node_cat_feat:\n",
    "            ノードの特徴量(node数, カテゴリ次元数*埋め込み次元数)\n",
    "        node_config_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 特徴次元数)\n",
    "        node_config_cont_feat:\n",
    "            設定毎のノードの特徴量(設定数, node数, 連続次元数)\n",
    "        edge_index:\n",
    "            エッジ(2, エッジ数)\n",
    "        node_splits:\n",
    "            サブグラフのインデックス（サブグラフ数, 2)\n",
    "\n",
    "        Returns:\n",
    "        torch.tensor: (設定数)\n",
    "        \"\"\"\n",
    "        # (ノード数,特徴数)のテンソルを作成\n",
    "        node_feat = self._join_node_feature(\n",
    "            node_opcode=node_opcode,\n",
    "            node_flag_feat=node_flag_feat,\n",
    "            node_cont_feat=node_cont_feat,\n",
    "            node_cat_feat=node_cat_feat,\n",
    "        )\n",
    "\n",
    "        # (設定数,ノード数,特徴数)のテンソルを作成\n",
    "        node_config_feat = self._join_node_config_feature(\n",
    "            node_config_feat=node_config_feat,\n",
    "            node_config_cont_feat=node_config_cont_feat,\n",
    "        )\n",
    "\n",
    "        # node_featの抽出器を通す\n",
    "        extracted_node_feat = self.node_feat_extractor(node_feat)\n",
    "\n",
    "        # node_config_featの抽出器を通す\n",
    "        extracted_node_config_feat = self.node_config_feat_extractor(node_config_feat)\n",
    "\n",
    "        # 設定毎のノード特徴に結合する\n",
    "        extracted_feat = self._join_entire_node_config_feat(\n",
    "            node_feat=extracted_node_feat,\n",
    "            node_config_feat=extracted_node_config_feat,\n",
    "        )\n",
    "\n",
    "        # GNN抽出器を通す\n",
    "        conved_extracted_feat = self.gnn_extractor(\n",
    "            x=extracted_feat,\n",
    "            edge_index=edge_index,\n",
    "        )\n",
    "\n",
    "        # 残差を足すイメージ\n",
    "        concat_feat = torch.concat([extracted_feat, conved_extracted_feat], 2)\n",
    "\n",
    "        # subgraph_global_pool_feat_list = []\n",
    "        # for subgraph_start_node_idx, subgraph_end_node_idx in node_splits:\n",
    "        #     subgraph_concat_feat = concat_feat[\n",
    "        #         :, subgraph_start_node_idx : subgraph_end_node_idx + 1, :\n",
    "        #     ]\n",
    "        #     # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        #     subgraph_global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "        #     subgraph_global_pool_feat_list.append(\n",
    "        #         torch.reshape(\n",
    "        #             subgraph_global_pool_feat,\n",
    "        #             (\n",
    "        #                 subgraph_global_pool_feat.shape[0],\n",
    "        #                 1,\n",
    "        #                 subgraph_global_pool_feat.shape[1],\n",
    "        #             ),\n",
    "        #         )\n",
    "        #     )\n",
    "        # # （設定数,サブグラフ数,特徴数)\n",
    "        # subgraph_global_pool_feat = torch.concat(subgraph_global_pool_feat_list, 1)\n",
    "        # subgraph_extracted_feat = self.subgraph_extractor(subgraph_global_pool_feat)\n",
    "\n",
    "        # ノードの特徴量を足し合わせる(Global mean Pooling)\n",
    "        # global_pool_feat = torch.mean(subgraph_extracted_feat, dim=1)\n",
    "        global_pool_feat = torch.mean(concat_feat, dim=1)\n",
    "\n",
    "        return torch.squeeze(self.fc(global_pool_feat))\n",
    "\n",
    "    def _join_node_feature(\n",
    "        self,\n",
    "        node_opcode: torch.Tensor,\n",
    "        node_flag_feat: torch.Tensor,\n",
    "        node_cont_feat: torch.Tensor,\n",
    "        node_cat_feat: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_featのテンソルを作成\"\"\"\n",
    "        # ノードの埋め込み表現\n",
    "        node_embeddings_list = []\n",
    "        node_embeddings_list.append(self.embeddings[\"op\"](node_opcode))\n",
    "        for cat_name, cat_index in self.cat_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](node_cat_feat[:, cat_index])\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (-1, node_embeddings.shape[-2] * node_embeddings.shape[-1]),\n",
    "            )\n",
    "            node_embeddings_list.append(node_embeddings)\n",
    "\n",
    "        # ノード毎で埋め込み、結合(ノード数, 特徴数)\n",
    "        node_embedding_feat = torch.concat(node_embeddings_list, 1)\n",
    "        node_feat = torch.concat(\n",
    "            [node_flag_feat, node_cont_feat, node_embedding_feat], 1\n",
    "        )\n",
    "        return node_feat\n",
    "\n",
    "    def _join_node_config_feature(\n",
    "        self, node_config_feat: torch.Tensor, node_config_cont_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"node_config_featのテンソルを作成\"\"\"\n",
    "        # 設定xノード毎で埋め込み(設定数, ノード数, 特徴数)\n",
    "        node_config_embeddings_list = []\n",
    "        for cat_name, cat_index in self.cat_config_status.index_dict.items():\n",
    "            node_embeddings = self.embeddings[cat_name](\n",
    "                node_config_feat[:, :, cat_index]\n",
    "            )\n",
    "            node_embeddings = torch.reshape(\n",
    "                node_embeddings,\n",
    "                (\n",
    "                    node_embeddings.shape[0],\n",
    "                    -1,\n",
    "                    node_embeddings.shape[-2] * node_embeddings.shape[-1],\n",
    "                ),\n",
    "            )\n",
    "            node_config_embeddings_list.append(node_embeddings)\n",
    "        node_config_feat = torch.concat(\n",
    "            node_config_embeddings_list + [node_config_cont_feat], 2\n",
    "        )\n",
    "        return node_config_feat\n",
    "\n",
    "    def _join_entire_node_config_feat(\n",
    "        self, node_feat: torch.Tensor, node_config_feat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # ノード毎の特徴量を設定数だけ縦に並べる\n",
    "        node_tiled_feat = torch.tile(\n",
    "            torch.reshape(node_feat, (1, node_feat.shape[0], node_feat.shape[1])),\n",
    "            (node_config_feat.shape[0], 1, 1),\n",
    "        )\n",
    "        return torch.concat([node_tiled_feat, node_config_feat], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ListMLE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.Tensor\n",
    "            予測（要素数, ）\n",
    "        labels: torch.Tensor\n",
    "            目的変数（要素数, ）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        \"\"\"\n",
    "        # 正解をソート\n",
    "        labels_sorted, labels_sorted_indice = labels.sort(descending=True, dim=1)\n",
    "        # 予測を正解順でソート\n",
    "        logits_sorted_by_true = torch.gather(logits, dim=1, index=labels_sorted_indice)\n",
    "        # 予測値の最大値で予測値を引く（expの爆発予防）\n",
    "        logits_max, _ = logits_sorted_by_true.max(dim=1, keepdim=True)\n",
    "        logits_sorted_by_true = logits_sorted_by_true - logits_max\n",
    "        # ランキングが低いものから累積する(その後正解順に戻す)\n",
    "        cumsums = torch.cumsum(logits_sorted_by_true.exp().flip(dims=[1]), dim=1).flip(\n",
    "            dims=[1]\n",
    "        )\n",
    "        # 誤差\n",
    "        negative_log_likelihood = torch.sum(\n",
    "            torch.log(cumsums) - logits_sorted_by_true, dim=1\n",
    "        )\n",
    "        return torch.mean(negative_log_likelihood)\n",
    "\n",
    "\n",
    "def rankNet(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    # here we generate every pair of indices from the range of document length in the batch\n",
    "    document_pairs_candidates = list(\n",
    "        itertools.product(range(y_true.shape[1]), repeat=2)\n",
    "    )\n",
    "\n",
    "    pairs_true = y_true[:, document_pairs_candidates]\n",
    "    selected_pred = y_pred[:, document_pairs_candidates]\n",
    "\n",
    "    # here we calculate the relative true relevance of every candidate pair\n",
    "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
    "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
    "\n",
    "    # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
    "    # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
    "    # positive ones for a simpler loss function formulation\n",
    "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
    "\n",
    "    pred_diffs = pred_diffs[the_mask]\n",
    "\n",
    "    weight = None\n",
    "    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
    "    # whether one document is better than the other and not about the actual difference in\n",
    "    # their relevancy levels\n",
    "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
    "    true_diffs = true_diffs[the_mask]\n",
    "\n",
    "    return nn.BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)\n",
    "\n",
    "\n",
    "def to_cpu_numpy(\n",
    "    params: Params, pred: torch.Tensor, truth: torch.Tensor\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    if params.device == \"cuda\":\n",
    "        pred_ = pred.cpu().detach().numpy()\n",
    "        truth_ = truth.cpu().detach().numpy()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pred_ = pred.detach().numpy()\n",
    "        truth_ = truth.detach().numpy()\n",
    "    return pred_, truth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "def evaluate_score(dataset: LayoutDataset, model: torch.nn.Module) -> pd.DataFrame:\n",
    "    \"\"\"データセット全件に対してコンペの評価指標を算出する\n",
    "    https://www.kaggle.com/competitions/predict-ai-model-runtime/overview\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    records = []\n",
    "    # 各グラフ毎にスコアを算出\n",
    "    for graph_index in range(len(dataset)):\n",
    "        # グラフ毎に1000件をバッチに分けて取得\n",
    "        preds, truths = [], []\n",
    "        for (\n",
    "            node_opcode,\n",
    "            node_flag_feat,\n",
    "            node_cont_feat,\n",
    "            node_cat_feat,\n",
    "            node_config_feat,\n",
    "            node_config_cont_feat,\n",
    "            edge_index,\n",
    "            node_splits,\n",
    "            target,\n",
    "        ) in dataset.getitem_as_random_batch(graph_index):\n",
    "            pred = model(\n",
    "                node_opcode=node_opcode,\n",
    "                node_flag_feat=node_flag_feat,\n",
    "                node_cont_feat=node_cont_feat,\n",
    "                node_cat_feat=node_cat_feat,\n",
    "                node_config_feat=node_config_feat,\n",
    "                node_config_cont_feat=node_config_cont_feat,\n",
    "                edge_index=edge_index,\n",
    "                node_splits=node_splits,\n",
    "            )\n",
    "            pred, truth = to_cpu_numpy(params, pred, target)\n",
    "            preds.append(pred)\n",
    "            truths.append(truth)\n",
    "\n",
    "        preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "\n",
    "        loss = rankNet(\n",
    "            torch.tensor(preds.reshape(1, -1)),\n",
    "            torch.tensor(truths.reshape(1, -1)),\n",
    "        )\n",
    "        graph_loss = loss.item()\n",
    "        score = kendalltau(truth, pred).correlation\n",
    "\n",
    "        record = dataset.get_ith_file_info(graph_index)\n",
    "        record.update(\n",
    "            {\n",
    "                \"graph_loss\": graph_loss,\n",
    "                \"score\": score,\n",
    "            }\n",
    "        )\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    dftrain: pd.DataFrame,\n",
    "    dfvalid: pd.DataFrame,\n",
    "    params: Params,\n",
    "    const: Const,\n",
    "    cat_status: CatStatus,\n",
    "    cat_config_status: CatStatus,\n",
    "    savedir: Path,\n",
    "    checkpoint_dir: Path = None,\n",
    ") -> None:\n",
    "    train_layout_dataset = LayoutDataset(\n",
    "        dataset=dftrain,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    valid_layout_dataset = LayoutDataset(\n",
    "        dataset=dfvalid,\n",
    "        params=params,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "\n",
    "    model = SimpleLayoutModel(\n",
    "        params=params,\n",
    "        const=const,\n",
    "        cat_status=cat_status,\n",
    "        cat_config_status=cat_config_status,\n",
    "    )\n",
    "    if checkpoint_dir is not None:\n",
    "        print(\"学習済みモデルを読み込みます\")\n",
    "        model.load_state_dict(torch.load(checkpoint_dir / f\"final_model.pt\"))\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=params.lr, weight_decay=params.weight_decay\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=params.T_max, eta_min=params.eta_min\n",
    "    )\n",
    "    # criterion = ListMLE()\n",
    "\n",
    "    best_score = -np.inf\n",
    "    records = []\n",
    "    for epoch in range(params.epoch):\n",
    "        model.train()\n",
    "\n",
    "        num_graph = len(train_layout_dataset)\n",
    "        pbar = tqdm(range(num_graph))\n",
    "        graph_indexes = random.sample(list(range(num_graph)), num_graph)\n",
    "\n",
    "        epoch_losses = []\n",
    "        epoch_loss = 0  # 各グラフの誤差を総和（エポックの誤差）\n",
    "\n",
    "        # グラフをシャッフルして取得\n",
    "        for i_graph, graph_index in enumerate(graph_indexes):\n",
    "            graph_info = train_layout_dataset.get_ith_file_info(graph_index)\n",
    "            graph_arch, graph_perm = graph_info[\"arch\"], graph_info[\"perm\"]\n",
    "            # 各グラフで1000件をバッチに分けて取得\n",
    "            preds, truths = [], []\n",
    "            graph_loss = 0  # バッチの誤差を総和（グラフの誤差）\n",
    "            num_batch_count = 0\n",
    "            for (\n",
    "                node_opcode,\n",
    "                node_flag_feat,\n",
    "                node_cont_feat,\n",
    "                node_cat_feat,\n",
    "                node_config_feat,\n",
    "                node_config_cont_feat,\n",
    "                edge_index,\n",
    "                node_splits,\n",
    "                target,\n",
    "            ) in train_layout_dataset.getitem_as_random_batch(graph_index):\n",
    "                out = model(\n",
    "                    node_opcode=node_opcode,\n",
    "                    node_flag_feat=node_flag_feat,\n",
    "                    node_cont_feat=node_cont_feat,\n",
    "                    node_cat_feat=node_cat_feat,\n",
    "                    node_config_feat=node_config_feat,\n",
    "                    node_config_cont_feat=node_config_cont_feat,\n",
    "                    edge_index=edge_index,\n",
    "                    node_splits=node_splits,\n",
    "                )\n",
    "                # loss = criterion(\n",
    "                #     torch.reshape(out, (1, out.shape[0])),\n",
    "                #     torch.reshape(target, (1, target.shape[0])),\n",
    "                # )\n",
    "                loss = rankNet(\n",
    "                    torch.reshape(out, (1, out.shape[0])),\n",
    "                    torch.reshape(target, (1, target.shape[0])),\n",
    "                )\n",
    "                loss.backward()\n",
    "                graph_loss += loss.item()\n",
    "\n",
    "                pred, truth = to_cpu_numpy(params, out, target)\n",
    "                preds.append(pred)\n",
    "                truths.append(truth)\n",
    "                num_batch_count += 1\n",
    "\n",
    "            # 各グラフ毎に勾配降下\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                max_norm=params.grad_clip_max_norm,\n",
    "                norm_type=params.grad_clip_norm_type,\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + i_graph / num_graph)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "            score = kendalltau(truth, pred).correlation\n",
    "            graph_loss /= num_batch_count  # 各バッチの平均をグラフの誤差とする\n",
    "            epoch_loss += graph_loss\n",
    "\n",
    "            record = {\n",
    "                \"epoch\": epoch,\n",
    "                \"i_graph\": i_graph,\n",
    "                f\"train-{graph_arch}-{graph_perm}/epoch_loss\": epoch_loss\n",
    "                / (i_graph + 1),\n",
    "                f\"train-{graph_arch}-{graph_perm}/graph_loss\": graph_loss,\n",
    "                f\"train-{graph_arch}-{graph_perm}/score\": score,\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            record.update(graph_info)\n",
    "            records.append(record)\n",
    "\n",
    "            wandb.log(record)\n",
    "            pbar.set_description(\n",
    "                f\"running loss: {epoch_loss / (i_graph + 1):.5f}, graph loss: {graph_loss:.5f} score: {score:.3f}\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "        model.eval()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        dfscore = evaluate_score(dataset=valid_layout_dataset, model=model)\n",
    "        avg_loss = dfscore[\"graph_loss\"].mean()\n",
    "        avg_score = dfscore[\"score\"].mean()\n",
    "        for _, row_score in dfscore.iterrows():\n",
    "            graph_arch, graph_perm = row_score[\"arch\"], row_score[\"perm\"]\n",
    "            record = {\n",
    "                \"epoch\": epoch,\n",
    "                \"i_graph\": -1,\n",
    "                \"arch\": graph_arch,\n",
    "                \"perm\": graph_perm,\n",
    "                \"filename\": row_score[\"filename\"],\n",
    "                f\"valid-{graph_arch}-{graph_perm}/epoch_loss\": avg_loss,\n",
    "                f\"valid-{graph_arch}-{graph_perm}/graph_loss\": row_score[\"graph_loss\"],\n",
    "                f\"valid-{graph_arch}-{graph_perm}/score\": row_score[\"score\"],\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            records.append(record)\n",
    "            wandb.log(record)\n",
    "\n",
    "        print(f\"[valid] current loss: {avg_loss:.5f} score: {avg_score:.3f}\")\n",
    "\n",
    "        if best_score < avg_score:\n",
    "            best_score = avg_score\n",
    "            torch.save(model.state_dict(), savedir / \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), savedir / f\"epoch{epoch + 1}_model.pt\")\n",
    "\n",
    "    dflog = pd.DataFrame(records)\n",
    "    dflog.to_csv(savedir / \"log.csv\", index=False)\n",
    "\n",
    "    torch.save(model.state_dict(), savedir / \"final_model.pt\")\n",
    "\n",
    "    del (\n",
    "        train_layout_dataset,\n",
    "        valid_layout_dataset,\n",
    "        model,\n",
    "        optimizer,\n",
    "        dfscore,\n",
    "        dflog,\n",
    "        records,\n",
    "    )\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exptname = str(Path().resolve()).split(\"/\")[-1]\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"predict-ai-model-runtime-for-sun-scan-clan\",\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"params\": asdict(params),\n",
    "#         \"const\": asdict(const),\n",
    "#         \"validation\": \"hold-out\",\n",
    "#     },\n",
    "#     name=exptname,\n",
    "#     tags=[\"all\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed_everything(43)\n",
    "# dftrain = dataset_dict[\"train\"]\n",
    "# dfvalid = dataset_dict[\"valid\"]\n",
    "\n",
    "# train_model(\n",
    "#     dftrain=dftrain,\n",
    "#     dfvalid=dfvalid,\n",
    "#     params=params,\n",
    "#     const=const,\n",
    "#     cat_status=cat_status,\n",
    "#     cat_config_status=cat_config_status,\n",
    "#     savedir=workdir,\n",
    "#     checkpoint_dir=None,\n",
    "# )\n",
    "# wandb.alert(title=exptname, text=f\"Train End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedir = workdir / f\"{arch}-{perm}\"\n",
    "# dflog = pd.read_csv(savedir / \"log.csv\")\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "# for i, ds in enumerate([\"train\", \"valid\"]):\n",
    "#     dflog_ = dflog.query(\"(phase == @ds)\").groupby(\"epoch\")\n",
    "#     axes[i][0].plot(dflog_[\"current_loss\"].mean(), label=\"total\")\n",
    "#     axes[i][1].plot(dflog_[\"score\"].mean(), label=\"taotal\")\n",
    "#     if i == 0:\n",
    "#         axes[i][0].legend()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedir = workdir\n",
    "\n",
    "# records = []\n",
    "\n",
    "# dftest = dataset_dict[\"test\"]\n",
    "\n",
    "# test_layout_dataset = LayoutDataset(\n",
    "#     dataset=dftest,\n",
    "#     params=params,\n",
    "#     cat_status=cat_status,\n",
    "#     cat_config_status=cat_config_status,\n",
    "# )\n",
    "# model = SimpleLayoutModel(\n",
    "#     params=params,\n",
    "#     const=const,\n",
    "#     cat_status=cat_status,\n",
    "#     cat_config_status=cat_config_status,\n",
    "# )\n",
    "# model.load_state_dict(torch.load(workdir / \"final_model.pt\"))\n",
    "# model.eval()\n",
    "\n",
    "# with tqdm(range(len(test_layout_dataset))) as pbar:\n",
    "#     for i in pbar:\n",
    "#         file_info = test_layout_dataset.get_ith_file_info(i)\n",
    "\n",
    "#         pred_list = []\n",
    "#         for (\n",
    "#             node_opcode,\n",
    "#             node_flag_feat,\n",
    "#             node_cont_feat,\n",
    "#             node_cat_feat,\n",
    "#             node_config_feat,\n",
    "#             node_config_cont_feat,\n",
    "#             edge_index,\n",
    "#             node_splits,\n",
    "#             target,\n",
    "#         ) in test_layout_dataset.getitem_as_batch(i):\n",
    "#             pred_batch = model(\n",
    "#                 node_opcode=node_opcode,\n",
    "#                 node_flag_feat=node_flag_feat,\n",
    "#                 node_cont_feat=node_cont_feat,\n",
    "#                 node_cat_feat=node_cat_feat,\n",
    "#                 node_config_feat=node_config_feat,\n",
    "#                 node_config_cont_feat=node_config_cont_feat,\n",
    "#                 edge_index=edge_index,\n",
    "#                 node_splits=node_splits,\n",
    "#             )\n",
    "#             if params.device == \"cuda\":\n",
    "#                 pred_batch = pred_batch.cpu().detach().numpy()\n",
    "#             else:\n",
    "#                 pred_batch = pred_batch.detach().numpy()\n",
    "#             # pred_batchは高いものほどよい\n",
    "#             pred_batch = -pred_batch\n",
    "#             pred_list.append(pred_batch)\n",
    "\n",
    "#             del (\n",
    "#                 node_opcode,\n",
    "#                 node_flag_feat,\n",
    "#                 node_cont_feat,\n",
    "#                 node_cat_feat,\n",
    "#                 node_config_feat,\n",
    "#                 node_config_cont_feat,\n",
    "#                 edge_index,\n",
    "#                 node_splits,\n",
    "#                 target,\n",
    "#             )\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         pred = np.hstack(pred_list)\n",
    "\n",
    "#         ID = f\"layout:{file_info['arch']}:{file_info['perm']}:{file_info['filename']}\"\n",
    "#         records.append({\"ID\": ID, \"pred\": \";\".join(list(map(str, pred.argsort())))})\n",
    "\n",
    "# del test_layout_dataset, model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# dfpred = pd.DataFrame(records)\n",
    "# dfsub = pd.read_csv(inputdir / \"sample_submission.csv\")\n",
    "# dfsub = dfsub.merge(dfpred, on=\"ID\", how=\"left\")\n",
    "# dfsub[\"TopConfigs\"] = np.where(\n",
    "#     dfsub[\"pred\"].isnull(), dfsub[\"TopConfigs\"], dfsub[\"pred\"]\n",
    "# )\n",
    "# dfsub[[\"ID\", \"TopConfigs\"]].to_csv(savedir / f\"submission_final_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.alert(title=exptname, text=f\"Inference End\")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147857"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_layout_dataset = LayoutDataset(\n",
    "    dataset=dataset_dict[\"valid\"],\n",
    "    params=params,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model = SimpleLayoutModel(\n",
    "    params=params,\n",
    "    const=const,\n",
    "    cat_status=cat_status,\n",
    "    cat_config_status=cat_config_status,\n",
    ")\n",
    "model.load_state_dict(torch.load(workdir / \"final_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "num_params = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        num_params += p.numel()\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524e8f99a7894b4591ca0210754ab171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "eval_dict = {}\n",
    "# 各グラフ毎にスコアを算出\n",
    "for graph_index in tqdm(range(len(valid_layout_dataset))):\n",
    "    graph_info = valid_layout_dataset.get_ith_file_info(graph_index)\n",
    "    arch, perm, filename = (\n",
    "        graph_info[\"arch\"],\n",
    "        graph_info[\"perm\"],\n",
    "        graph_info[\"filename\"],\n",
    "    )\n",
    "    # グラフ毎に1000件をバッチに分けて取得\n",
    "    preds, truths = [], []\n",
    "    for (\n",
    "        node_opcode,\n",
    "        node_flag_feat,\n",
    "        node_cont_feat,\n",
    "        node_cat_feat,\n",
    "        node_config_feat,\n",
    "        node_config_cont_feat,\n",
    "        edge_index,\n",
    "        node_splits,\n",
    "        target,\n",
    "    ) in valid_layout_dataset.getitem_as_batch(graph_index):\n",
    "        pred = model(\n",
    "            node_opcode=node_opcode,\n",
    "            node_flag_feat=node_flag_feat,\n",
    "            node_cont_feat=node_cont_feat,\n",
    "            node_cat_feat=node_cat_feat,\n",
    "            node_config_feat=node_config_feat,\n",
    "            node_config_cont_feat=node_config_cont_feat,\n",
    "            edge_index=edge_index,\n",
    "            node_splits=node_splits,\n",
    "        )\n",
    "        pred, truth = to_cpu_numpy(params, pred, target)\n",
    "        preds.append(pred)\n",
    "        truths.append(truth)\n",
    "\n",
    "    preds, truths = np.hstack(preds), np.hstack(truths)\n",
    "    eval_dict[f\"{arch}-{perm}-{filename}\"] = {\n",
    "        \"pred\": preds,\n",
    "        \"truth\": truths,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(workdir / \"evaluation.pkl\", \"wb\") as f:\n",
    "    pkl.dump(eval_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(workdir / \"evaluation.pkl\", \"rb\") as f:\n",
    "    eval_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_i in range(len(valid_layout_dataset)):\n",
    "    row = valid_layout_dataset.rows[graph_i]\n",
    "\n",
    "    filepath, filename = row[\"filepath\"], row[\"filename\"]\n",
    "    arch, perm = row[\"arch\"], row[\"perm\"]\n",
    "\n",
    "    runtime = np.load(filepath)[\"config_runtime\"]\n",
    "    key = f\"{arch}-{perm}-{filename}\"\n",
    "    argsorted = np.argsort(runtime)\n",
    "\n",
    "    eval_dict[key][\"runtime\"] = runtime\n",
    "    eval_dict[key][\"sorted\"] = argsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': array([84.65656 , 84.65656 , 83.9334  , ..., 84.674095, 84.560555,\n",
       "        84.674736], dtype=float32),\n",
       " 'truth': array([ 9.,  6., 10., ...,  9.,  4.,  6.], dtype=float32),\n",
       " 'runtime': array([57941591, 57946389, 57941475, ..., 57240847, 57260677, 57243437]),\n",
       " 'sorted': array([16826, 17568, 36924, ..., 43862, 43871, 43793])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = eval_dict[\"nlp-default-albert_en_xlarge_batch_size_16_test\"]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entire': 0.2977103817573796,\n",
       " 'top': 0.027264926979169702,\n",
       " 'sample_min': 0.2435410908853032,\n",
       " 'sample_mean': 0.2939847372078976,\n",
       " 'sample_max': 0.34074091871167567,\n",
       " 'tpu_graph': 0.2742359213237993}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(pred: np.ndarray, runtime: np.ndarray) -> dict[str, float]:\n",
    "    truth_sorted = np.argsort(runtime)\n",
    "    top1000 = truth_sorted[:1000]\n",
    "\n",
    "    num_samples = min(pred.shape[0], 1000)\n",
    "    score_samples = []\n",
    "    for _ in range(100):\n",
    "        samples = random.sample(list(range(pred.shape[0])), num_samples)\n",
    "        _score_sample = kendalltau(runtime[samples], -pred[samples]).correlation\n",
    "        score_samples.append(_score_sample)\n",
    "\n",
    "    third = num_samples // 3\n",
    "    numbers = list(range(pred.shape[0]))\n",
    "    paper_samples = (\n",
    "        numbers[:third]\n",
    "        + numbers[-third:]\n",
    "        + random.sample(numbers[third:-third], num_samples - 2 * third)\n",
    "    )\n",
    "    paper_score = kendalltau(runtime[paper_samples], -pred[paper_samples]).correlation\n",
    "\n",
    "    score = kendalltau(runtime, -pred).correlation\n",
    "    top_score = kendalltau(runtime[top1000], -pred[top1000]).correlation\n",
    "    return {\n",
    "        \"entire\": score,\n",
    "        \"top\": top_score,\n",
    "        \"sample_min\": np.min(score_samples),\n",
    "        \"sample_mean\": np.mean(score_samples),\n",
    "        \"sample_max\": np.max(score_samples),\n",
    "        \"tpu_graph\": paper_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entire</th>\n",
       "      <th>top</th>\n",
       "      <th>sample_min</th>\n",
       "      <th>sample_mean</th>\n",
       "      <th>sample_max</th>\n",
       "      <th>tpu_graph</th>\n",
       "      <th>arch</th>\n",
       "      <th>perm</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297710</td>\n",
       "      <td>0.027265</td>\n",
       "      <td>0.239272</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.354712</td>\n",
       "      <td>0.261422</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>albert_en_xlarge_batch_size_16_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.520705</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.458884</td>\n",
       "      <td>0.520852</td>\n",
       "      <td>0.568973</td>\n",
       "      <td>0.493009</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>bert_en_cased_L-12_H-768_A-12_batch_size_16_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.464523</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.410444</td>\n",
       "      <td>0.463333</td>\n",
       "      <td>0.504307</td>\n",
       "      <td>0.482040</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>bert_multi_cased_L-12_H-768_A-12_batch_size_16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.571434</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.534760</td>\n",
       "      <td>0.568467</td>\n",
       "      <td>0.614591</td>\n",
       "      <td>0.587233</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259860</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.205376</td>\n",
       "      <td>0.263365</td>\n",
       "      <td>0.319668</td>\n",
       "      <td>0.080380</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.460203</td>\n",
       "      <td>0.090241</td>\n",
       "      <td>0.421440</td>\n",
       "      <td>0.461021</td>\n",
       "      <td>0.515860</td>\n",
       "      <td>0.543119</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.318574</td>\n",
       "      <td>0.062870</td>\n",
       "      <td>0.266653</td>\n",
       "      <td>0.317277</td>\n",
       "      <td>0.372550</td>\n",
       "      <td>0.220994</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.352070</td>\n",
       "      <td>0.061480</td>\n",
       "      <td>0.285631</td>\n",
       "      <td>0.350225</td>\n",
       "      <td>0.402190</td>\n",
       "      <td>0.232781</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-512_A-8_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.479221</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.426352</td>\n",
       "      <td>0.483275</td>\n",
       "      <td>0.533377</td>\n",
       "      <td>0.514883</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.476380</td>\n",
       "      <td>-0.027860</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.477242</td>\n",
       "      <td>0.516558</td>\n",
       "      <td>0.509227</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.460692</td>\n",
       "      <td>0.039523</td>\n",
       "      <td>0.405666</td>\n",
       "      <td>0.464393</td>\n",
       "      <td>0.526282</td>\n",
       "      <td>0.396301</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-12_H-768_A-12_bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.450313</td>\n",
       "      <td>-0.002932</td>\n",
       "      <td>0.405451</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.493263</td>\n",
       "      <td>0.478128</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-2_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.488653</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>0.433308</td>\n",
       "      <td>0.488487</td>\n",
       "      <td>0.532283</td>\n",
       "      <td>0.486760</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-4_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.375658</td>\n",
       "      <td>0.031412</td>\n",
       "      <td>0.300618</td>\n",
       "      <td>0.372301</td>\n",
       "      <td>0.447641</td>\n",
       "      <td>0.393044</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-4_H-512_A-8_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.427376</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>0.354469</td>\n",
       "      <td>0.429322</td>\n",
       "      <td>0.484167</td>\n",
       "      <td>0.392638</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.447990</td>\n",
       "      <td>0.063981</td>\n",
       "      <td>0.378034</td>\n",
       "      <td>0.443564</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.448660</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.431339</td>\n",
       "      <td>-0.023358</td>\n",
       "      <td>0.372431</td>\n",
       "      <td>0.427204</td>\n",
       "      <td>0.471184</td>\n",
       "      <td>0.472662</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-512_A-8_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.507340</td>\n",
       "      <td>-0.050780</td>\n",
       "      <td>0.460079</td>\n",
       "      <td>0.506895</td>\n",
       "      <td>0.546534</td>\n",
       "      <td>0.509618</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.499377</td>\n",
       "      <td>0.060442</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.498739</td>\n",
       "      <td>0.547950</td>\n",
       "      <td>0.432470</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.347351</td>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.271712</td>\n",
       "      <td>0.344749</td>\n",
       "      <td>0.407078</td>\n",
       "      <td>0.340554</td>\n",
       "      <td>nlp</td>\n",
       "      <td>default</td>\n",
       "      <td>talking-heads_large_batch_size_16_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.781792</td>\n",
       "      <td>-0.068577</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.781539</td>\n",
       "      <td>0.804407</td>\n",
       "      <td>0.777092</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>albert_en_xlarge_batch_size_16_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.686198</td>\n",
       "      <td>0.277731</td>\n",
       "      <td>0.646489</td>\n",
       "      <td>0.685310</td>\n",
       "      <td>0.712487</td>\n",
       "      <td>0.694676</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>bert_en_cased_L-12_H-768_A-12_batch_size_16_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.869967</td>\n",
       "      <td>0.720966</td>\n",
       "      <td>0.854615</td>\n",
       "      <td>0.869987</td>\n",
       "      <td>0.884657</td>\n",
       "      <td>0.873872</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>bert_multi_cased_L-12_H-768_A-12_batch_size_16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.744176</td>\n",
       "      <td>0.385443</td>\n",
       "      <td>0.731388</td>\n",
       "      <td>0.743862</td>\n",
       "      <td>0.762501</td>\n",
       "      <td>0.733199</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.617154</td>\n",
       "      <td>0.471311</td>\n",
       "      <td>0.587681</td>\n",
       "      <td>0.616057</td>\n",
       "      <td>0.647435</td>\n",
       "      <td>0.626717</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-128_A-2_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.740729</td>\n",
       "      <td>0.275094</td>\n",
       "      <td>0.698816</td>\n",
       "      <td>0.738642</td>\n",
       "      <td>0.764922</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.756774</td>\n",
       "      <td>0.743433</td>\n",
       "      <td>0.737793</td>\n",
       "      <td>0.755931</td>\n",
       "      <td>0.773642</td>\n",
       "      <td>0.762684</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-256_A-4_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.861979</td>\n",
       "      <td>-0.051137</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.861579</td>\n",
       "      <td>0.871584</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-512_A-8_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.869643</td>\n",
       "      <td>0.705878</td>\n",
       "      <td>0.857510</td>\n",
       "      <td>0.869972</td>\n",
       "      <td>0.885140</td>\n",
       "      <td>0.872286</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.768013</td>\n",
       "      <td>0.099740</td>\n",
       "      <td>0.746545</td>\n",
       "      <td>0.768347</td>\n",
       "      <td>0.788944</td>\n",
       "      <td>0.767284</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-10_H-768_A-12_bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.884826</td>\n",
       "      <td>0.714897</td>\n",
       "      <td>0.872363</td>\n",
       "      <td>0.884525</td>\n",
       "      <td>0.894015</td>\n",
       "      <td>0.878007</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-12_H-768_A-12_bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.753710</td>\n",
       "      <td>0.502876</td>\n",
       "      <td>0.738162</td>\n",
       "      <td>0.753542</td>\n",
       "      <td>0.768856</td>\n",
       "      <td>0.753535</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-2_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.848413</td>\n",
       "      <td>0.674406</td>\n",
       "      <td>0.833181</td>\n",
       "      <td>0.848966</td>\n",
       "      <td>0.859331</td>\n",
       "      <td>0.858750</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-4_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.905004</td>\n",
       "      <td>0.515056</td>\n",
       "      <td>0.898525</td>\n",
       "      <td>0.905092</td>\n",
       "      <td>0.910254</td>\n",
       "      <td>0.905770</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-4_H-512_A-8_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.825571</td>\n",
       "      <td>0.559102</td>\n",
       "      <td>0.810929</td>\n",
       "      <td>0.825386</td>\n",
       "      <td>0.842337</td>\n",
       "      <td>0.829195</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.766201</td>\n",
       "      <td>0.750093</td>\n",
       "      <td>0.744960</td>\n",
       "      <td>0.765921</td>\n",
       "      <td>0.791756</td>\n",
       "      <td>0.756207</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-256_A-4_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.833732</td>\n",
       "      <td>-0.097843</td>\n",
       "      <td>0.818348</td>\n",
       "      <td>0.835197</td>\n",
       "      <td>0.846269</td>\n",
       "      <td>0.845206</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-512_A-8_batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.645210</td>\n",
       "      <td>0.241334</td>\n",
       "      <td>0.620570</td>\n",
       "      <td>0.646222</td>\n",
       "      <td>0.673277</td>\n",
       "      <td>0.628275</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.897955</td>\n",
       "      <td>0.654672</td>\n",
       "      <td>0.889121</td>\n",
       "      <td>0.897815</td>\n",
       "      <td>0.904872</td>\n",
       "      <td>0.900459</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>small_bert_bert_en_uncased_L-6_H-768_A-12_batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.719724</td>\n",
       "      <td>0.409410</td>\n",
       "      <td>0.700055</td>\n",
       "      <td>0.719215</td>\n",
       "      <td>0.745218</td>\n",
       "      <td>0.712801</td>\n",
       "      <td>nlp</td>\n",
       "      <td>random</td>\n",
       "      <td>talking-heads_large_batch_size_16_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.422484</td>\n",
       "      <td>0.077908</td>\n",
       "      <td>0.376053</td>\n",
       "      <td>0.426006</td>\n",
       "      <td>0.472721</td>\n",
       "      <td>0.344798</td>\n",
       "      <td>xla</td>\n",
       "      <td>default</td>\n",
       "      <td>bert_pretraining.4x4.fp16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.188600</td>\n",
       "      <td>-0.084511</td>\n",
       "      <td>0.134192</td>\n",
       "      <td>0.190930</td>\n",
       "      <td>0.242272</td>\n",
       "      <td>0.207637</td>\n",
       "      <td>xla</td>\n",
       "      <td>default</td>\n",
       "      <td>inception_v3_batch_128_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.285037</td>\n",
       "      <td>-0.025587</td>\n",
       "      <td>0.242981</td>\n",
       "      <td>0.285302</td>\n",
       "      <td>0.326288</td>\n",
       "      <td>0.227291</td>\n",
       "      <td>xla</td>\n",
       "      <td>default</td>\n",
       "      <td>resnet50.4x4.fp16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.126612</td>\n",
       "      <td>-0.088803</td>\n",
       "      <td>0.057046</td>\n",
       "      <td>0.125868</td>\n",
       "      <td>0.178537</td>\n",
       "      <td>0.207603</td>\n",
       "      <td>xla</td>\n",
       "      <td>default</td>\n",
       "      <td>resnet_v1_50_official_batch_128_bf16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.274283</td>\n",
       "      <td>-0.014897</td>\n",
       "      <td>0.226914</td>\n",
       "      <td>0.276972</td>\n",
       "      <td>0.318824</td>\n",
       "      <td>0.269923</td>\n",
       "      <td>xla</td>\n",
       "      <td>default</td>\n",
       "      <td>tf2_bert_pretrain_dynamic_batch_size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.149257</td>\n",
       "      <td>-0.037988</td>\n",
       "      <td>0.114497</td>\n",
       "      <td>0.147837</td>\n",
       "      <td>0.202709</td>\n",
       "      <td>0.172956</td>\n",
       "      <td>xla</td>\n",
       "      <td>default</td>\n",
       "      <td>unet_3d.4x4.bf16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.824928</td>\n",
       "      <td>0.490947</td>\n",
       "      <td>0.812778</td>\n",
       "      <td>0.824923</td>\n",
       "      <td>0.835723</td>\n",
       "      <td>0.826169</td>\n",
       "      <td>xla</td>\n",
       "      <td>random</td>\n",
       "      <td>bert_pretraining.4x4.fp16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.785025</td>\n",
       "      <td>0.132110</td>\n",
       "      <td>0.764218</td>\n",
       "      <td>0.784161</td>\n",
       "      <td>0.797346</td>\n",
       "      <td>0.793063</td>\n",
       "      <td>xla</td>\n",
       "      <td>random</td>\n",
       "      <td>inception_v3_batch_128_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.186559</td>\n",
       "      <td>0.420689</td>\n",
       "      <td>0.110052</td>\n",
       "      <td>0.186420</td>\n",
       "      <td>0.229728</td>\n",
       "      <td>0.193976</td>\n",
       "      <td>xla</td>\n",
       "      <td>random</td>\n",
       "      <td>resnet50.4x4.fp16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.425585</td>\n",
       "      <td>0.485116</td>\n",
       "      <td>0.363663</td>\n",
       "      <td>0.425261</td>\n",
       "      <td>0.476474</td>\n",
       "      <td>0.434118</td>\n",
       "      <td>xla</td>\n",
       "      <td>random</td>\n",
       "      <td>resnet_v1_50_official_batch_128_bf16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.656198</td>\n",
       "      <td>0.183192</td>\n",
       "      <td>0.623066</td>\n",
       "      <td>0.654139</td>\n",
       "      <td>0.680737</td>\n",
       "      <td>0.651403</td>\n",
       "      <td>xla</td>\n",
       "      <td>random</td>\n",
       "      <td>tf2_bert_pretrain_dynamic_batch_size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.513171</td>\n",
       "      <td>0.513171</td>\n",
       "      <td>0.513171</td>\n",
       "      <td>0.513171</td>\n",
       "      <td>0.513171</td>\n",
       "      <td>0.513171</td>\n",
       "      <td>xla</td>\n",
       "      <td>random</td>\n",
       "      <td>unet_3d.4x4.bf16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      entire       top  sample_min  sample_mean  sample_max  tpu_graph arch  \\\n",
       "0   0.297710  0.027265    0.239272     0.301800    0.354712   0.261422  nlp   \n",
       "1   0.520705  0.011956    0.458884     0.520852    0.568973   0.493009  nlp   \n",
       "2   0.464523  0.029500    0.410444     0.463333    0.504307   0.482040  nlp   \n",
       "3   0.571434  0.043426    0.534760     0.568467    0.614591   0.587233  nlp   \n",
       "4   0.259860  0.002121    0.205376     0.263365    0.319668   0.080380  nlp   \n",
       "5   0.460203  0.090241    0.421440     0.461021    0.515860   0.543119  nlp   \n",
       "6   0.318574  0.062870    0.266653     0.317277    0.372550   0.220994  nlp   \n",
       "7   0.352070  0.061480    0.285631     0.350225    0.402190   0.232781  nlp   \n",
       "8   0.479221  0.000266    0.426352     0.483275    0.533377   0.514883  nlp   \n",
       "9   0.476380 -0.027860    0.431250     0.477242    0.516558   0.509227  nlp   \n",
       "10  0.460692  0.039523    0.405666     0.464393    0.526282   0.396301  nlp   \n",
       "11  0.450313 -0.002932    0.405451     0.448500    0.493263   0.478128  nlp   \n",
       "12  0.488653  0.022327    0.433308     0.488487    0.532283   0.486760  nlp   \n",
       "13  0.375658  0.031412    0.300618     0.372301    0.447641   0.393044  nlp   \n",
       "14  0.427376 -0.032227    0.354469     0.429322    0.484167   0.392638  nlp   \n",
       "15  0.447990  0.063981    0.378034     0.443564    0.488131   0.448660  nlp   \n",
       "16  0.431339 -0.023358    0.372431     0.427204    0.471184   0.472662  nlp   \n",
       "17  0.507340 -0.050780    0.460079     0.506895    0.546534   0.509618  nlp   \n",
       "18  0.499377  0.060442    0.455044     0.498739    0.547950   0.432470  nlp   \n",
       "19  0.347351  0.048851    0.271712     0.344749    0.407078   0.340554  nlp   \n",
       "20  0.781792 -0.068577    0.757081     0.781539    0.804407   0.777092  nlp   \n",
       "21  0.686198  0.277731    0.646489     0.685310    0.712487   0.694676  nlp   \n",
       "22  0.869967  0.720966    0.854615     0.869987    0.884657   0.873872  nlp   \n",
       "23  0.744176  0.385443    0.731388     0.743862    0.762501   0.733199  nlp   \n",
       "24  0.617154  0.471311    0.587681     0.616057    0.647435   0.626717  nlp   \n",
       "25  0.740729  0.275094    0.698816     0.738642    0.764922   0.733646  nlp   \n",
       "26  0.756774  0.743433    0.737793     0.755931    0.773642   0.762684  nlp   \n",
       "27  0.861979 -0.051137    0.848115     0.861579    0.871584   0.863014  nlp   \n",
       "28  0.869643  0.705878    0.857510     0.869972    0.885140   0.872286  nlp   \n",
       "29  0.768013  0.099740    0.746545     0.768347    0.788944   0.767284  nlp   \n",
       "30  0.884826  0.714897    0.872363     0.884525    0.894015   0.878007  nlp   \n",
       "31  0.753710  0.502876    0.738162     0.753542    0.768856   0.753535  nlp   \n",
       "32  0.848413  0.674406    0.833181     0.848966    0.859331   0.858750  nlp   \n",
       "33  0.905004  0.515056    0.898525     0.905092    0.910254   0.905770  nlp   \n",
       "34  0.825571  0.559102    0.810929     0.825386    0.842337   0.829195  nlp   \n",
       "35  0.766201  0.750093    0.744960     0.765921    0.791756   0.756207  nlp   \n",
       "36  0.833732 -0.097843    0.818348     0.835197    0.846269   0.845206  nlp   \n",
       "37  0.645210  0.241334    0.620570     0.646222    0.673277   0.628275  nlp   \n",
       "38  0.897955  0.654672    0.889121     0.897815    0.904872   0.900459  nlp   \n",
       "39  0.719724  0.409410    0.700055     0.719215    0.745218   0.712801  nlp   \n",
       "40  0.422484  0.077908    0.376053     0.426006    0.472721   0.344798  xla   \n",
       "41  0.188600 -0.084511    0.134192     0.190930    0.242272   0.207637  xla   \n",
       "42  0.285037 -0.025587    0.242981     0.285302    0.326288   0.227291  xla   \n",
       "43  0.126612 -0.088803    0.057046     0.125868    0.178537   0.207603  xla   \n",
       "44  0.274283 -0.014897    0.226914     0.276972    0.318824   0.269923  xla   \n",
       "45  0.149257 -0.037988    0.114497     0.147837    0.202709   0.172956  xla   \n",
       "46  0.824928  0.490947    0.812778     0.824923    0.835723   0.826169  xla   \n",
       "47  0.785025  0.132110    0.764218     0.784161    0.797346   0.793063  xla   \n",
       "48  0.186559  0.420689    0.110052     0.186420    0.229728   0.193976  xla   \n",
       "49  0.425585  0.485116    0.363663     0.425261    0.476474   0.434118  xla   \n",
       "50  0.656198  0.183192    0.623066     0.654139    0.680737   0.651403  xla   \n",
       "51  0.513171  0.513171    0.513171     0.513171    0.513171   0.513171  xla   \n",
       "\n",
       "       perm                                           filename  \n",
       "0   default                albert_en_xlarge_batch_size_16_test  \n",
       "1   default   bert_en_cased_L-12_H-768_A-12_batch_size_16_test  \n",
       "2   default  bert_multi_cased_L-12_H-768_A-12_batch_size_16...  \n",
       "3   default  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  \n",
       "4   default  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  \n",
       "5   default  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  \n",
       "6   default  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  \n",
       "7   default  small_bert_bert_en_uncased_L-10_H-512_A-8_batc...  \n",
       "8   default  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  \n",
       "9   default  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  \n",
       "10  default  small_bert_bert_en_uncased_L-12_H-768_A-12_bat...  \n",
       "11  default  small_bert_bert_en_uncased_L-2_H-256_A-4_batch...  \n",
       "12  default  small_bert_bert_en_uncased_L-4_H-256_A-4_batch...  \n",
       "13  default  small_bert_bert_en_uncased_L-4_H-512_A-8_batch...  \n",
       "14  default  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  \n",
       "15  default  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  \n",
       "16  default  small_bert_bert_en_uncased_L-6_H-512_A-8_batch...  \n",
       "17  default  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  \n",
       "18  default  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  \n",
       "19  default            talking-heads_large_batch_size_16_train  \n",
       "20   random                albert_en_xlarge_batch_size_16_test  \n",
       "21   random   bert_en_cased_L-12_H-768_A-12_batch_size_16_test  \n",
       "22   random  bert_multi_cased_L-12_H-768_A-12_batch_size_16...  \n",
       "23   random  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  \n",
       "24   random  small_bert_bert_en_uncased_L-10_H-128_A-2_batc...  \n",
       "25   random  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  \n",
       "26   random  small_bert_bert_en_uncased_L-10_H-256_A-4_batc...  \n",
       "27   random  small_bert_bert_en_uncased_L-10_H-512_A-8_batc...  \n",
       "28   random  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  \n",
       "29   random  small_bert_bert_en_uncased_L-10_H-768_A-12_bat...  \n",
       "30   random  small_bert_bert_en_uncased_L-12_H-768_A-12_bat...  \n",
       "31   random  small_bert_bert_en_uncased_L-2_H-256_A-4_batch...  \n",
       "32   random  small_bert_bert_en_uncased_L-4_H-256_A-4_batch...  \n",
       "33   random  small_bert_bert_en_uncased_L-4_H-512_A-8_batch...  \n",
       "34   random  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  \n",
       "35   random  small_bert_bert_en_uncased_L-6_H-256_A-4_batch...  \n",
       "36   random  small_bert_bert_en_uncased_L-6_H-512_A-8_batch...  \n",
       "37   random  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  \n",
       "38   random  small_bert_bert_en_uncased_L-6_H-768_A-12_batc...  \n",
       "39   random            talking-heads_large_batch_size_16_train  \n",
       "40  default                          bert_pretraining.4x4.fp16  \n",
       "41  default                       inception_v3_batch_128_train  \n",
       "42  default                                  resnet50.4x4.fp16  \n",
       "43  default               resnet_v1_50_official_batch_128_bf16  \n",
       "44  default               tf2_bert_pretrain_dynamic_batch_size  \n",
       "45  default                                   unet_3d.4x4.bf16  \n",
       "46   random                          bert_pretraining.4x4.fp16  \n",
       "47   random                       inception_v3_batch_128_train  \n",
       "48   random                                  resnet50.4x4.fp16  \n",
       "49   random               resnet_v1_50_official_batch_128_bf16  \n",
       "50   random               tf2_bert_pretrain_dynamic_batch_size  \n",
       "51   random                                   unet_3d.4x4.bf16  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "for key in eval_dict:\n",
    "    row = eval_dict[key]\n",
    "    record = evaluate(row[\"pred\"], row[\"runtime\"])\n",
    "    arch, perm = key.split(\"-\")[0], key.split(\"-\")[1]\n",
    "    filename = key.replace(f\"{arch}-{perm}-\", \"\")\n",
    "    record.update({\"arch\": arch, \"perm\": perm, \"filename\": filename})\n",
    "    records.append(record)\n",
    "dfeval = pd.DataFrame(records)\n",
    "dfeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeval.to_excel(workdir / \"evaluate.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
